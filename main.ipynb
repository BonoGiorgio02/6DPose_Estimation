{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BonoGiorgio02/6DPose_Estimation/blob/saves/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266fc8f6",
      "metadata": {
        "id": "266fc8f6"
      },
      "source": [
        "# 6D Pose Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b41fabe",
      "metadata": {
        "id": "6b41fabe"
      },
      "source": [
        "## Set up the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "E0JE8X2xVStz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0JE8X2xVStz",
        "outputId": "8785a7d8-446c-4cb8-efd0-47b46b86cf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yWjezEEI8X8G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWjezEEI8X8G",
        "outputId": "8e125599-720b-421b-e129-52255460682b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/6D_pose_estimation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "A_eTTkV1Vb_B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_eTTkV1Vb_B",
        "outputId": "b1ce8a8a-e710-478f-eeb7-0585db95f66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation\n"
          ]
        }
      ],
      "source": [
        "path = !pwd\n",
        "path = path[0]\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ha7S6vgzPytp",
      "metadata": {
        "id": "ha7S6vgzPytp"
      },
      "outputs": [],
      "source": [
        "# PRIMO BLOCCO - Solo installazioni (senza import!)\n",
        "%%capture\n",
        "\n",
        "!pip install comet_ml\n",
        "import comet_ml\n",
        "from comet_ml import Experiment\n",
        "from comet_ml.integration.pytorch import watch\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "# Installa PRIMA tutte le dipendenze PyTorch Geometric\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib seaborn scikit-learn pyyaml plotly open3d Pillow ultralytics wandb numpy-quaternion trimesh\n",
        "\n",
        "print(\"Installazione completata! RIAVVIA IL RUNTIME prima di procedere.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "RwjUcz2OP1RR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RwjUcz2OP1RR",
        "outputId": "df3fd311-c5c1-43c6-d75b-641e7b87d6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "✅ PyTorch Geometric installato correttamente!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "PyTorch Geometric version: 2.6.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import open3d as o3d\n",
        "import itertools\n",
        "import shutil\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import wandb\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from torchvision import models\n",
        "import cv2\n",
        "from torch.optim import Adam\n",
        "import quaternion\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from ultralytics import YOLO\n",
        "from torchvision.transforms import v2\n",
        "import trimesh\n",
        "\n",
        "# Import PyTorch Geometric DOPO l'installazione e il riavvio\n",
        "import torch_geometric\n",
        "from torch import Tensor\n",
        "from torch_geometric.nn import knn_interpolate, MessagePassing\n",
        "from torch_geometric.nn.pool import fps, radius\n",
        "\n",
        "IMG_WIDTH = 640\n",
        "IMG_HEIGHT = 480\n",
        "\n",
        "# Test per verificare che tutto funzioni\n",
        "try:\n",
        "    from torch_geometric.nn.pool import fps\n",
        "    print(\"✅ PyTorch Geometric installato correttamente!\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Errore: {e}\")\n",
        "    print(\"Assicurati di aver riavviato il runtime dopo l'installazione.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "QH5gsY-RFddT",
      "metadata": {
        "id": "QH5gsY-RFddT"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install torch torchvision torchaudio matplotlib seaborn scikit-learn pyyaml plotly open3d pcl Pillow ultralytics wandb numpy-quaternion trimesh torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "gf5k-eDYFvve",
      "metadata": {
        "id": "gf5k-eDYFvve"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import yaml\n",
        "# import torch\n",
        "# import torchvision\n",
        "# import open3d as o3d\n",
        "# import itertools\n",
        "# import shutil\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch import nn, optim\n",
        "# import torch.nn.functional as F\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import plotly.graph_objects as go\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from torch.utils.data import DataLoader\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as patches\n",
        "# import wandb\n",
        "# from scipy.spatial.transform import Rotation as R\n",
        "# from torchvision import models\n",
        "# import cv2\n",
        "# from torch.optim import Adam\n",
        "# import quaternion\n",
        "# from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "# from ultralytics import YOLO\n",
        "# from torchvision.transforms import v2\n",
        "# import trimesh\n",
        "# import torch_geometric\n",
        "# from torch import Tensor\n",
        "# from torch_geometric.nn import knn_interpolate, MessagePassing\n",
        "# from torch_geometric.nn.pool import fps, radius\n",
        "\n",
        "# IMG_WIDTH = 640\n",
        "# IMG_HEIGHT = 480"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "XUM7EgBOLhGi",
      "metadata": {
        "id": "XUM7EgBOLhGi"
      },
      "outputs": [],
      "source": [
        "# os.environ['TORCH'] = torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "PMwssk2LJAnD",
      "metadata": {
        "id": "PMwssk2LJAnD"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "OuzZ9_Zc3u2z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuzZ9_Zc3u2z",
        "outputId": "3029dacc-2f12-4c68-8dce-b917ff8e0005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Cuda\")\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"Cuda not available, use mps\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"Use CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GoX7b-8FF3nd",
      "metadata": {
        "id": "GoX7b-8FF3nd"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd88aaa0",
      "metadata": {
        "id": "bd88aaa0"
      },
      "source": [
        "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aOqq3Zw44sbh",
      "metadata": {
        "id": "aOqq3Zw44sbh"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/6D_pose_estimation/datasets/linemod/DenseFusion/Linemod_preprocessed_zip/Linemod_preprocessed.zip /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "49c2a5a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49c2a5a2",
        "outputId": "cd7456c5-9f26-4ea2-a97b-3cb869131838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/linemod/DenseFusion\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/datasets/linemod/DenseFusion/\n",
        "%cd /content/datasets/linemod/DenseFusion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "GHb-b4cl5jTK",
      "metadata": {
        "id": "GHb-b4cl5jTK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip /content/Linemod_preprocessed.zip -d /content/datasets/linemod/DenseFusion/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cqZgXdA6n4FU",
      "metadata": {
        "id": "cqZgXdA6n4FU"
      },
      "source": [
        "## Modify the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "iP_KKnQ-ow4y",
      "metadata": {
        "id": "iP_KKnQ-ow4y"
      },
      "outputs": [],
      "source": [
        "for el in [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]:\n",
        "    shutil.copy(f\"./Linemod_preprocessed/data/{el}/gt.yml\", f\"./Linemod_preprocessed/{el}_gt.yml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "xUvhbBU7oka8",
      "metadata": {
        "id": "xUvhbBU7oka8"
      },
      "outputs": [],
      "source": [
        "def change_02gt(path):\n",
        "    with open(path, 'r') as f:\n",
        "        pose_data = yaml.load(f, Loader=yaml.CLoader)  # Usiamo safe_load per maggiore sicurezza\n",
        "\n",
        "    filtered_data = {}\n",
        "    for key, value in pose_data.items():\n",
        "        filtered_frame = [obj for obj in value if int(obj['obj_id']) == 2]\n",
        "        if filtered_frame:\n",
        "            filtered_data[key] = filtered_frame\n",
        "\n",
        "    # Custom Dumper per mantenere il formato originale\n",
        "    class OriginalFormatDumper(yaml.Dumper):\n",
        "        def increase_indent(self, flow=False, indentless=False):\n",
        "            return super().increase_indent(flow, False)  # Forza indentazione corretta\n",
        "\n",
        "    # Configurazione per mantenere il formato desiderato\n",
        "    yaml_str = yaml.dump(\n",
        "        filtered_data,\n",
        "        sort_keys=True,\n",
        "        default_flow_style=None,  # None = auto (usa flow-style per liste interne)\n",
        "        Dumper=OriginalFormatDumper,\n",
        "        width=float(\"inf\"),\n",
        "        indent=2,  # Indentazione standard\n",
        "        allow_unicode=True,\n",
        "    )\n",
        "\n",
        "    # Salva il risultato nel file\n",
        "    with open(path, 'w') as f:\n",
        "        yaml.dump(\n",
        "            filtered_data,\n",
        "            f,\n",
        "            sort_keys=True,\n",
        "            default_flow_style=None,\n",
        "            Dumper=OriginalFormatDumper,\n",
        "            width=float(\"inf\"),\n",
        "            indent=2,\n",
        "        )\n",
        "\n",
        "change_02gt(\"./Linemod_preprocessed/02_gt.yml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "gtOLz-5Tn7tW",
      "metadata": {
        "id": "gtOLz-5Tn7tW"
      },
      "outputs": [],
      "source": [
        "def quaternion_gt(input_path= \"./Linemod_preprocessed/\"):\n",
        "\n",
        "  for gt in os.scandir(input_path):\n",
        "        if not gt.is_dir() and gt.name.endswith(('.yaml', '.yml')):\n",
        "\n",
        "          with open(gt, 'r') as f:\n",
        "            pose_data = yaml.load(f, Loader=yaml.CLoader)\n",
        "            modified_data = {}\n",
        "\n",
        "            for key,value in pose_data.items():\n",
        "              modified_poses = []\n",
        "\n",
        "              for pose in value:\n",
        "                quat= quaternion.from_rotation_matrix(np.array(pose[\"cam_R_m2c\"]).reshape(3,3))\n",
        "                pose[\"quaternion\"]= [quat.w, quat.x, quat.y, quat.z]\n",
        "                modified_poses.append(pose)\n",
        "              modified_data[key] = modified_poses\n",
        "\n",
        "            output_file = os.path.join(input_path, gt.name)\n",
        "\n",
        "            with open(gt, 'w') as f:\n",
        "                yaml.dump(\n",
        "                    modified_data,\n",
        "                    f,\n",
        "                    default_flow_style=None,\n",
        "                    width=float(\"inf\"),\n",
        "                    sort_keys=True\n",
        "                )\n",
        "quaternion_gt()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fb7108",
      "metadata": {
        "id": "15fb7108"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912e3ec7",
      "metadata": {
        "id": "912e3ec7"
      },
      "source": [
        "### Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d913bcb",
      "metadata": {
        "id": "6d913bcb"
      },
      "outputs": [],
      "source": [
        "img_path = \"./Linemod_preprocessed/data/02/rgb/0101.png\"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb8328b",
      "metadata": {
        "id": "0eb8328b"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset): # used to load and preprocess data\n",
        "    def __init__(self, dataset_root, split='train', train_ratio=0.7, seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_root (str): Path to the dataset directory.\n",
        "            split (str): 'train', 'validation' or 'test'.\n",
        "            train_ratio (float): Percentage of data used for training (default 70%).\n",
        "            seed (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.dataset_root = dataset_root\n",
        "        self.split = split\n",
        "        self.train_ratio = train_ratio\n",
        "        self.seed = seed\n",
        "\n",
        "        # Get list of all samples (folder_id, sample_id)\n",
        "        self.samples = self.get_all_samples()\n",
        "\n",
        "        # Check if samples were found\n",
        "        if not self.samples:\n",
        "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
        "\n",
        "        # Split into training and validation+test sets\n",
        "        labels = [el[0] for el in self.samples]\n",
        "        self.train_samples, self.val_test_samples = train_test_split(\n",
        "            self.samples, train_size=self.train_ratio, random_state=self.seed, stratify=labels\n",
        "        )\n",
        "\n",
        "        # split validation+test set (by default 30% of the original dataset) into validation and test sets\n",
        "        labels = [el[0] for el in self.val_test_samples]\n",
        "        self.val_samples, self.test_samples = train_test_split(self.val_test_samples, train_size=0.5, random_state=self.seed, stratify=labels)\n",
        "\n",
        "        # Select the appropriate split\n",
        "        if split == \"train\":\n",
        "            self.samples = self.train_samples\n",
        "        elif split == \"validation\":\n",
        "            self.samples = self.val_samples\n",
        "        else:\n",
        "            self.samples = self.test_samples\n",
        "\n",
        "        # Define image transformations: QUESTO PER LA BASELINE\n",
        "        if self.split == 'train':\n",
        "            # Define image transformations\n",
        "        #     self.transform_img = v2.Compose([\n",
        "        #       # v2.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "        #       # v2.RandomGrayscale(p=0.1),\n",
        "        #       # v2.RandomApply([v2.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "        #       # v2.ToImage(),  # Converte in PILImage o equivalente\n",
        "        #       v2.ToDtype(torch.float32),  # Converte in Tensor con valori tra 0 e 1\n",
        "        #       v2.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "        #   ])\n",
        "        #     self.transform_crop = v2.Compose([\n",
        "        #       # v2.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "        #       # v2.RandomGrayscale(p=0.1),\n",
        "        #       # v2.RandomApply([v2.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "        #       v2.Resize((224, 224)),\n",
        "        #       v2.ToImage(),  # Converte in PILImage o equivalente\n",
        "        #       v2.ToDtype(torch.float32),  # Converte in Tensor con valori tra 0 e 1\n",
        "        #       v2.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "        #   ])\n",
        "\n",
        "        # else:\n",
        "        #     self.transform_img = v2.Compose([\n",
        "        #         v2.ToImage(),  # Converte in PILImage o equivalente\n",
        "        #         v2.ToDtype(torch.float32),  # Converte in Tensor con valori tra 0 e 1\n",
        "        #         v2.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "        #     ])\n",
        "\n",
        "        #     self.transform_crop = v2.Compose([\n",
        "        #         v2.Resize((224, 224)),\n",
        "        #         v2.ToImage(),  # Converte in PILImage o equivalente\n",
        "        #         v2.ToDtype(torch.float32),  # Converte in Tensor con valori tra 0 e 1\n",
        "        #         v2.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "        #     ])\n",
        "\n",
        "            self.transform_img = transforms.Compose([\n",
        "                                # transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "                                # transforms.RandomGrayscale(p=0.1),\n",
        "                                # transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "                                transforms.ToTensor(),  # converte in float32 e normalizza in [0, 1]\n",
        "                                # transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "                            ])\n",
        "\n",
        "            self.transform_crop = transforms.Compose([\n",
        "                                # transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "                                # transforms.RandomGrayscale(p=0.1),\n",
        "                                # transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "                                transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "    ])\n",
        "        else:\n",
        "              self.transform_img = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                # transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "                            ])\n",
        "\n",
        "              self.transform_crop = transforms.Compose([\n",
        "                  transforms.Resize((224, 224)),\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "              ])\n",
        "\n",
        "        self.ground_truths = {}\n",
        "\n",
        "        for el in [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]:\n",
        "\n",
        "          pose_file = os.path.join(self.dataset_root, f\"{el}_gt.yml\")\n",
        "\n",
        "          with open(pose_file, 'r') as f:\n",
        "            pose_data = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "          dati = {}\n",
        "          chiavi_da_estrarre = ['cam_t_m2c', 'cam_R_m2c', 'quaternion', 'obj_bb', 'obj_id']\n",
        "          dati_estratti = {}\n",
        "\n",
        "          for key, value in pose_data.items():\n",
        "            entry = value[0]\n",
        "\n",
        "            # Estraggo solo le chiavi desiderate\n",
        "            estratti = {k: entry[k] for k in chiavi_da_estrarre if k in entry}\n",
        "\n",
        "            # Salvo nel dizionario dei dati estratti\n",
        "            dati_estratti[key]= estratti\n",
        "          self.ground_truths[el] = dati_estratti\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_samples_id(self):\n",
        "        return self.samples\n",
        "\n",
        "    def get_all_samples(self):\n",
        "        \"\"\"Retrieve the list of all available sample indices from all folders.\"\"\"\n",
        "        samples = []\n",
        "        for folder_id in range(1, 16):  # Assuming folders are named 01 to 15\n",
        "            folder_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\")\n",
        "            #print(folder_path)\n",
        "            if os.path.exists(folder_path):\n",
        "                # get id of the images\n",
        "                sample_ids = sorted([int(f.split('.')[0]) for f in os.listdir(folder_path) if f.endswith('.png')])\n",
        "                samples.extend([(folder_id, sid) for sid in sample_ids])  # Store (folder_id, sample_id)\n",
        "        return samples\n",
        "\n",
        "    def load_config(self, folder_id):\n",
        "        \"\"\"Load YAML configuration files for camera intrinsics and object info for a specific folder.\"\"\"\n",
        "        # camera_intrinsics_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", 'info.yml')\n",
        "        objects_info_path = os.path.join(self.dataset_root, 'models', f\"models_info.yml\")\n",
        "\n",
        "        # with open(camera_intrinsics_path, 'r') as f:\n",
        "            # camera_intrinsics = yaml.load(f, Loader=yaml.FullLoader)\n",
        "        camera_intrinsics = [572.4114, 573.57043, 325.2611, 242.04899]\n",
        "\n",
        "        with open(objects_info_path, 'r') as f:\n",
        "            objects_info = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "        return camera_intrinsics, objects_info\n",
        "\n",
        "    #Define here some usefull functions to access the data\n",
        "    def load_image(self, img_path):\n",
        "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        return self.transform_img(img)\n",
        "\n",
        "    #Define here some usefull functions to access the data\n",
        "    def load_cropped_image(self, img_path, bbox):\n",
        "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        x, y, w, h = bbox\n",
        "        cropped_img = img.crop((x, y, x+w, y+h))\n",
        "        return self.transform_crop(cropped_img)\n",
        "\n",
        "    def load_depth(self, depth_path):\n",
        "        \"\"\"Load a depth image and convert to tensor.\"\"\"\n",
        "        return cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
        "\n",
        "    def load_mask(self, path):\n",
        "        return cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.uint8)\n",
        "\n",
        "    def depth_to_pointcloud(self, masked_depth, intrinsics):\n",
        "        \"\"\"\n",
        "        Convert pixel coordinates + depth to 3D metric coordinates using intrinsic parameters.\n",
        "        Returns x, y, z as Nx3 array.\n",
        "        \"\"\"\n",
        "        fx, fy, cx, cy = intrinsics\n",
        "        height, width = masked_depth.shape\n",
        "        valid_mask = masked_depth > 0\n",
        "\n",
        "        u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
        "        u_valid = u[valid_mask]\n",
        "        v_valid = v[valid_mask]\n",
        "        z_valid = masked_depth[valid_mask].astype(np.float32) / 1000.0\n",
        "\n",
        "        # Ensure z_valid is not zero or very small to avoid division by zero\n",
        "        valid_depth_mask = z_valid > 1e-6\n",
        "        u_valid = u_valid[valid_depth_mask]\n",
        "        v_valid = v_valid[valid_depth_mask]\n",
        "        z_valid = z_valid[valid_depth_mask]\n",
        "\n",
        "        x_meters = (u_valid - cx) * z_valid / fx\n",
        "        y_meters = (v_valid - cy) * z_valid / fy\n",
        "        z_meters = z_valid\n",
        "\n",
        "        pointcloud = np.stack([x_meters, y_meters, z_meters], axis=-1).reshape(-1, 3)  # Shape: (N, 3)\n",
        "        return pointcloud\n",
        "\n",
        "    def load_6d_pose(self, folder_id, sample_id):\n",
        "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
        "        pose = self.ground_truths[f\"{folder_id:02d}\"][int(sample_id)]\n",
        "        # print(self.traslations[f\"{folder_id:02d}\"][f\"{sample_id:04d\n",
        "        # pose_file = os.path.join(self.dataset_root, f\"{folder_id:02d}_gt.yml\")\n",
        "\n",
        "        # Load the ground truth poses from the gt.yml file\n",
        "        # with open(pose_file, 'r') as f:\n",
        "            # pose_data = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "        # The pose data is a dictionary where each key corresponds to a frame with pose info\n",
        "        # We assume sample_id corresponds to the key in pose_data\n",
        "        # if sample_id not in pose_data:\n",
        "            # raise KeyError(f\"Sample ID {sample_id} not found in gt.yml for folder {folder_id}.\")\n",
        "\n",
        "        # for pose in pose_data[sample_id]: # There can be more than one pose per sample, but take the one of label=folder_id\n",
        "            # Extract translation and rotation\n",
        "            # if (int(pose['obj_id']) == int(folder_id)):\n",
        "        translation = np.array(pose['cam_t_m2c'], dtype=np.float32)/1000.0  # [3] ---> (x,y,z)\n",
        "        rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
        "        quaternion = np.array(pose['quaternion'], dtype=np.float32)  # [4] ---> quaternion\n",
        "        bbox_base = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
        "        # bbox is top left corner and width and height info, YOLO needs center coordinates and width and height\n",
        "        # x_min, y_min, width, height = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
        "        cropped_img = self.load_cropped_image(os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\", f\"{sample_id:04d}.png\"), np.array(pose['obj_bb'], dtype=np.float32))\n",
        "\n",
        "                # Compute initial center\n",
        "                # x_center = x_min + width / 2\n",
        "                # y_center = y_min + height / 2\n",
        "\n",
        "                # Clip center to image bounds and adjust width/height accordingly\n",
        "                # if x_center < 0:\n",
        "                #     width += 2 * x_center  # x_center is negative, subtract its absolute value * 2 from width\n",
        "                #     x_center = 0\n",
        "                # elif x_center > IMG_WIDTH:\n",
        "                #     width -= 2 * (x_center - IMG_WIDTH)\n",
        "                #     x_center = IMG_WIDTH\n",
        "\n",
        "                # # if y_center<0 take the max(0,y_center), if y_center>IMG_WIDTH take min(IMG_WIDTH, y_center)\n",
        "                # if y_center < 0:\n",
        "                #     height += 2 * y_center\n",
        "                #     y_center = 0\n",
        "                # elif y_center > IMG_HEIGHT:\n",
        "                #     height -= 2 * (y_center - IMG_HEIGHT)\n",
        "                #     y_center = IMG_HEIGHT\n",
        "\n",
        "                # Ensure width and height are not negative\n",
        "                # Caso in cui la bounding box sia completamente al di fuori dell'immagine (non dovrebbe mai capitare)\n",
        "                # width = max(0, width)\n",
        "                # height = max(0, height)\n",
        "                # store coordinates of the center and width and height of the bounding box normalized to the\n",
        "                # image width=640 pixels and height=480 pixels\n",
        "                # bbox_YOLO = np.array([x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT], dtype=np.float32)\n",
        "\n",
        "        obj_id = np.array(pose['obj_id'], dtype=np.float32) # [1] ---> label\n",
        "                # break\n",
        "        # BISOGNA AGGIUNGERE bbox_YOLO\n",
        "        return cropped_img, translation, rotation, quaternion, bbox_base, obj_id\n",
        "\n",
        "    def __len__(self):\n",
        "        #Return the total number of samples in the selected split.\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Load a dataset sample.\n",
        "        folder_id, sample_id = self.samples[idx]\n",
        "\n",
        "        # Load the correct camera intrinsics and object info for this folder\n",
        "        # camera_intrinsics, objects_info = self.load_config(folder_id)\n",
        "\n",
        "        img_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"rgb/{sample_id:04d}.png\")\n",
        "        # depth_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"depth/{sample_id:04d}.png\")\n",
        "        # mask_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"mask/{sample_id:04d}.png\")\n",
        "\n",
        "        img = self.load_image(img_path)\n",
        "        # depth = self.load_depth(depth_path)\n",
        "        # mask = self.load_mask(mask_path)\n",
        "\n",
        "        # mask_binary = mask != 0\n",
        "        # masked_depth = np.where(mask_binary, depth, 0)\n",
        "\n",
        "        # pointcloud = self.depth_to_pointcloud(masked_depth, camera_intrinsics),\n",
        "\n",
        "        # BISOGNA AGGIUNGERE bbox_YOLO\n",
        "        cropped_img, translation, rotation, quaternion, bbox_base, obj_id = self.load_6d_pose(folder_id, sample_id)\n",
        "\n",
        "        #Dictionary with all the data\n",
        "        return {\n",
        "            \"rgb\": img,\n",
        "            \"cropped_img\": cropped_img.to(device),\n",
        "            # \"depth\": torch.tensor(depth, dtype=torch.float32),\n",
        "            # \"pointcloud\": torch.tensor(pointcloud, dtype=torch.float32),\n",
        "            # \"camera_intrinsics\": camera_intrinsics,\n",
        "            # \"objects_info\": objects_info,\n",
        "            \"translation\": torch.tensor(translation).to(device),\n",
        "            \"rotation\": torch.tensor(rotation).to(device),\n",
        "            \"quaternion\": torch.tensor(quaternion).to(device),\n",
        "            \"bbox_base\": torch.tensor(bbox_base).to(device),\n",
        "            # \"bbox_YOLO\": torch.tensor(bbox_YOLO),\n",
        "            \"obj_id\": torch.tensor(obj_id).to(device),\n",
        "            \"sample_id\": torch.tensor(self.samples[idx]).to(device)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dec46f9",
      "metadata": {
        "id": "0dec46f9"
      },
      "outputs": [],
      "source": [
        "dataset_root = \"./Linemod_preprocessed/\"\n",
        "\n",
        "train_dataset = CustomDataset(dataset_root, split=\"train\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "\n",
        "val_dataset = CustomDataset(dataset_root, split=\"validation\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "test_dataset = CustomDataset(dataset_root, split=\"test\")\n",
        "print(f\"Testing samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J_nF3utavmIA",
      "metadata": {
        "id": "J_nF3utavmIA"
      },
      "source": [
        "### Visualization 2D - 3D - 2D con y e y ribaltata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ygOfbYD1uaDp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ygOfbYD1uaDp",
        "outputId": "56782d13-7bce-465c-b263-ba0fd419c51c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a0a7dbaf91a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmasked_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dtype:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Min depth:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "depth_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data\\\\01\\\\depth\\\\0000.png'\n",
        "mask_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data\\\\01\\\\mask\\\\0000.png'\n",
        "\n",
        "# Parametri intrinseci della camera LineMOD (dai tuoi dati)\n",
        "fx = 572.4114  # focal length x\n",
        "fy = 573.57043  # focal length y\n",
        "cx = 325.2611  # principal point x\n",
        "cy = 242.04899  # principal point y\n",
        "\n",
        "# Carica la depth map come immagine a 16 bit\n",
        "depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)  # mantiene dtype uint16\n",
        "mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "mask_binary = mask_img != 0\n",
        "\n",
        "masked_depth = np.where(mask_binary, depth_img, 0)\n",
        "\n",
        "print(\"Shape:\", depth_img.shape)\n",
        "print(\"Dtype:\", depth_img.dtype)\n",
        "print(\"Min depth:\", np.min(depth_img))\n",
        "print(\"Max depth:\", np.max(depth_img))\n",
        "print(\"Pixels = 0:\", np.sum(depth_img == 0))\n",
        "print(\"Total pixels:\", 480*640)\n",
        "\n",
        "def pixel_to_camera_coordinates(masked_depth, fx, fy, cx, cy):\n",
        "    \"\"\"\n",
        "    Converte coordinate pixel + depth in coordinate metriche 3D usando i parametri intrinseci\n",
        "\n",
        "    Formula di conversione:\n",
        "    X = (u - cx) * Z / fx\n",
        "    Y = (v - cy) * Z / fy\n",
        "    Z = depth\n",
        "\n",
        "    dove (u,v) sono coordinate pixel e Z è la profondità\n",
        "    \"\"\"\n",
        "    height, width = masked_depth.shape\n",
        "\n",
        "    # Trova tutti i pixel con profondità valida (non zero)\n",
        "    valid_mask = masked_depth > 0\n",
        "\n",
        "    # Crea griglia di coordinate pixel\n",
        "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
        "\n",
        "    # Estrai coordinate dei pixel validi\n",
        "    u_valid = u[valid_mask]  # coordinate u (pixel)\n",
        "    v_valid = v[valid_mask]  # coordinate v (pixel)\n",
        "    z_valid = masked_depth[valid_mask].astype(np.float32) / 1000.0  # profondità in metri\n",
        "\n",
        "    # Conversione da coordinate pixel a coordinate camera (tutte in metri)\n",
        "    x_meters = (u_valid - cx) * z_valid / fx\n",
        "    y_meters = (v_valid - cy) * z_valid / fy\n",
        "    z_meters = z_valid\n",
        "\n",
        "    return x_meters, y_meters, z_meters, u_valid, v_valid\n",
        "\n",
        "# Crea la point cloud 3D in coordinate metriche\n",
        "x_meters, y_meters, z_meters, u_pixels, v_pixels = pixel_to_camera_coordinates(masked_depth, fx, fy, cx, cy)\n",
        "\n",
        "print(f\"\\n=== Point Cloud 3D Info (Coordinate Metriche) ===\")\n",
        "print(f\"Numero di punti validi: {len(x_meters)}\")\n",
        "print(f\"Range X (metri): {x_meters.min():.3f} - {x_meters.max():.3f}\")\n",
        "print(f\"Range Y (metri): {y_meters.min():.3f} - {y_meters.max():.3f}\")\n",
        "print(f\"Range Z (metri): {z_meters.min():.3f} - {z_meters.max():.3f}\")\n",
        "\n",
        "print(f\"\\n=== Parametri Camera ===\")\n",
        "print(f\"fx: {fx}, fy: {fy}\")\n",
        "print(f\"cx: {cx}, cy: {cy}\")\n",
        "print(f\"Dimensioni immagine: {masked_depth.shape[1]}x{masked_depth.shape[0]} pixel\")\n",
        "\n",
        "# === Visualizzazione Comparativa ===\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Immagine depth originale\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Depth Original\", fontsize=12)\n",
        "plt.imshow(depth_img, cmap='gray')\n",
        "plt.colorbar(label='Depth (mm)')\n",
        "plt.xlabel('X (pixel)')\n",
        "plt.ylabel('Y (pixel)')\n",
        "\n",
        "# Masked Depth\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title(\"Masked Depth\", fontsize=12)\n",
        "plt.imshow(masked_depth, cmap='gray')\n",
        "plt.colorbar(label='Depth (mm)')\n",
        "plt.xlabel('X (pixel)')\n",
        "plt.ylabel('Y (pixel)')\n",
        "\n",
        "# Distribuzione punti in coordinate pixel (per confronto)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title(\"Point Distribution (Pixel Coords)\", fontsize=12)\n",
        "scatter_pixel = plt.scatter(u_pixels, v_pixels, c=z_meters, cmap='viridis', s=6, alpha=0.6)\n",
        "plt.colorbar(scatter_pixel, label='Depth (m)')\n",
        "plt.xlabel('X (pixel)')\n",
        "plt.ylabel('Y (pixel)')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# === COORDINATE METRICHE (LA PARTE IMPORTANTE) ===\n",
        "\n",
        "# Plot 3D principale - COORDINATE METRICHE\n",
        "ax1 = fig.add_subplot(2, 2, 4, projection='3d')\n",
        "ax1.set_title(\"3D Point Cloud\\n(X,Y,Z tutti in metri)\", fontsize=12)\n",
        "scatter3d = ax1.scatter(x_meters, y_meters, z_meters, c=z_meters, cmap='plasma', s=1, alpha=0.7)\n",
        "ax1.set_xlabel('X (m)')\n",
        "ax1.set_ylabel('Y (m)')\n",
        "ax1.set_zlabel('Z (m)')\n",
        "plt.colorbar(scatter3d, ax=ax1, shrink=0.5, aspect=5, label='Depth (m)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Visualizzazione 3D dettagliata con coordinate metriche ===\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Plot 3D con diversi angoli di vista\n",
        "angles = [(30, 45), (60, 30), (0, 0), (90, 90)]\n",
        "titles = [\"3D View (30°, 45°)\", \"3D View (60°, 30°)\", \"Front View (X-Y)\", \"Top View (X-Z)\"]\n",
        "\n",
        "for i, (elev, azim) in enumerate(angles):\n",
        "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
        "    scatter = ax.scatter(x_meters, y_meters, z_meters, c=z_meters, cmap='plasma', s=0.8, alpha=0.7)\n",
        "    ax.set_title(f\"{titles[i]} - Metric Coordinates\", fontsize=12)\n",
        "    ax.set_xlabel('X (m)')\n",
        "    ax.set_ylabel('Y (m)')\n",
        "    ax.set_zlabel('Z (m)')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "\n",
        "    if i == 0:  # Aggiungi colorbar solo al primo plot\n",
        "        plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5, label='Depth (m)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Salva i dati ===\n",
        "# Salva la point cloud in coordinate metriche\n",
        "point_cloud_metric = np.column_stack((x_meters, y_meters, z_meters))\n",
        "point_cloud_pixel = np.column_stack((u_pixels, v_pixels, z_meters))\n",
        "\n",
        "np.save('masked_depth_3d_metric_coords.npy', point_cloud_metric)\n",
        "np.save('masked_depth_3d_pixel_coords.npy', point_cloud_pixel)\n",
        "\n",
        "print(f\"\\n=== Salvataggio ===\")\n",
        "print(f\"Point cloud METRICA salvata in 'masked_depth_3d_metric_coords.npy'\")\n",
        "print(f\"Point cloud PIXEL salvata in 'masked_depth_3d_pixel_coords.npy'\")\n",
        "print(f\"Shape metrica: {point_cloud_metric.shape}\")\n",
        "print(f\"Shape pixel: {point_cloud_pixel.shape}\")\n",
        "print(f\"Formato metrico: [x_metri, y_metri, z_metri]\")\n",
        "print(f\"Formato pixel: [x_pixel, y_pixel, z_metri]\")\n",
        "\n",
        "# Mostra alcuni esempi di punti\n",
        "print(f\"\\nPrimi 10 punti (COORDINATE METRICHE):\")\n",
        "print(f\"[X_m, Y_m, Z_m]\")\n",
        "for i in range(min(10, len(point_cloud_metric))):\n",
        "    x, y, z = point_cloud_metric[i]\n",
        "    print(f\"[{x:+.3f}, {y:+.3f}, {z:.3f}]\")\n",
        "\n",
        "print(f\"\\nPrimi 10 punti (COORDINATE PIXEL + Z metri):\")\n",
        "print(f\"[X_px, Y_px, Z_m]\")\n",
        "for i in range(min(10, len(point_cloud_pixel))):\n",
        "    x, y, z = point_cloud_pixel[i]\n",
        "    print(f\"[{x:3.0f}, {y:3.0f}, {z:.3f}]\")\n",
        "\n",
        "# Calcola e mostra il centroide dell'oggetto\n",
        "centroid = np.mean(point_cloud_metric, axis=0)\n",
        "print(f\"\\nCentroide dell'oggetto (coordinate metriche):\")\n",
        "print(f\"X: {centroid[0]:.3f} m\")\n",
        "print(f\"Y: {centroid[1]:.3f} m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BJQZCPauk0c",
      "metadata": {
        "id": "0BJQZCPauk0c"
      },
      "outputs": [],
      "source": [
        "# Carica la point cloud in coordinate metriche\n",
        "# Se hai già salvato la point cloud\n",
        "# point_cloud_metric = np.load('masked_depth_3d_metric_coords.npy')\n",
        "\n",
        "# Altrimenti puoi usare direttamente x_meters, y_meters, z_meters\n",
        "point_cloud_metric = np.column_stack((x_meters, y_meters, z_meters))\n",
        "\n",
        "# Crea un oggetto PointCloud di open3d\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "pcd.points = o3d.utility.Vector3dVector(point_cloud_metric)\n",
        "\n",
        "# (Opzionale) Aggiungi colore in base alla profondità (Z)\n",
        "colors = (z_meters - z_meters.min()) / (z_meters.max() - z_meters.min())\n",
        "colors = plt.cm.plasma(colors)[:, :3]  # Usa colormap matplotlib, prendi solo RGB\n",
        "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "# Visualizza la point cloud con viewer 3D interattivo\n",
        "o3d.visualization.draw_geometries([pcd], window_name=\"3D Point Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tUWIaNt_vh8D",
      "metadata": {
        "id": "tUWIaNt_vh8D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "depth_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data\\\\01\\\\depth\\\\0000.png'\n",
        "rgb_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data\\\\01\\\\rgb\\\\0000.png'\n",
        "mask_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data\\\\01\\\\mask\\\\0000.png'\n",
        "\n",
        "# === Carica dati ===\n",
        "# (Modifica il path per caricare la tua immagine e profondità)\n",
        "rgb = cv2.imread(rgb_path)\n",
        "rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
        "depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32) / 1000.0  # in metri\n",
        "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Carica parametri intrinseci della camera\n",
        "fx, fy = 572.4114, 573.57043\n",
        "cx, cy = 325.2611, 242.04899\n",
        "\n",
        "# === Estrai pixel validi ===\n",
        "v, u = np.where(mask > 0)\n",
        "z = depth[v, u]\n",
        "x = (u - cx) * z / fx\n",
        "y = (v - cy) * z / fy\n",
        "\n",
        "# === Point cloud senza inversione Y ===\n",
        "pc_natural = np.column_stack((x, y, z))\n",
        "\n",
        "# === Point cloud con inversione Y ===\n",
        "y_inv = -y\n",
        "pc_inverted = np.column_stack((x, y_inv, z))\n",
        "\n",
        "# === Proiezione 3D -> 2D ===\n",
        "def project_to_2d(x, y, z):\n",
        "    u_proj = (x * fx) / z + cx\n",
        "    v_proj = (y * fy) / z + cy\n",
        "    return u_proj, v_proj\n",
        "\n",
        "u_nat, v_nat = project_to_2d(pc_natural[:,0], pc_natural[:,1], pc_natural[:,2])\n",
        "u_inv, v_inv = project_to_2d(pc_inverted[:,0], pc_inverted[:,1], pc_inverted[:,2])\n",
        "\n",
        "# === Visualizza i risultati ===\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
        "\n",
        "# Immagine originale\n",
        "axs[0].imshow(rgb)\n",
        "axs[0].set_title('Immagine RGB Originale')\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Proiezione senza inversione\n",
        "axs[1].imshow(rgb)\n",
        "axs[1].scatter(u_nat, v_nat, s=1, c='red')\n",
        "axs[1].set_title('Proiezione senza inversione Y')\n",
        "axs[1].axis('off')\n",
        "\n",
        "# Proiezione con inversione Y\n",
        "axs[2].imshow(rgb)\n",
        "axs[2].scatter(u_inv, v_inv, s=1, c='blue')\n",
        "axs[2].set_title('Proiezione con inversione Y')\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MuIkxlEHvZ9z",
      "metadata": {
        "id": "MuIkxlEHvZ9z"
      },
      "outputs": [],
      "source": [
        "root_path = r'.\\\\datasets\\\\linemod\\\\DenseFusion\\\\Linemod_preprocessed\\\\data'\n",
        "\n",
        "def extract_cam_K_from_info(info):\n",
        "    # info è un dict con chiavi come '0', '1', ... ciascuna con un dict che contiene 'cam_K'\n",
        "    cam_K_list = []\n",
        "    for key, val in info.items():\n",
        "        if \"cam_K\" in val:\n",
        "            cam_K_list.append(val[\"cam_K\"])\n",
        "    return cam_K_list\n",
        "\n",
        "all_cam_Ks = []  # lista di tutte le cam_K trovate in tutti i file\n",
        "\n",
        "for obj_class in sorted(os.listdir(root_path)):\n",
        "    class_path = os.path.join(root_path, obj_class)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    info_path = os.path.join(class_path, \"info.yml\")\n",
        "    if not os.path.exists(info_path):\n",
        "        continue\n",
        "\n",
        "    with open(info_path, 'r') as f:\n",
        "        info = yaml.safe_load(f)\n",
        "\n",
        "    cam_Ks = extract_cam_K_from_info(info)\n",
        "    if len(cam_Ks) == 0:\n",
        "        print(f\"[Warning] Nessun cam_K trovato in {obj_class}/info.yml\")\n",
        "        continue\n",
        "\n",
        "    # Aggiungi tutti i cam_K di questo file alla lista globale\n",
        "    all_cam_Ks.extend(cam_Ks)\n",
        "\n",
        "# Ora controlla che tutti i cam_K siano identici\n",
        "if len(all_cam_Ks) == 0:\n",
        "    print(\"Nessun cam_K trovato in nessun file.\")\n",
        "else:\n",
        "    first_cam_K = all_cam_Ks[0]\n",
        "    all_equal = all(cam_K == first_cam_K for cam_K in all_cam_Ks)\n",
        "\n",
        "    if all_equal:\n",
        "        print(\"Tutti i cam_K sono uguali in tutti i file info.yml\")\n",
        "    else:\n",
        "        print(\"Ci sono cam_K diversi tra i file info.yml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3276b3ba",
      "metadata": {
        "id": "3276b3ba"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869154a1",
      "metadata": {
        "id": "869154a1"
      },
      "source": [
        "Structure the data such that\n",
        "```\n",
        "datasets/\n",
        "├── data.yaml\n",
        "│\n",
        "├── train/\n",
        "│   ├── images/\n",
        "│   │\n",
        "│   └── labels/\n",
        "│  \n",
        "├── val/\n",
        "│\n",
        "└── test/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d182a1",
      "metadata": {
        "id": "67d182a1"
      },
      "outputs": [],
      "source": [
        "# divide the dataset into training, validation and testing set\n",
        "train_samples = train_dataset.get_samples_id()\n",
        "validation_samples = val_dataset.get_samples_id()\n",
        "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6253d1e",
      "metadata": {
        "id": "a6253d1e"
      },
      "source": [
        "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
        "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
        "\n",
        "The working directory is in the ```DenseFusion```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b0f3e4",
      "metadata": {
        "id": "97b0f3e4"
      },
      "outputs": [],
      "source": [
        "# create a folder to contain the dataset for YOLO model\n",
        "os.makedirs(\"./datasets/linemod/YOLO/datasets\", exist_ok=True)\n",
        "\n",
        "# count number of distinct classes\n",
        "number_classes = 0\n",
        "class_names = []\n",
        "for el in os.scandir(\"./datasets/linemod/DenseFusion/Linemod_preprocessed/data\"):\n",
        "    # if entry is a directory and its name is an integer value (this is just to avoid counting non directories or other directories)\n",
        "    if (el.is_dir() and el.name.isdigit()):\n",
        "        class_names.append(el.name)\n",
        "        number_classes += 1\n",
        "\n",
        "# get string of all class names\n",
        "class_names.sort() # sort the names\n",
        "names = \"[\"\n",
        "for index, el in enumerate(class_names):\n",
        "    # if last element don't add comma\n",
        "    if index == number_classes-1:\n",
        "        names += f\"'{str(el)}'\"\n",
        "    else:\n",
        "        names += f\"'{str(el)}',\"\n",
        "names += \"]\"\n",
        "\n",
        "# create data.yaml (as class names use ids of the folder)\n",
        "content = f\"\"\"train: ./train/images\n",
        "val: ./val/images\n",
        "test: ./test/images\n",
        "\n",
        "nc: {number_classes}\n",
        "names: {names}\"\"\"\n",
        "# write to file\n",
        "with open(\"./datasets/linemod/YOLO/datasets/data.yaml\", \"w\") as fout:\n",
        "    fout.write(content)\n",
        "fout.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3ba3d0",
      "metadata": {
        "id": "3b3ba3d0"
      },
      "source": [
        "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4a0976",
      "metadata": {
        "id": "6d4a0976"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to have easily access to the index\n",
        "index_dict = dict()\n",
        "for index, el in enumerate(class_names):\n",
        "    index_dict[int(el)] = index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca3c7b5",
      "metadata": {
        "id": "8ca3c7b5"
      },
      "source": [
        "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767b3d4b",
      "metadata": {
        "id": "767b3d4b"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# create images and labels\n",
        "# dataset = [train_samples, validation_samples, test_samples]\n",
        "folder_names = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "# count also the number of instances of each class\n",
        "classes = range(0, number_classes)\n",
        "counter_df = pd.DataFrame()\n",
        "for idx in range(3):\n",
        "    if idx == 0:\n",
        "        dataset = train_samples\n",
        "    elif idx == 1:\n",
        "        dataset = validation_samples\n",
        "    else:\n",
        "        dataset = test_samples\n",
        "    print(f\"------------------------------{folder_names[idx].upper()}------------------------------\")\n",
        "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/images\", exist_ok=True)\n",
        "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/labels\", exist_ok=True)\n",
        "    classCount = {temp_class: 0 for temp_class in range(0,number_classes)} # initialize dictionary for counting\n",
        "    total = 0 # used to normalize count\n",
        "    for el in tqdm(dataset, desc=\"Moving...\"):\n",
        "        if int(el[0]) == 2:\n",
        "            # el is (folderId, sampleId)\n",
        "            _, _, bbox, obj_id = train_dataset.load_6d_pose(el[0], el[1])\n",
        "            # copy image into the new folder\n",
        "            # avoid overwriting the files, so concat also the name of the folderId to the destination file\n",
        "            shutil.copy(f\"./Linemod_preprocessed/data/{el[0]:02d}/rgb/{el[1]:04d}.png\", f\"../YOLO/datasets/{folder_names[idx]}/images/{el[0]:02d}_{el[1]:04d}.png\")\n",
        "            # create label file with the same name as the image\n",
        "            with open(f\"../YOLO/datasets/{folder_names[idx]}/labels/{el[0]:02d}_{el[1]:04d}.txt\", \"w\") as fout:\n",
        "                # bbox is a list of values in the form of [x_center, y_center, width, height] and obj_id a list of class labels\n",
        "                # where each label is in the format 01-15\n",
        "                content = \"\"\n",
        "                classCount[index_dict[int(obj_id)]] += 1\n",
        "                total += 1\n",
        "                content += f\"{index_dict[int(obj_id)]} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\"\n",
        "                fout.write(content)\n",
        "            fout.close()\n",
        "\n",
        "    # store in the dataframe\n",
        "    values = pd.array(list(classCount.values()))/total\n",
        "    counter_df[folder_names[idx]] = values.copy()\n",
        "\n",
        "# plot distribution of labels in training, validation and test set\n",
        "plt.grid(alpha=0.5)\n",
        "sns.lineplot(data=counter_df)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(range(0,number_classes))\n",
        "plt.savefig(f\"../../../images/YOLO_dataset_distribution.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ede843b",
      "metadata": {
        "id": "8ede843b"
      },
      "source": [
        "### Visualize data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b007448",
      "metadata": {
        "id": "3b007448"
      },
      "source": [
        "Visualize depth image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d3c02b",
      "metadata": {
        "id": "71d3c02b"
      },
      "outputs": [],
      "source": [
        "img_path = \"./Linemod_preprocessed/data/02/depth/0101.png\"\n",
        "img = Image.open(img_path)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944e99f0",
      "metadata": {
        "id": "944e99f0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# Plot image with bounding box\n",
        "\n",
        "# Load the ground truth poses from the gt.yml file\n",
        "with open(\"./Linemod_preprocessed/data/02/gt.yml\", 'r') as f:\n",
        "  pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
        "pose = pose_data[101][6] # access image 0 (start counting from 0) and get first object in that image (in case of multiple objects)\n",
        "\n",
        "bbox = np.array(pose['obj_bb'], dtype=np.float32) #[4]\n",
        "obj_id = np.array(pose['obj_id'], dtype=np.float32) #[1]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(img)\n",
        "\n",
        "# Create a rectangle patch\n",
        "rect = patches.Rectangle(\n",
        "    (bbox[0], bbox[1]),  # (x, y)\n",
        "    bbox[2],             # width\n",
        "    bbox[3],             # height\n",
        "    linewidth=2,\n",
        "    edgecolor='red',\n",
        "    facecolor='none'\n",
        ")\n",
        "\n",
        "# Add the rectangle to the plot\n",
        "ax.add_patch(rect)\n",
        "\n",
        "# Optionally add object ID label (write a bit above the top left corner)\n",
        "ax.text(bbox[0], bbox[1] - 20, f'ID: {int(obj_id)}', color='yellow', fontsize=12, backgroundcolor='black')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbde860",
      "metadata": {
        "id": "6bbde860"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# Plot image with bounding box\n",
        "\n",
        "# Load the ground truth poses from the gt.yml file\n",
        "with open(\"../YOLO/datasets/val/labels/02_0101.txt\", 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.split(\" \")\n",
        "        bbox = np.array(parts[1:5], dtype=np.float32) #[4]\n",
        "        obj_id = np.array(parts[0], dtype=np.float32) #[1]\n",
        "        if obj_id == 7.0:\n",
        "            x_center, y_center, width, height = bbox[0:4]\n",
        "            x_center = x_center*IMG_WIDTH\n",
        "            y_center = y_center*IMG_HEIGHT\n",
        "            width = width*IMG_WIDTH\n",
        "            height = height*IMG_HEIGHT\n",
        "            x_min = x_center-(width/2)\n",
        "            y_min = y_center-(height/2)\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.imshow(img)\n",
        "\n",
        "            # Create a rectangle patch\n",
        "            rect = patches.Rectangle(\n",
        "                (x_min, y_min),  # (x, y)\n",
        "                width,             # width\n",
        "                height,             # height\n",
        "                linewidth=2,\n",
        "                edgecolor='red',\n",
        "                facecolor='none'\n",
        "            )\n",
        "\n",
        "            # Add the rectangle to the plot\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Optionally add object ID label (write a bit above the top left corner)\n",
        "            ax.text(x_min, y_min - 20, f'ID: {int(obj_id)}', color='yellow', fontsize=10, backgroundcolor='black')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b4e4994",
      "metadata": {
        "id": "8b4e4994"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "print(f\"Training loader: {len(train_loader)}\")\n",
        "print(f\"Validation loader: {len(val_loader)}\")\n",
        "print(f\"Test loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ded634b",
      "metadata": {
        "id": "8ded634b"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Get only the first 1 batch\n",
        "train_subset_num_batches = 1\n",
        "val_subset_num_batches = 1\n",
        "test_subset_num_batches = 1\n",
        "train_subset = list(itertools.islice(train_loader, train_subset_num_batches))\n",
        "val_subset = list(itertools.islice(val_loader, val_subset_num_batches))\n",
        "test_subset = list(itertools.islice(test_loader, test_subset_num_batches))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa0b57c",
      "metadata": {
        "id": "9fa0b57c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Get one batch from the train loader (4 images)\n",
        "batch = next(iter(train_loader)) # it uses load_6d_pose, so one pose per object\n",
        "\n",
        "# Extract relevant data\n",
        "rgb_images = batch[\"rgb\"]         # (B, 3, H, W)\n",
        "bboxes = batch[\"bbox\"]            # (B, 4) in pixel coords: x_min, y_min, x_max, y_max\n",
        "obj_ids = batch[\"obj_id\"]         # (B,)\n",
        "\n",
        "# Convert to numpy and rearrange channels\n",
        "rgb_images = rgb_images.permute(0, 2, 3, 1).numpy()  # (B, H, W, 3)\n",
        "bboxes = bboxes.numpy()\n",
        "obj_ids = obj_ids.numpy()\n",
        "\n",
        "# Plot settings\n",
        "batch_size = rgb_images.shape[0]\n",
        "cols = min(4, batch_size)\n",
        "rows = (batch_size + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(batch_size):\n",
        "    ax = axes[i]\n",
        "    img = rgb_images[i]\n",
        "    # each element is [x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT]\n",
        "    x_center, y_center, width, height = bboxes[i]\n",
        "    # remove normalization\n",
        "    x_center = x_center*IMG_WIDTH\n",
        "    y_center = y_center*IMG_HEIGHT\n",
        "    width = width*IMG_WIDTH\n",
        "    height = height*IMG_HEIGHT\n",
        "    x_min = x_center-(width/2)\n",
        "    y_min = y_center-(height/2)\n",
        "    obj_id = obj_ids[i]\n",
        "\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Sample {i}\")\n",
        "\n",
        "    # Draw bounding box\n",
        "    rect = patches.Rectangle(\n",
        "        (x_min, y_min),   # (x_min, y_min)\n",
        "        width,              # width\n",
        "        height,              # height\n",
        "        linewidth=2,\n",
        "        edgecolor='red',\n",
        "        facecolor='none'\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Add object ID as label\n",
        "    ax.text(\n",
        "        x_min,\n",
        "        y_min - 10,\n",
        "        f'ID: {int(obj_id)}',\n",
        "        color='yellow',\n",
        "        fontsize=10,\n",
        "        backgroundcolor='black'\n",
        "    )\n",
        "\n",
        "# Hide unused axes if batch_size < cols * rows\n",
        "for j in range(batch_size, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71678e8",
      "metadata": {
        "id": "a71678e8"
      },
      "outputs": [],
      "source": [
        "# !pwd\n",
        "# %cd ../../test/labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f520edce",
      "metadata": {
        "id": "f520edce"
      },
      "outputs": [],
      "source": [
        "# path = !pwd\n",
        "# %cd datasets/linemod/YOLO/datasets/test/labels/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f399ac90",
      "metadata": {
        "id": "f399ac90"
      },
      "outputs": [],
      "source": [
        "# total = 0\n",
        "# for l in os.listdir(path):\n",
        "    # with open(l, 'r') as f:\n",
        "        # lines = f.readlines()\n",
        "        # for line in lines:\n",
        "            # parts = line.split(\" \")\n",
        "            # x_center = parts[1]\n",
        "            # y_center = parts[2]\n",
        "            # if float(x_center) == 0.0 or float(y_center) == 0.0 or float(x_center) == 1.0 or float(y_center) == 1.0:\n",
        "                # print(l)\n",
        "                # total += 1\n",
        "                # break\n",
        "# print(total)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s1sD-RYfaBiS",
      "metadata": {
        "id": "s1sD-RYfaBiS"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80313348",
      "metadata": {
        "id": "80313348"
      },
      "source": [
        "## Training Object Detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf706c86",
      "metadata": {
        "id": "bf706c86"
      },
      "source": [
        "Check if CUDA available, otherwise try with MPS and then CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B5WC-oSLGF1p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5WC-oSLGF1p",
        "outputId": "9c41a3ce-2480-43ae-cbf4-db9f31233c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation\n"
          ]
        }
      ],
      "source": [
        "# %cd ./drive/MyDrive/6D_pose_estimation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CCFXPX0SVyt1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCFXPX0SVyt1",
        "outputId": "8145a485-3bac-43c4-b7a4-e6eb8d4e8dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation\n"
          ]
        }
      ],
      "source": [
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pulSmociGfcg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pulSmociGfcg",
        "outputId": "949b1192-a0d6-4cf5-addd-f2218eb76c4a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_path = !pwd\n",
        "path = old_path[0]\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K6_R2TsCGiq9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6_R2TsCGiq9",
        "outputId": "ee8aa573-1f96-484f-ea2a-f970c98a4fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: ./train/images\n",
            "val: ./val/images\n",
            "test: ./test/images\n",
            "\n",
            "nc: 13\n",
            "names: [\"01\",\"02\",\"04\",\"05\",\"06\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"]\n"
          ]
        }
      ],
      "source": [
        "# Ricordarsi di cambiare il yaml e il percorso interno in base all'account in cui siete\n",
        "# with open(\"./datasets/linemod/YOLO/datasets/data.yaml\",\"r\") as f:\n",
        "  # f= f.read()\n",
        "  # print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Abg6c53ONiO7",
      "metadata": {
        "id": "Abg6c53ONiO7"
      },
      "source": [
        "## Training YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YtE-i6fvY3kH",
      "metadata": {
        "id": "YtE-i6fvY3kH"
      },
      "outputs": [],
      "source": [
        "model_path = f\"./checkpoints/yolo11n.pt\"\n",
        "model = YOLO(model_path)\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "IMG_SIZE = 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03517373",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "03517373",
        "outputId": "b15c451c-0225-4ebd-89b9-03be3e42d9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/data.yaml, degrees=120, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.4, hsv_s=0.4, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=./checkpoints/yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0001, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=20, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 14.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=80 with nc=13\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    433207  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11n summary: 181 layers, 2,592,375 parameters, 2,592,359 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 63.9±135.5 ms, read: 1.0±0.3 MB/s, size: 460.5 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/train/labels... 11060 images, 0 backgrounds, 0 corrupt: 100%|██████████| 11060/11060 [11:29<00:00, 16.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.7±0.0 ms, read: 1.7±0.2 MB/s, size: 437.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/val/labels... 2370 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2370/2370 [02:19<00:00, 16.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/val/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/50      8.47G      1.754      4.115      1.589         97        640: 100%|██████████| 173/173 [01:07<00:00,  2.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:11<00:00,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.757      0.734      0.837      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/50         9G      1.459       1.94      1.279        109        640: 100%|██████████| 173/173 [01:01<00:00,  2.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.808      0.607      0.772      0.352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/50         9G       1.39      1.396      1.243        109        640: 100%|██████████| 173/173 [01:01<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.837      0.799      0.906        0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/50      9.02G      1.319      1.105      1.217        106        640: 100%|██████████| 173/173 [01:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.877      0.886      0.956      0.486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/50      9.03G       1.24     0.9274      1.177        104        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370       0.93      0.913      0.974      0.665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/50      9.04G      1.181     0.8249      1.154         99        640: 100%|██████████| 173/173 [01:03<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.886      0.872      0.933      0.663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/50      9.06G      1.139     0.7595       1.13         90        640: 100%|██████████| 173/173 [01:02<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.923      0.886      0.945      0.645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/50      9.07G      1.113     0.7185      1.121         82        640: 100%|██████████| 173/173 [01:03<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.932      0.947      0.976      0.718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/50      9.09G      1.077     0.6762      1.103         98        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.953      0.947      0.986      0.758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/50       9.1G      1.069     0.6574        1.1         94        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370       0.98      0.984      0.993      0.745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/50      9.12G      1.053     0.6348      1.096        102        640: 100%|██████████| 173/173 [01:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.983      0.983      0.994      0.773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/50      9.13G      1.031     0.6173      1.086         98        640: 100%|██████████| 173/173 [01:02<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.984      0.994      0.995      0.757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/50      9.14G      1.001     0.5977      1.075        103        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.958      0.951      0.982      0.782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/50      9.16G     0.9917     0.5876      1.065         99        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.982      0.977      0.994      0.778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/50      9.17G     0.9726     0.5656      1.057         85        640: 100%|██████████| 173/173 [01:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.993      0.998      0.995      0.783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/50      9.19G     0.9585     0.5554      1.055         94        640: 100%|██████████| 173/173 [01:02<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.995      0.999      0.995      0.774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/50       9.2G      0.948     0.5401       1.05         99        640: 100%|██████████| 173/173 [01:02<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.996      0.991      0.994      0.815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/50      9.22G     0.9377     0.5314      1.041         96        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.997      0.998      0.995      0.808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/50      9.23G     0.9253     0.5251       1.04        107        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.997      0.999      0.995      0.773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/50      9.25G      0.917     0.5204      1.038         86        640: 100%|██████████| 173/173 [01:01<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.997      0.994      0.995      0.801\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      21/50      9.26G     0.9023     0.5069      1.032         99        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.992      0.991      0.994      0.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      22/50      9.28G     0.8948     0.5002       1.03         91        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998          1      0.995      0.826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/50      9.29G     0.8811     0.4889      1.021        112        640: 100%|██████████| 173/173 [01:03<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998          1      0.995      0.837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/50      9.31G     0.8759     0.4842      1.018         91        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998          1      0.995      0.835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/50      9.32G     0.8671     0.4766      1.016         99        640: 100%|██████████| 173/173 [01:03<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998      0.999      0.995      0.841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/50      9.34G     0.8542     0.4717      1.012         96        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/50      9.35G     0.8429     0.4615      1.006         93        640: 100%|██████████| 173/173 [01:03<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998          1      0.995      0.829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/50      9.37G     0.8479     0.4665      1.007        106        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/50      9.38G     0.8277     0.4486     0.9996         96        640: 100%|██████████| 173/173 [01:02<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/50       9.4G     0.8186     0.4434     0.9975        104        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998      0.999      0.995      0.852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/50      9.41G     0.8114     0.4345     0.9916         92        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/50      9.43G     0.8091     0.4351     0.9912         95        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      33/50      9.44G     0.7949     0.4258     0.9888         83        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      34/50      9.46G     0.7917     0.4223     0.9855         99        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999      0.999      0.995      0.853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      35/50      9.47G     0.7828     0.4136     0.9833         93        640: 100%|██████████| 173/173 [01:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999      0.999      0.995       0.86\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      36/50      9.48G      0.775     0.4121      0.979         98        640: 100%|██████████| 173/173 [01:03<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      37/50       9.5G      0.767     0.4033     0.9759        101        640: 100%|██████████| 173/173 [01:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      38/50      9.51G     0.7667     0.4092     0.9784        109        640: 100%|██████████| 173/173 [01:03<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      39/50      9.53G     0.7561     0.3988     0.9743         90        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      40/50      9.54G     0.7464     0.3908     0.9672         98        640: 100%|██████████| 173/173 [01:02<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      41/50      9.56G     0.6614     0.2729     0.9123         52        640: 100%|██████████| 173/173 [01:06<00:00,  2.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      42/50      9.57G     0.6379     0.2625     0.9032         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      43/50      9.59G     0.6243     0.2587     0.8971         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995       0.86\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      44/50       9.6G     0.6148     0.2548     0.8961         52        640: 100%|██████████| 173/173 [00:59<00:00,  2.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995       0.87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      45/50      9.62G     0.6046     0.2488     0.8918         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      46/50      9.63G     0.5965     0.2446     0.8837         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      47/50      9.65G     0.5908     0.2416      0.883         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      48/50      9.66G     0.5816     0.2384     0.8791         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.877\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      49/50      9.68G     0.5717     0.2338      0.876         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      50/50      9.69G     0.5657     0.2314     0.8764         52        640: 100%|██████████| 173/173 [01:00<00:00,  2.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:10<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "50 epochs completed in 1.022 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,584,687 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:13<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.998          1      0.995      0.867\n",
            "                    01        186        186      0.995          1      0.995      0.874\n",
            "                    02        182        182      0.999          1      0.995      0.855\n",
            "                    04        180        180      0.999          1      0.995      0.865\n",
            "                    05        180        180      0.999          1      0.995      0.891\n",
            "                    06        177        177      0.999          1      0.995      0.867\n",
            "                    08        178        178          1          1      0.995      0.873\n",
            "                    09        188        188      0.996          1      0.995      0.879\n",
            "                    10        188        188      0.999          1      0.995       0.88\n",
            "                    11        183        183      0.995      0.999      0.995      0.804\n",
            "                    12        185        185      0.999          1      0.995      0.872\n",
            "                    13        173        173      0.999          1      0.995      0.866\n",
            "                    14        184        184      0.999          1      0.995      0.869\n",
            "                    15        186        186      0.999          1      0.995      0.879\n",
            "Speed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Devi essere in 6D\n",
        "results = model.train(\n",
        "        data=f\"./datasets/linemod/YOLO/datasets/data.yaml\",\n",
        "        epochs=epochs,\n",
        "        batch=batch_size,\n",
        "        device=device,\n",
        "        imgsz=IMG_SIZE,\n",
        "        augment=True,\n",
        "        flipud=0.5,\n",
        "        fliplr=0.5,\n",
        "        hsv_h=0.4,\n",
        "        hsv_s=0.4,\n",
        "        hsv_v=0.4,\n",
        "        degrees=120,\n",
        "        translate=0.1,\n",
        "        scale=0.5,\n",
        "        shear=20,\n",
        "        perspective=0.0001,\n",
        "        exist_ok=True,\n",
        "        patience=5, #number of epoch to wait without improvement in validation metrics before early stopping the train. Helps prevent overfitting.\n",
        "        dropout=0.3\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c08d2b",
      "metadata": {
        "id": "44c08d2b"
      },
      "source": [
        "Copy model file to ```checkpoints```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2327518a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2327518a",
        "outputId": "dc23d1a1-9b5b-4894-f805-dc2bf870d42b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/checkpoints/best.pt'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.copy(f\"./datasets/linemod/YOLO/runs/detect/train/weights/best.pt\", f\"./checkpoints/best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NUExAwEnOeBe",
      "metadata": {
        "id": "NUExAwEnOeBe"
      },
      "source": [
        "## Testing YOLO best.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wnpsz8bm8VpZ",
      "metadata": {
        "id": "Wnpsz8bm8VpZ"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(f\"./checkpoints/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k3HGcK_LOnjp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3HGcK_LOnjp",
        "outputId": "35a9b038-b0c3-4cba-d681-4a282de044ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11n summary (fused): 100 layers, 2,584,687 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.6±0.0 ms, read: 1.3±0.3 MB/s, size: 467.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/test/labels... 2370 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2370/2370 [03:07<00:00, 12.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1clSdIvJJw8QuywtOyjjWKwogqb-DdNZh/6D_pose_estimation/YOLO/datasets/test/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 38/38 [00:14<00:00,  2.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       2370       2370      0.999          1      0.995      0.877\n",
            "                    01        185        185      0.999          1      0.995      0.864\n",
            "                    02        182        182      0.999          1      0.995      0.866\n",
            "                    04        180        180      0.999          1      0.995      0.879\n",
            "                    05        179        179      0.999          1      0.995      0.914\n",
            "                    06        177        177      0.999          1      0.995      0.882\n",
            "                    08        178        178      0.999          1      0.995      0.882\n",
            "                    09        188        188      0.999          1      0.995      0.898\n",
            "                    10        188        188          1      0.998      0.995        0.9\n",
            "                    11        183        183          1          1      0.995      0.812\n",
            "                    12        186        186      0.999          1      0.995      0.868\n",
            "                    13        173        173      0.999          1      0.995      0.865\n",
            "                    14        184        184      0.999          1      0.995      0.892\n",
            "                    15        187        187      0.999          1      0.995      0.884\n",
            "Speed: 0.4ms preprocess, 0.6ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "results = model.val(\n",
        "        data=f\"./datasets/linemod/YOLO/datasets/data.yaml\",\n",
        "        epochs=epochs,\n",
        "        batch=batch_size,\n",
        "        imgsz=IMG_SIZE,\n",
        "        device=device,\n",
        "        split=\"test\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oVvjDjLdU1O4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVvjDjLdU1O4",
        "outputId": "3c15b086-ef0b-4ab1-9767-fbd0e937ade1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics Solutions: ✅ {'source': None, 'model': './checkpoints/best.pt', 'classes': None, 'show_conf': True, 'show_labels': True, 'region': None, 'colormap': 21, 'show_in': True, 'show_out': True, 'up_angle': 145.0, 'down_angle': 90, 'kpts': [6, 8, 10], 'analytics_type': 'line', 'figsize': (12.8, 7.2), 'blur_ratio': 0.5, 'vision_point': (20, 20), 'crop_dir': './YOLO/datasets/val/crop/', 'json_file': None, 'line_width': 2, 'records': 5, 'fps': 30.0, 'max_hist': 5, 'meter_per_pixel': 0.05, 'max_speed': 120, 'show': True, 'iou': 0.7, 'conf': 0.25, 'device': None, 'max_det': 300, 'half': False, 'tracker': 'botsort.yaml', 'verbose': True, 'data': 'images'}\n",
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "WARNING ⚠️ show=True disabled for crop solution, results will be saved in the directory named: ./YOLO/datasets/val/crop/\n"
          ]
        }
      ],
      "source": [
        "# from ultralytics import solutions, INUTILE\n",
        "# cropper = solutions.ObjectCropper(show=True, model= \"./checkpoints/best.pt\", crop_dir = \"./datasets/linemod/YOLO/datasets/val/crop/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bYF6uywMj0Lr",
      "metadata": {
        "id": "bYF6uywMj0Lr"
      },
      "source": [
        "## PoseEstimation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Ga14jl3CwXC",
      "metadata": {
        "id": "9Ga14jl3CwXC"
      },
      "source": [
        "### Eseguita solo per sicurezza per avere i gt corretti e norm std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aof_xYeFCd_q",
      "metadata": {
        "id": "aof_xYeFCd_q"
      },
      "outputs": [],
      "source": [
        "for el in [\"01\",\"02\",\"04\",\"05\",\"06\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"]:\n",
        "  shutil.copy(f\"./datasets/linemod/YOLO/datasets_cropped/{el}_gt.yml\", f\"./datasets/linemod/YOLO/datasets_cropped_no_masked/{el}_gt.yml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YF9etKSMuLto",
      "metadata": {
        "id": "YF9etKSMuLto"
      },
      "outputs": [],
      "source": [
        "# Directory base\n",
        "base_dir = './datasets/linemod/DenseFusion/Linemod_preprocessed/data'\n",
        "subfolders = [f'{i:02d}' for i in range(1, 16)]\n",
        "\n",
        "# Trasformazione in tensor [C, H, W]\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Somme cumulative\n",
        "sum_rgb = torch.zeros(3)\n",
        "sum_sq_rgb = torch.zeros(3)\n",
        "n_pixels = 0\n",
        "\n",
        "# Loop su tutte le cartelle\n",
        "for subfolder in tqdm(subfolders, desc=\"Processing folders\"):\n",
        "    rgb_dir = os.path.join(base_dir, subfolder, 'rgb')\n",
        "    if not os.path.isdir(rgb_dir):\n",
        "        continue\n",
        "    for filename in os.listdir(rgb_dir):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            path = os.path.join(rgb_dir, filename)\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_tensor = to_tensor(img)  # shape: [3, H, W]\n",
        "\n",
        "            # Aggiungi alla somma\n",
        "            sum_rgb += img_tensor.sum(dim=[1, 2])\n",
        "            sum_sq_rgb += (img_tensor ** 2).sum(dim=[1, 2])\n",
        "            n_pixels += img_tensor.shape[1] * img_tensor.shape[2]\n",
        "\n",
        "# Calcolo finale\n",
        "mean = sum_rgb / n_pixels\n",
        "std = torch.sqrt((sum_sq_rgb / n_pixels) - (mean ** 2))\n",
        "\n",
        "print(f\"Mean per channel: {mean}\")\n",
        "print(f\"Std per channel: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BCVCG6kzC17y",
      "metadata": {
        "id": "BCVCG6kzC17y"
      },
      "source": [
        "### Partire da qui se non è necessario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZTsWAG6xrB9G",
      "metadata": {
        "id": "ZTsWAG6xrB9G"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Extract features from images using ResNet specified backbone\n",
        "        returns a tensor of shape (batch_size, feature_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone='resnet18', weights= None):\n",
        "        \"\"\"Import Resnet architecture pretrained\n",
        "\n",
        "        Args:\n",
        "            backbone (str, optional): Name of the Resnet model. Defaults to 'resnet18'.\n",
        "            weights (str, optional): Pretrained weights of the Resnet\n",
        "        \"\"\"\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.weights= weights\n",
        "\n",
        "        if backbone == 'resnet18':\n",
        "            self.backbone = models.resnet18(weights= \"ResNet18_Weights.IMAGENET1K_V1\" if self.weights==None else self.weights)\n",
        "            self.feature_dim = 512\n",
        "\n",
        "        elif backbone == 'resnet50':\n",
        "            self.backbone = models.resnet50(weights= \"ResNet50_Weights.IMAGENET1K_V2\"  if self.weights==None else self.weights)\n",
        "            self.feature_dim = 2048\n",
        "\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        features = self.adaptive_pool(features)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        return features\n",
        "\n",
        "    def get_feature_dim(self):\n",
        "        return self.feature_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eVU-XT9orCha",
      "metadata": {
        "id": "eVU-XT9orCha"
      },
      "outputs": [],
      "source": [
        "class PosePredictorModel(nn.Module):\n",
        "    \"\"\"After Resnet, add a second NN to predict translation and rotation\n",
        "        returns translation and rotation predictions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone='resnet18', hidden_dim=512):\n",
        "        \"\"\"Initialize model and import Resnet backbone\n",
        "\n",
        "        Args:\n",
        "            backbone (str, optional): Name of the Resnet architecture to import. Defaults to 'resnet18'. Otherwise resnet50\n",
        "            hidden_dim (int, optional): Hidden layer dimension. Defaults to 512.\n",
        "        \"\"\"\n",
        "        super(PosePredictorModel, self).__init__()\n",
        "\n",
        "        # Feature extractor, import resnet architecture\n",
        "        self.feature_extractor = FeatureExtractor(backbone)\n",
        "        feature_dim = self.feature_extractor.get_feature_dim()\n",
        "\n",
        "        # Fully connected layers for the pose prediction\n",
        "        # self.fc_layers = nn.Sequential(\n",
        "        #     nn.Linear(feature_dim, hidden_dim//2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.3),\n",
        "        #     nn.Linear(hidden_dim//2, hidden_dim//2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.3),\n",
        "        #     nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "        #     nn.ReLU()\n",
        "        # )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "          nn.Linear(feature_dim, hidden_dim),\n",
        "          nn.BatchNorm1d(hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.3),\n",
        "\n",
        "          nn.Linear(hidden_dim, hidden_dim),\n",
        "          nn.BatchNorm1d(hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.3),\n",
        "\n",
        "          nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "          nn.BatchNorm1d(hidden_dim // 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.3),\n",
        "\n",
        "          nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "          nn.BatchNorm1d(hidden_dim // 4),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Output heads separated for translation e rotation\n",
        "        self.translation_head = nn.Linear(hidden_dim//4, 3) # // 2 per add con matrice di rotazione, // 4 per quaternion\n",
        "        self.rotation_head = nn.Linear(hidden_dim//4, 4)  # //2, 9 rotation matrix; //4, 4 per quaternion\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        #  FC layers\n",
        "        x = self.fc_layers(features)\n",
        "\n",
        "        # Predict translation and rotation\n",
        "        translation = self.translation_head(x)\n",
        "        rotation_flat = self.rotation_head(x)\n",
        "\n",
        "        # Reshape rotation matrix in 3x3 shape\n",
        "        batch_size = rotation_flat.size(0)\n",
        "        rotation_matrix = rotation_flat.view(batch_size, 4) # 3, 3 for rotation matrix, 4 for quaternion\n",
        "\n",
        "        return translation, rotation_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lv_moOuIrFnE",
      "metadata": {
        "id": "Lv_moOuIrFnE"
      },
      "outputs": [],
      "source": [
        "class PoseLoss(nn.Module):\n",
        "    \"\"\"Loss function combines translation and rotation\n",
        "        returns a weighted sum of the translation and rotation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0, beta=1.0):\n",
        "        \"\"\"Initialize alpha and beta values.\n",
        "\n",
        "        Args:\n",
        "            alpha (float, optional): Wieght (importance) to give to translation. Defaults to 1.0.\n",
        "            beta (float, optional): Wieght (importance) to give to rotation. Defaults to 1.0.\n",
        "        \"\"\"\n",
        "        super(PoseLoss, self).__init__()\n",
        "        self.alpha = alpha  # peso per translation loss\n",
        "        self.beta = beta    # peso per rotation loss\n",
        "\n",
        "    def forward(self, pred_trans, pred_rot, gt_trans, gt_rot):\n",
        "        \"\"\"Initialize alpha and beta values.\n",
        "\n",
        "          Args:\n",
        "              pred_trans (matrix , optional): Predicted translation.\n",
        "              pred_rot (matrix, optional): Predicted rotation.\n",
        "              gt_trans (matrix, optional): Ground truth translation.\n",
        "              gt_rot (matrix, optional): Ground truth rotation.\n",
        "        \"\"\"\n",
        "        # Translation loss (MSE)\n",
        "        trans_loss = F.mse_loss(pred_trans, gt_trans)\n",
        "\n",
        "        # Frobenius norm of the differences between matrices\n",
        "        rot_loss = F.mse_loss(pred_rot, gt_rot.view(-1, 4)) # view(-1,4) per quaternion, view(-1, 3, 3) per rotation\n",
        "        # rot_loss = self.quaternion_loss(pred_rot, gt_rot.view(-1, 4)) # view(-1,4) per quaternion, view(-1, 3, 3) per rotation\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = self.alpha * trans_loss + self.beta * rot_loss\n",
        "\n",
        "        return total_loss, trans_loss, rot_loss\n",
        "\n",
        "    def quaternion_loss(self, pred_q, gt_q):\n",
        "        pred_q = F.normalize(pred_q, dim=-1)\n",
        "        gt_q = F.normalize(gt_q, dim=-1)\n",
        "        dot = torch.sum(pred_q * gt_q, dim=-1).abs()\n",
        "        return 1 - dot.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ngg7wG9CrJGQ",
      "metadata": {
        "id": "ngg7wG9CrJGQ"
      },
      "outputs": [],
      "source": [
        "class PoseEstimationTrainer:\n",
        "    \"\"\"Trainer class for the Pose Estimation model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, train_loader, val_loader, device='cuda', config=None, experiment=None):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            model (torch model): Model to be trained\n",
        "            train_loader (dataloader): train dataloader\n",
        "            val_loader (dataloader): validation dataloader\n",
        "            device (str, optional): cuda or cpu. Defaults to 'cuda'.\n",
        "            config (dict, optional): configuration file. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.config = config or {}\n",
        "\n",
        "        # Loss function e optimizer\n",
        "        self.criterion = PoseLoss(alpha=1.0, beta=1.0)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max= config['num_epochs'], eta_min=1e-6)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "        # Tracking delle metriche\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.step = 0\n",
        "\n",
        "        # # Log model architecture to wandb\n",
        "        # if wandb.run is not None:\n",
        "        #     wandb.watch(self.model, log=\"all\", log_freq=50)\n",
        "\n",
        "        self.experiment = experiment\n",
        "\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train the model for one epoch.\n",
        "\n",
        "        Returns:\n",
        "            avg_loss (float): average total loss of the epoch\n",
        "            avg_trans_loss (float): average translation loss of the epoch\n",
        "            avg_rot_loss (float): average rotation loss of the epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_trans_loss = 0\n",
        "        total_rot_loss = 0\n",
        "        num_batches = len(self.train_loader)\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc='Training')\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            images = batch['cropped_img']\n",
        "            gt_trans = batch['translation']\n",
        "            gt_rot = batch['quaternion'] # ['quaternion'] per quaternion ['rotation']\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            pred_trans, pred_rot = self.model(images)\n",
        "\n",
        "            # Compute  loss\n",
        "            loss, trans_loss, rot_loss = self.criterion(pred_trans, pred_rot, gt_trans, gt_rot)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Aggiorna metriche\n",
        "            total_loss += loss.item()\n",
        "            total_trans_loss += trans_loss.item()\n",
        "            total_rot_loss += rot_loss.item()\n",
        "\n",
        "            # Log batch metrics to wandb\n",
        "            if batch_idx %20 ==0:   # per batch_size==1 sarà da cambiare\n",
        "\n",
        "                self.experiment.log_metrics({\n",
        "                    \"batch_loss\": loss.item(),\n",
        "                    \"batch_trans_loss\": trans_loss.item(),\n",
        "                    \"batch_rot_loss\": rot_loss.item(),\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr'],\n",
        "                    \"step\": self.step\n",
        "                })\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Trans': f'{trans_loss.item():.4f}',\n",
        "                'Rot': f'{rot_loss.item():.4f}',\n",
        "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_trans_loss = total_trans_loss / num_batches\n",
        "        avg_rot_loss = total_rot_loss / num_batches\n",
        "\n",
        "        return avg_loss, avg_trans_loss, avg_rot_loss\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model on the validation set after each epoch.\n",
        "\n",
        "        Returns:\n",
        "            avg_loss (float): average total loss of the epoch\n",
        "            avg_trans_loss (float): average translation loss of the epoch\n",
        "            avg_rot_loss (float): average rotation loss of the epoch\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_trans_loss = 0\n",
        "        total_rot_loss = 0\n",
        "        num_batches = len(self.val_loader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
        "                images = batch['cropped_img']\n",
        "                gt_trans = batch['translation']\n",
        "                gt_rot = batch['quaternion'] # ['quaternion'] per quaternion ['rotation']\n",
        "\n",
        "                # Forward pass\n",
        "                pred_trans, pred_rot = self.model(images)\n",
        "\n",
        "                # Calcola loss\n",
        "                loss, trans_loss, rot_loss = self.criterion(pred_trans, pred_rot, gt_trans, gt_rot)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_trans_loss += trans_loss.item()\n",
        "                total_rot_loss += rot_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_trans_loss = total_trans_loss / num_batches\n",
        "        avg_rot_loss = total_rot_loss / num_batches\n",
        "\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_metrics({\n",
        "                \"val_loss\": avg_loss,\n",
        "                \"val_trans_loss\": avg_trans_loss,\n",
        "                \"val_rot_loss\": avg_rot_loss,\n",
        "                \"epoch\": len(self.train_losses)\n",
        "            })\n",
        "\n",
        "        return avg_loss, avg_trans_loss, avg_rot_loss\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        \"\"\"Train the model for the specified number of epochs.\n",
        "\n",
        "        Args:\n",
        "            num_epochs (int): number of epochs to train the model for\n",
        "        \"\"\"\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "        if self.experiment is not None:\n",
        "          with self.experiment.train():\n",
        "            watch(self.model)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_trans_loss, train_rot_loss = self.train_epoch()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_trans_loss, val_rot_loss = self.validate()\n",
        "\n",
        "            # Scheduler step\n",
        "            self.scheduler.step()\n",
        "\n",
        "            # Save metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Log epoch metrics to wandb\n",
        "            if self.experiment is not None:\n",
        "                self.experiment.log_metrics({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"train_trans_loss\": train_trans_loss,\n",
        "                    \"train_rot_loss\": train_rot_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"val_trans_loss\": val_trans_loss,\n",
        "                    \"val_rot_loss\": val_rot_loss,\n",
        "                })\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f} (Trans: {train_trans_loss:.4f}, Rot: {train_rot_loss:.4f})')\n",
        "            print(f'Val Loss: {val_loss:.4f} (Trans: {val_trans_loss:.4f}, Rot: {val_rot_loss:.4f})')\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "\n",
        "                os.makedirs(f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/baseline/\", exist_ok=True)\n",
        "\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "                batch_size = self.config.get('batch_size', 32)\n",
        "                best_model_path = (\n",
        "                    f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/baseline/{self.config['name_saved_file']}_{self.config['backbone']}\" # quaternion\n",
        "                    f\"_bs{batch_size}.pth\"\n",
        "                )\n",
        "\n",
        "                # Save the model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'train_loss': train_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                    'config': self.config\n",
        "                }, best_model_path)\n",
        "\n",
        "                # Log model to wandb\n",
        "                if self.experiment is not None:\n",
        "                    self.experiment.log_metric(\"best_val_loss\", val_loss)\n",
        "\n",
        "                print(f'Saved best model with val_loss: {val_loss:.4f}')\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    def plot_losses(self):\n",
        "        \"\"\"Plot the training and validation losses.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Log plot to wandb\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_image(image_data= plt, name= \"Plot losses\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O8MY3SkmrG6f",
      "metadata": {
        "id": "O8MY3SkmrG6f"
      },
      "outputs": [],
      "source": [
        "class ADDMetric:\n",
        "    \"\"\" ADD metric to evaluate prediction of the NN comparing it with the CAD model.\n",
        "        returns the average distance between the predicted pose and the ground truth pose.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, test_loader, models_3D_dir, symmetric_objects=None, device='cuda', experiment=None, config=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model (torch model): model to evaluate\n",
        "            test_loader (dataloader): test loader\n",
        "            models_3D_dir (str): path to the directory containing 3D models\n",
        "            symmetric_objects (list, optional): list of object_id that are symmetric. Defaults to None.\n",
        "            device (str, optional): device to use. Defaults to 'cuda'.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.test_loader = test_loader\n",
        "        self.models_3D_dir = models_3D_dir\n",
        "        self.symmetric_objects = symmetric_objects or []\n",
        "        self.device = device\n",
        "        self.models_points_dict = self.load_models_points(self.models_3D_dir)\n",
        "        # read the diameters for each object\n",
        "        self.objects_info = self.load_objects_models_info()\n",
        "        self.experiment = experiment\n",
        "        self.config = config\n",
        "\n",
        "    def load_models_points(self, models_dir):\n",
        "        \"\"\"Load the 3D model points (vertices) for the LINEMOD dataset in a dictionary {class_name: points}.\n",
        "\n",
        "        Args:\n",
        "            models_dir (_type_): path of the .ply files\n",
        "\n",
        "        Returns:\n",
        "            model_points_dict: dictionary of model points for each object class\n",
        "        \"\"\"\n",
        "        model_points_dict = {}\n",
        "        class_names = [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]\n",
        "\n",
        "        for obj_id in class_names:\n",
        "            model_path = os.path.join(models_dir, f'obj_{obj_id}.ply')\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    # Carica il modello 3D\n",
        "                    mesh = trimesh.load(model_path)\n",
        "\n",
        "                    # Estrai punti dalla superficie o usa vertices\n",
        "                    if hasattr(mesh, 'vertices') and mesh.vertices is not None:\n",
        "                        points = torch.tensor(mesh.vertices/1000.0, dtype=torch.float32).to(device)\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    # Check for NaN or infinite values\n",
        "                    if torch.any(torch.isnan(points)) or torch.any(torch.isinf(points)):\n",
        "                        # Remove NaN/Inf points\n",
        "                        valid_mask = ~(torch.any(torch.isnan(points), dim=1) | torch.any(torch.isinf(points), dim=1))\n",
        "                        points = points[valid_mask].to(device)\n",
        "\n",
        "                    if len(points) == 0:\n",
        "                        print(f\"No valid points found for object {obj_id}\")\n",
        "                        continue\n",
        "                    model_points_dict[obj_id] = points\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading model {model_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return model_points_dict\n",
        "\n",
        "    def load_objects_models_info(self):\n",
        "        with open(f\"{self.models_3D_dir}/models_info.yml\", 'r') as f:\n",
        "            return yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "    def compute_add(self, pred_pose, gt_pose, object_id, threshold=0.1):\n",
        "        \"\"\"\n",
        "        Compute ADD metric for a single prediction\n",
        "\n",
        "        Args:\n",
        "            pred_pose (tuple): (translation, rotation_matrix)\n",
        "            gt_pose (tuple): (translation, rotation_matrix)\n",
        "            object_id (str): ID of the object\n",
        "            threshold (float): threshold to consider the prediction correct\n",
        "        \"\"\"\n",
        "        pred_trans, pred_rot = pred_pose\n",
        "        gt_trans, gt_rot = gt_pose\n",
        "\n",
        "        # Check for NaN values in poses\n",
        "        if torch.any(torch.isnan(pred_trans)) or torch.any(torch.isnan(pred_rot)):\n",
        "            print(f\"NaN values found in predicted pose in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        if torch.any(torch.isnan(gt_trans)) or torch.any(torch.isnan(gt_rot)):\n",
        "            print(f\"NaN values found in ground truth pose in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        try:\n",
        "        # Convert Torch quaternion [w, x, y, z] -> numpy.quaternion(w, x, y, z)\n",
        "            pred_quat = np.quaternion(pred_rot[0].item(), pred_rot[1].item(), pred_rot[2].item(), pred_rot[3].item())\n",
        "            gt_quat = np.quaternion(gt_rot[0].item(), gt_rot[1].item(), gt_rot[2].item(), gt_rot[3].item())\n",
        "\n",
        "            # Get rotation matrices (3x3 numpy)\n",
        "            pred_rot = torch.tensor(quaternion.as_rotation_matrix(pred_quat), dtype=torch.float32, device=pred_rot.device)\n",
        "            gt_rot = torch.tensor(quaternion.as_rotation_matrix(gt_quat), dtype=torch.float32, device=gt_rot.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting quaternion to rotation matrix for object {object_id}: {e}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        # Take 3D model points\n",
        "        if object_id not in self.models_points_dict:\n",
        "            print(f\"Object with object id {object_id} is not present in models_point_dict\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        model_points = self.models_points_dict[object_id]  # Shape: (N, 3)\n",
        "\n",
        "        # Ensure proper shapes\n",
        "        if pred_rot.shape != (3, 3) or gt_rot.shape != (3, 3):\n",
        "            print(f\"Invalid shape for rotation in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        try:\n",
        "            # Transform points with the predicted pose\n",
        "            pred_points = torch.matmul(model_points, pred_rot.T) + pred_trans.reshape(1, 3)\n",
        "\n",
        "            # Transform points with the gt pose\n",
        "            gt_points = torch.matmul(model_points, gt_rot.T) + gt_trans.reshape(1, 3)\n",
        "\n",
        "            # Check for NaN in transformed points\n",
        "            if torch.any(torch.isnan(pred_points)) or torch.any(torch.isnan(gt_points)):\n",
        "                print(f\"NaN values found in transformed points in object {object_id}\")\n",
        "                return torch.nan, False\n",
        "\n",
        "            # Compute ADD\n",
        "            if object_id in self.symmetric_objects:\n",
        "                # For symmetric objects, compute ADD-S\n",
        "                distances = []\n",
        "                for pred_point in pred_points:\n",
        "                    diffs = gt_points - pred_point.reshape(1, 3)\n",
        "                    point_distances = torch.linalg.norm(diffs, axis=1)\n",
        "                    min_dist = torch.min(point_distances)\n",
        "                    distances.append(min_dist)\n",
        "                distances = torch.tensor(distances)\n",
        "            else:\n",
        "                # For non-symmetric objects, compute standard ADD\n",
        "                distances = torch.linalg.norm(pred_points - gt_points, axis=1)\n",
        "\n",
        "            avg_distance = torch.mean(distances)\n",
        "\n",
        "            # Check if the prediction is correct\n",
        "            is_correct = 1.0 if avg_distance < threshold else 0.0\n",
        "\n",
        "            return avg_distance, is_correct\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore dirante il calcolo ADD per l'oggetto {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "    def evaluate_model_with_add(self):\n",
        "        \"\"\"Evaluate the model using the ADD metric.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        results = {}\n",
        "        all_distances = []\n",
        "        all_correct = []\n",
        "        total_processed = 0\n",
        "        total_skipped = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(self.test_loader, desc='Evaluating ADD')):\n",
        "                images = batch['cropped_img'].to(self.device)\n",
        "                gt_trans = batch['translation'].to(device)\n",
        "                gt_rot = batch['quaternion'].to(device)\n",
        "                object_ids = batch['obj_id'].to(device)\n",
        "\n",
        "                # Predict\n",
        "                try:\n",
        "                    pred_trans, pred_rot = self.model(images)\n",
        "\n",
        "                except Exception as e:\n",
        "                    raise ValueError(f\"Error during model prediction: {e}\")\n",
        "\n",
        "                # Compute ADD for each object\n",
        "                for i in range(len(images)):\n",
        "                    obj_id_int = int(object_ids[i])\n",
        "                    obj_id = f\"{obj_id_int:02d}\"\n",
        "\n",
        "                    if obj_id in self.models_points_dict:\n",
        "                        pred_pose = (pred_trans[i], pred_rot[i])\n",
        "                        gt_pose = (gt_trans[i], gt_rot[i])\n",
        "\n",
        "                        distance, is_correct = self.compute_add(\n",
        "                            pred_pose, gt_pose, obj_id, threshold=0.1\n",
        "                        )\n",
        "\n",
        "                        if not torch.isnan(distance):\n",
        "                            all_distances.append(distance)\n",
        "                            all_correct.append(is_correct)\n",
        "                            total_processed += 1\n",
        "\n",
        "                            if obj_id not in results:\n",
        "                                results[obj_id] = {'distances': [], 'correct': []}\n",
        "\n",
        "                            results[obj_id]['distances'].append(distance)\n",
        "                            results[obj_id]['correct'].append(is_correct)\n",
        "                        else:\n",
        "                            total_skipped += 1\n",
        "                    else:\n",
        "                        total_skipped += 1\n",
        "\n",
        "        if len(all_distances) == 0:\n",
        "            print(\"No valid objects found for evaluation.\")\n",
        "            return torch.nan, torch.nan, {}\n",
        "\n",
        "        # Compute overall metrics\n",
        "        all_distances = torch.tensor(all_distances)\n",
        "        all_correct = torch.tensor(all_correct)\n",
        "        overall_add = torch.mean(all_distances)\n",
        "        overall_accuracy = torch.mean(all_correct)\n",
        "\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_metrics({\n",
        "                \"test_add_score\": overall_add,\n",
        "                \"test_accuracy\": overall_accuracy,\n",
        "                \"total_processed\": total_processed,\n",
        "                \"total_skipped\": total_skipped\n",
        "            })\n",
        "\n",
        "        print(f\"\\nOverall ADD: {overall_add:.4f}\")\n",
        "        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "        # Print per-object results\n",
        "        print(\"\\nPer-object results:\")\n",
        "        for obj_id, obj_results in results.items():\n",
        "            obj_add = torch.mean(torch.tensor(obj_results['distances']))\n",
        "            obj_acc = torch.mean(torch.tensor(obj_results['correct']))\n",
        "            num_samples = len(obj_results['distances'])\n",
        "            print(f\"Object {obj_id}: ADD={obj_add:.4f}, Acc={obj_acc:.4f}, Samples={num_samples}\")\n",
        "\n",
        "        return overall_add, overall_accuracy, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H9-uVqmQsxXM",
      "metadata": {
        "id": "H9-uVqmQsxXM"
      },
      "outputs": [],
      "source": [
        "def plotPose(pathImage, translation_gt, rotation_gt, translation_pred, rotation_pred, experiment=None):\n",
        "\n",
        "    image = cv2.imread(pathImage)\n",
        "    transparent_image = image.copy()\n",
        "\n",
        "    rotat_gt = rotation_gt.to(device).float()\n",
        "    trans_gt = translation_gt.to(device).float()\n",
        "    rotat_pred = rotation_pred.to(device).float()\n",
        "    trans_pred = translation_pred.to(device).float()\n",
        "\n",
        "    image_id = pathImage.split(\"/\")[-1].split(\".\")[0]\n",
        "    label = pathImage.split(\"/\")[-3]\n",
        "    camera_intrinsics = torch.tensor([572.4114, 0.0, 325.2611, 0.0, 573.57043, 242.04899, 0.0, 0.0, 1.0]).reshape(3, 3).to(device)\n",
        "\n",
        "    meshModel = trimesh.load(f\"./Linemod_preprocessed/models/obj_{label}.ply\")\n",
        "    vertices = torch.tensor(meshModel.vertices / 1000, dtype=torch.float32).to(device)\n",
        "    min_corner = vertices.min(dim=0).values\n",
        "    max_corner = vertices.max(dim=0).values\n",
        "\n",
        "    bounding_box_3d = torch.tensor([\n",
        "        [min_corner[0], min_corner[1], min_corner[2]],\n",
        "        [max_corner[0], min_corner[1], min_corner[2]],\n",
        "        [max_corner[0], max_corner[1], min_corner[2]],\n",
        "        [min_corner[0], max_corner[1], min_corner[2]],\n",
        "        [min_corner[0], min_corner[1], max_corner[2]],\n",
        "        [max_corner[0], min_corner[1], max_corner[2]],\n",
        "        [max_corner[0], max_corner[1], max_corner[2]],\n",
        "        [min_corner[0], max_corner[1], max_corner[2]],\n",
        "    ], dtype=torch.float32).to(device)\n",
        "\n",
        "    if rotat_gt.numel() == 4:\n",
        "        rotat_gt = torch.tensor(\n",
        "            quaternion.as_rotation_matrix(np.quaternion(*rotat_gt.cpu().numpy())),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "    else:\n",
        "        rotat_gt = rotat_gt.reshape(3, 3)\n",
        "\n",
        "    if rotat_pred.numel() == 4:\n",
        "        rotat_pred = torch.tensor(\n",
        "            quaternion.as_rotation_matrix(np.quaternion(*rotat_pred.cpu().numpy())),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "    else:\n",
        "        rotat_pred = rotat_pred.reshape(3, 3)\n",
        "\n",
        "    axes_3d = torch.tensor([\n",
        "        [0, 0, 0],\n",
        "        [0.15, 0, 0],\n",
        "        [0, 0.15, 0],\n",
        "        [0, 0, 0.15]\n",
        "    ], dtype=torch.float32).to(device)\n",
        "\n",
        "    axes_cam_gt = (rotat_gt @ axes_3d.T).T + trans_gt\n",
        "    bounding_box_3d_cam_gt = (rotat_gt @ bounding_box_3d.T).T + trans_gt\n",
        "\n",
        "    axes_2d_gt = (camera_intrinsics @ axes_cam_gt.T).T\n",
        "    axes_2d_gt = axes_2d_gt[:, :2] / axes_2d_gt[:, 2:3]\n",
        "    bounding_box_2d_gt = (camera_intrinsics @ bounding_box_3d_cam_gt.T).T\n",
        "    bounding_box_2d_gt = (bounding_box_2d_gt[:, :2] / bounding_box_2d_gt[:, 2:3]).int()\n",
        "\n",
        "    p_gt = [tuple(el.cpu().numpy()) for el in bounding_box_2d_gt]\n",
        "    edges = [(0,1), (1,2), (2,3), (3,0), (0,4), (1,5), (2,6), (3,7), (4,5), (5,6), (6,7), (7,4)]\n",
        "    for el in edges:\n",
        "        cv2.line(image, p_gt[el[0]], p_gt[el[1]], (0,0,255), 5)\n",
        "\n",
        "    p0_gt = tuple(axes_2d_gt[0].cpu().int().numpy())\n",
        "    p1_gt = tuple(axes_2d_gt[1].cpu().int().numpy())\n",
        "    p2_gt = tuple(axes_2d_gt[2].cpu().int().numpy())\n",
        "    p3_gt = tuple(axes_2d_gt[3].cpu().int().numpy())\n",
        "\n",
        "    cv2.arrowedLine(image, p0_gt, p1_gt, (0, 0, 255), 2)\n",
        "    cv2.arrowedLine(image, p0_gt, p2_gt, (0, 255, 0), 2)\n",
        "    cv2.arrowedLine(image, p0_gt, p3_gt, (255, 0, 0), 2)\n",
        "\n",
        "    axes_cam_pred = (rotat_pred @ axes_3d.T).T + trans_pred\n",
        "    bounding_box_3d_cam_pred = (rotat_pred @ bounding_box_3d.T).T + trans_pred\n",
        "\n",
        "    axes_2d_pred = (camera_intrinsics @ axes_cam_pred.T).T\n",
        "    axes_2d_pred = axes_2d_pred[:, :2] / axes_2d_pred[:, 2:3]\n",
        "    bounding_box_2d_pred = (camera_intrinsics @ bounding_box_3d_cam_pred.T).T\n",
        "    bounding_box_2d_pred = (bounding_box_2d_pred[:, :2] / bounding_box_2d_pred[:, 2:3]).int()\n",
        "\n",
        "    p_pred = [tuple(el.cpu().numpy()) for el in bounding_box_2d_pred]\n",
        "    for el in edges:\n",
        "        cv2.line(image, p_pred[el[0]], p_pred[el[1]], (255, 0, 0), 5)\n",
        "\n",
        "    p0_pred = tuple(axes_2d_pred[0].cpu().int().numpy())\n",
        "    p1_pred = tuple(axes_2d_pred[1].cpu().int().numpy())\n",
        "    p2_pred = tuple(axes_2d_pred[2].cpu().int().numpy())\n",
        "    p3_pred = tuple(axes_2d_pred[3].cpu().int().numpy())\n",
        "\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p1_pred, (0, 0, 255), 2)\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p2_pred, (0, 255, 0), 2)\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p3_pred, (255, 0, 0), 2)\n",
        "\n",
        "    overlapImage = cv2.addWeighted(transparent_image, 0.5, image, 1, 0)\n",
        "    img = cv2.cvtColor(overlapImage, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(img)\n",
        "    if experiment is not None:\n",
        "      experiment.log_image(image_data=img, name= f\"{label}_{image_id}\")\n",
        "    plt.title(\"Object Pose Estimation (prediction is transparent)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sdfVKnprWbXR",
      "metadata": {
        "id": "sdfVKnprWbXR"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I9voAeo9rKUO",
      "metadata": {
        "collapsed": true,
        "id": "I9voAeo9rKUO"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"project_name\": \"baseline_quaternion\",\n",
        "    \"experiment_name\": \"mse_loss_step_optim\",\n",
        "    \"batch_size\": 32,\n",
        "    \"num_epochs\": 25,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"backbone\": \"resnet18\",\n",
        "    \"hidden_dim\": 512,\n",
        "    \"img_size\": 224,\n",
        "    \"alpha\": 1.0,\n",
        "    \"beta\": 1.0,\n",
        "    \"add_threshold\": 0.1,\n",
        "    \"symmetric_objects\": [\"10\"],\n",
        "    \"name_saved_file\": \"mse_loss_step\"\n",
        "}\n",
        "\n",
        "MODELS_DIR = \"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/models\"\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Configuration: {config}\")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "\n",
        "# # --------------------------\n",
        "# from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# # Numero di campioni desiderati nel subset\n",
        "# subset_size = 10\n",
        "# subset_indices = list(range(subset_size))\n",
        "\n",
        "# # Creiamo i subset dei dataset originali\n",
        "# train_subset = Subset(train_loader.dataset, subset_indices)\n",
        "# val_subset = Subset(val_loader.dataset, subset_indices)\n",
        "# test_subset = Subset(test_loader.dataset, subset_indices)\n",
        "\n",
        "# # Creiamo i nuovi DataLoader a partire dai subset\n",
        "# train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "# val_loader = DataLoader(val_subset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "# test_loader = DataLoader(test_subset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "# # ----------------------\n",
        "\n",
        "\n",
        "# Modello\n",
        "model = PosePredictorModel(\n",
        "    backbone=config[\"backbone\"],\n",
        "    hidden_dim=config[\"hidden_dim\"]\n",
        ").to(DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "experiment = comet_ml.start(\n",
        "    api_key=\"qt32gGMiNUQ0MX8Kc1jhRdvlo\",\n",
        "    project_name=config['project_name'],\n",
        "    experiment_config=comet_ml.ExperimentConfig(\n",
        "        name=config[\"experiment_name\"],\n",
        "        parse_args=False)\n",
        ")\n",
        "\n",
        "experiment.log_parameters(config)\n",
        "\n",
        "trainer = PoseEstimationTrainer(model, train_loader, val_loader, device=DEVICE, config=config, experiment=experiment)\n",
        "trainer.train(num_epochs=config[\"num_epochs\"])\n",
        "\n",
        "!mkdir -p /content/checkpoints\n",
        "!cp -r /content/drive/MyDrive/6D_pose_estimation/checkpoints /content\n",
        "\n",
        "\n",
        "checkpoint = torch.load(f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/baseline/{config['name_saved_file']}_{config['backbone']}_bs{config['batch_size']}.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "\n",
        "add_metric = ADDMetric(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    models_3D_dir=MODELS_DIR,\n",
        "    symmetric_objects=config[\"symmetric_objects\"],\n",
        "    device=DEVICE,\n",
        "    experiment=experiment,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"Evaluating with ADD metric...\")\n",
        "add_score, accuracy, detailed_results = add_metric.evaluate_model_with_add()\n",
        "\n",
        "\n",
        "print(f\"\\nFinal Results:\\nADD Score: {add_score:.4f}\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "test_batch = next(iter(test_loader))\n",
        "\n",
        "for idx, batch in enumerate(test_loader):\n",
        "    images = batch['rgb'].to(device)\n",
        "    gt_trans = batch['translation']\n",
        "    gt_rot = batch['rotation']\n",
        "    object_ids = batch['obj_id']\n",
        "    sample_id = batch[\"sample_id\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_trans, pred_rot = model(images)\n",
        "      pred_trans = pred_trans\n",
        "      pred_rot = pred_rot\n",
        "\n",
        "      for i in range(len(images)):\n",
        "        if i == 0:\n",
        "          img_path = f\"./Linemod_preprocessed/data/{sample_id[i][0]:02d}/rgb/{sample_id[i][1]:04d}.png\"\n",
        "\n",
        "          plotPose(img_path, gt_trans[i], gt_rot[i], pred_trans[i], pred_rot[i], experiment)\n",
        "print(f\"Plot salvati su comet_ml in project: {config['project_name']}, experiment: {config['experiment_name']}\")\n",
        "\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qwiFHDeunMxU",
      "metadata": {
        "id": "qwiFHDeunMxU"
      },
      "source": [
        "# Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tXB6F-7vnRad",
      "metadata": {
        "id": "tXB6F-7vnRad"
      },
      "source": [
        "## PointeNet++"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W7wK5uUcMCb7",
      "metadata": {
        "id": "W7wK5uUcMCb7"
      },
      "source": [
        "#### trova immagini che non compaiono\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aIsw8TaOxxLB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIsw8TaOxxLB",
        "outputId": "8dec32ac-be09-4dc5-dcfd-dd2bb034fb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15783\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "for el in os.listdir(\"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/data\"):\n",
        "  for i in os.listdir(f\"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/data/{el}/mask\"):\n",
        "    count+=1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CoyTNegUyZI-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoyTNegUyZI-",
        "outputId": "d53735d4-837f-4131-b5d8-8352be45ddd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15800\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset)+ len(val_dataset)+len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sx_W7p7X0A_A",
      "metadata": {
        "id": "Sx_W7p7X0A_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def compare_rgb_mask_in_data(root, extensions={'.png', '.jpg', '.jpeg', '.bmp'}):\n",
        "    \"\"\"\n",
        "    Per ogni classe in `root`, confronta i file tra le cartelle `rgb/` e `mask/`.\n",
        "    Stampa i file presenti solo in una delle due.\n",
        "\n",
        "    Args:\n",
        "        root (str): Percorso della cartella principale (es. \"data/\")\n",
        "        extensions (set): Estensioni immagine da considerare\n",
        "    \"\"\"\n",
        "    classes = sorted(os.listdir(root))\n",
        "    only_rgb= 0\n",
        "    only_mask=0\n",
        "    print(classes)\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(root, class_name)\n",
        "        rgb_path = os.path.join(class_path, 'rgb')\n",
        "        mask_path = os.path.join(class_path, 'mask')\n",
        "\n",
        "        if not os.path.isdir(rgb_path) or not os.path.isdir(mask_path):\n",
        "            print(f\"⚠️  Mancano cartelle rgb o mask in '{class_name}'\")\n",
        "            continue\n",
        "\n",
        "        rgb_files = {f for f in os.listdir(rgb_path) if os.path.splitext(f)[1].lower() in extensions}\n",
        "        mask_files = {f for f in os.listdir(mask_path) if os.path.splitext(f)[1].lower() in extensions}\n",
        "\n",
        "        only_in_rgb = sorted(rgb_files - mask_files)\n",
        "        only_in_mask = sorted(mask_files - rgb_files)\n",
        "\n",
        "        if only_in_rgb:\n",
        "            print(f\"\\n📁 Classe '{class_name}' — solo in *rgb*:\")\n",
        "            for f in only_in_rgb:\n",
        "                print(f\"  {f}\")\n",
        "                only_rgb+=1\n",
        "\n",
        "        if only_in_mask:\n",
        "            print(f\"\\n📁 Classe '{class_name}' — solo in *mask*:\")\n",
        "            for f in only_in_mask:\n",
        "                print(f\"  {f}\")\n",
        "                only_mask+=1\n",
        "\n",
        "    print(f\"Totale file solo in rgb: {only_rgb}\")\n",
        "    print(f\"Totale file solo in mask: {only_mask}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "geY6oFkz0GIT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geY6oFkz0GIT",
        "outputId": "71494909-78fd-4f20-ad9a-3b2b60e3d3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n",
            "\n",
            "📁 Classe '02' — solo in *mask*:\n",
            "  1214.png\n",
            "\n",
            "📁 Classe '15' — solo in *rgb*:\n",
            "  1225.png\n",
            "  1226.png\n",
            "  1227.png\n",
            "  1228.png\n",
            "  1229.png\n",
            "  1230.png\n",
            "  1231.png\n",
            "  1232.png\n",
            "  1233.png\n",
            "  1234.png\n",
            "  1235.png\n",
            "  1236.png\n",
            "  1237.png\n",
            "  1238.png\n",
            "  1239.png\n",
            "  1240.png\n",
            "  1241.png\n",
            "  1242.png\n",
            "Totale file solo in rgb: 18\n",
            "Totale file solo in mask: 1\n"
          ]
        }
      ],
      "source": [
        "compare_rgb_mask_in_data(\"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C8JiNwv2p-ww",
      "metadata": {
        "id": "C8JiNwv2p-ww"
      },
      "source": [
        "#### CustomDatasetPointCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "y6rqLeeiwG9L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6rqLeeiwG9L",
        "outputId": "d7312b88-e6b2-49d7-9243-70f5a4deea88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 11047\n",
            "Validation samples: 2367\n",
            "Testing samples: 2368\n"
          ]
        }
      ],
      "source": [
        "class CustomDataset(Dataset): # used to load and preprocess data\n",
        "    def __init__(self, dataset_root, split='train', train_ratio=0.7, seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_root (str): Path to the dataset directory.\n",
        "            split (str): 'train', 'validation' or 'test'.\n",
        "            train_ratio (float): Percentage of data used for training (default 70%).\n",
        "            seed (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.dataset_root = dataset_root\n",
        "        self.split = split\n",
        "        self.train_ratio = train_ratio\n",
        "        self.seed = seed\n",
        "\n",
        "        # Get list of all samples (folder_id, sample_id)\n",
        "        self.samples = self.get_all_samples()\n",
        "        self.camera_intrinsics = torch.tensor([572.4114, 573.57043, 325.2611, 242.04899]).to(device)\n",
        "\n",
        "        # Check if samples were found\n",
        "        if not self.samples:\n",
        "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
        "\n",
        "        # Split into training and validation+test sets\n",
        "        labels = [el[0] for el in self.samples]\n",
        "        self.train_samples, self.val_test_samples = train_test_split(\n",
        "            self.samples, train_size=self.train_ratio, random_state=self.seed, stratify=labels\n",
        "        )\n",
        "\n",
        "        # split validation+test set (by default 30% of the original dataset) into validation and test sets\n",
        "        labels = [el[0] for el in self.val_test_samples]\n",
        "        self.val_samples, self.test_samples = train_test_split(self.val_test_samples, train_size=0.5, random_state=self.seed, stratify=labels)\n",
        "\n",
        "        # Select the appropriate split\n",
        "        if split == \"train\":\n",
        "            self.samples = self.train_samples\n",
        "        elif split == \"validation\":\n",
        "            self.samples = self.val_samples\n",
        "        else:\n",
        "            self.samples = self.test_samples\n",
        "\n",
        "        # Define image transformations: QUESTO PER LA BASELINE\n",
        "        if self.split == 'train':\n",
        "\n",
        "            self.transform_img = transforms.Compose([\n",
        "                               transforms.ToTensor(),  # converte in float32 e normalizza in [0, 1]\n",
        "                            ])\n",
        "\n",
        "            self.transform_crop = transforms.Compose([\n",
        "                                # transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "                                # transforms.RandomGrayscale(p=0.1),\n",
        "                                # transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "    ])\n",
        "        else:\n",
        "              self.transform_img = transforms.Compose([\n",
        "                                transforms.ToTensor()\n",
        "                            ])\n",
        "\n",
        "              self.transform_crop = transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean=[0.3348, 0.3165, 0.3105], std=[0.2521, 0.2496, 0.2502])\n",
        "              ])\n",
        "\n",
        "        self.ground_truths = {}\n",
        "\n",
        "        for el in [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]:\n",
        "\n",
        "          pose_file = os.path.join(self.dataset_root, f\"{el}_gt.yml\")\n",
        "\n",
        "          with open(pose_file, 'r') as f:\n",
        "            pose_data = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "          dati = {}\n",
        "          chiavi_da_estrarre = ['cam_t_m2c', 'cam_R_m2c', 'quaternion', 'obj_bb', 'obj_id']\n",
        "          dati_estratti = {}\n",
        "\n",
        "          for key, value in pose_data.items():\n",
        "            entry = value[0]\n",
        "\n",
        "            # Estraggo solo le chiavi desiderate\n",
        "            estratti = {k: entry[k] for k in chiavi_da_estrarre if k in entry}\n",
        "\n",
        "            # Salvo nel dizionario dei dati estratti\n",
        "            dati_estratti[key]= estratti\n",
        "          self.ground_truths[el] = dati_estratti\n",
        "\n",
        "    def get_samples_id(self):\n",
        "        return self.samples\n",
        "\n",
        "    def get_all_samples(self):\n",
        "        \"\"\"Retrieve the list of sample indices from all folders, skipping those that already have a mask.\"\"\"\n",
        "        samples = []\n",
        "        for folder_id in range(1, 16):  # Assuming folders are named 01 to 15\n",
        "            rgb_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\")\n",
        "            mask_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"mask\")\n",
        "\n",
        "            if os.path.exists(rgb_path):\n",
        "                # Prendi tutti i nomi dei file RGB\n",
        "                sample_ids = sorted([\n",
        "                    int(f.split('.')[0]) for f in os.listdir(rgb_path)\n",
        "                    if f.endswith('.png') and os.path.exists(os.path.join(mask_path, f))\n",
        "                ])\n",
        "                samples.extend([(folder_id, sid) for sid in sample_ids])  # Store (folder_id, sample_id)\n",
        "        return samples\n",
        "\n",
        "\n",
        "    # def load_config(self, folder_id):\n",
        "    #     \"\"\"Load YAML configuration files for camera intrinsics and object info for a specific folder.\"\"\"\n",
        "    #     # camera_intrinsics_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", 'info.yml')\n",
        "    #     objects_info_path = os.path.join(self.dataset_root, 'models', f\"models_info.yml\")\n",
        "\n",
        "    #     # with open(camera_intrinsics_path, 'r') as f:\n",
        "    #         # camera_intrinsics = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    #     camera_intrinsics = [572.4114, 573.57043, 325.2611, 242.04899]\n",
        "\n",
        "    #     with open(objects_info_path, 'r') as f:\n",
        "    #         objects_info = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "    #     return camera_intrinsics, objects_info\n",
        "\n",
        "    #Define here some usefull functions to access the data\n",
        "    def load_image(self, img_path):\n",
        "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR_RGB)\n",
        "        return self.transform_img(img)\n",
        "\n",
        "    #Define here some usefull functions to access the data\n",
        "    def load_cropped_image(self, img_path, bbox):\n",
        "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
        "        img=Image.open(img_path).convert('RGB')\n",
        "        x,y,w,h = bbox\n",
        "        cropped_img = img.crop((x,y,x+w,y+h))\n",
        "        return self.transform_crop(cropped_img)\n",
        "\n",
        "    def load_depth(self, depth_path):\n",
        "        \"\"\"Load a depth image and convert to tensor.\"\"\"\n",
        "        return cv2.imread(depth_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
        "\n",
        "    def load_mask(self, path):\n",
        "        return cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.uint8)\n",
        "\n",
        "    def depth_to_pointcloud(self, masked_depth, intrinsics):\n",
        "        \"\"\"\n",
        "        Convert pixel coordinates + depth to 3D metric coordinates using intrinsic parameters.\n",
        "        Returns x, y, z as Nx3 array.\n",
        "        \"\"\"\n",
        "        fx, fy, cx, cy = intrinsics\n",
        "        height, width = masked_depth.shape\n",
        "        valid_mask = masked_depth > 0\n",
        "\n",
        "        u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
        "        u_valid = torch.tensor(u[valid_mask]).to(device)\n",
        "        v_valid = torch.tensor(v[valid_mask]).to(device)\n",
        "        z_valid = torch.tensor(masked_depth[valid_mask] / 1000.0).to(device)\n",
        "\n",
        "        # Ensure z_valid is not zero or very small to avoid division by zero\n",
        "        valid_depth_mask = z_valid > 1e-6\n",
        "        u_valid = u_valid[valid_depth_mask]\n",
        "        v_valid = v_valid[valid_depth_mask]\n",
        "        z_valid = z_valid[valid_depth_mask]\n",
        "\n",
        "        x_meters = (u_valid - cx) * z_valid / fx\n",
        "        y_meters = (v_valid - cy) * z_valid / fy\n",
        "        z_meters = z_valid\n",
        "\n",
        "        pointcloud = torch.stack([x_meters, y_meters, z_meters], axis=-1).reshape(-1, 3)  # Shape: (N, 3)\n",
        "\n",
        "        return pointcloud\n",
        "\n",
        "    def load_6d_pose(self, folder_id, sample_id):\n",
        "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
        "        pose = self.ground_truths[f\"{folder_id:02d}\"][int(sample_id)]\n",
        "        translation = np.array(pose['cam_t_m2c'], dtype=np.float32)/1000.0  # [3] ---> (x,y,z)\n",
        "        rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
        "        quaternion = np.array(pose['quaternion'], dtype=np.float32)  # [4] ---> quaternion\n",
        "        bbox_base = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
        "        cropped_img = self.load_cropped_image(os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\", f\"{sample_id:04d}.png\"), np.array(pose['obj_bb'], dtype=np.float32))\n",
        "        obj_id = np.array(pose['obj_id'], dtype=np.float32) # [1] ---> label\n",
        "        return cropped_img, translation, rotation, quaternion, bbox_base, obj_id\n",
        "\n",
        "    def __len__(self):\n",
        "        #Return the total number of samples in the selected split.\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Load a dataset sample.\n",
        "        folder_id, sample_id = self.samples[idx]\n",
        "        img_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"rgb/{sample_id:04d}.png\")\n",
        "        depth_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"depth/{sample_id:04d}.png\")\n",
        "        mask_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"mask/{sample_id:04d}.png\")\n",
        "        img = self.load_image(img_path)\n",
        "        depth = self.load_depth(depth_path)\n",
        "        mask = self.load_mask(mask_path)\n",
        "        mask_binary = mask != 0\n",
        "        masked_depth = np.where(mask_binary, depth, 0)\n",
        "\n",
        "        pointcloud = self.depth_to_pointcloud(masked_depth, self.camera_intrinsics)\n",
        "        sample_points = fps(pointcloud, None, ratio=802/pointcloud.size(0))[:800]\n",
        "        pointcloud = pointcloud[sample_points]\n",
        "\n",
        "        # BISOGNA AGGIUNGERE bbox_YOLO\n",
        "        cropped_img, translation, rotation, quaternion, bbox_base, obj_id = self.load_6d_pose(folder_id, sample_id)\n",
        "\n",
        "        #Dictionary with all the data\n",
        "        return {\n",
        "            # \"rgb\": img,\n",
        "            \"cropped_img\": cropped_img.to(device),\n",
        "            # \"depth\": torch.tensor(depth, dtype=torch.float32),\n",
        "            \"pointcloud\": pointcloud.to(device),\n",
        "            \"camera_intrinsics\": self.camera_intrinsics,\n",
        "            \"translation\": torch.tensor(translation).to(device),\n",
        "            \"rotation\": torch.tensor(rotation).to(device),\n",
        "            \"quaternion\": torch.tensor(quaternion).to(device),\n",
        "            \"bbox_base\": torch.tensor(bbox_base).to(device),\n",
        "            \"obj_id\": torch.tensor(obj_id).to(device),\n",
        "            \"sample_id\": torch.tensor(self.samples[idx]).to(device)\n",
        "        }\n",
        "\n",
        "dataset_root = \"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/\"\n",
        "\n",
        "train_dataset = CustomDataset(dataset_root, split='train')\n",
        "print(f'Training samples: {len(train_dataset)}')\n",
        "\n",
        "val_dataset = CustomDataset(dataset_root, split='validation')\n",
        "print(f'Validation samples: {len(val_dataset)}')\n",
        "\n",
        "test_dataset = CustomDataset(dataset_root, split='test')\n",
        "print(f'Testing samples: {len(test_dataset)}')\n",
        "\n",
        "def pointcloud_collate_fn(batch):\n",
        "  # Trova dimensioni massime per il padding\n",
        "  max_H = max(item['cropped_img'].shape[1] for item in batch)\n",
        "  max_W = max(item['cropped_img'].shape[2] for item in batch)\n",
        "\n",
        "  padded_cropped_imgs = []\n",
        "  paddings = []\n",
        "\n",
        "  for item in batch:\n",
        "    # --- Pad simmetrico immagine ---\n",
        "    img = item['cropped_img']\n",
        "    _, H, W = img.shape\n",
        "    pad_H = max_H - H\n",
        "    pad_W = max_W - W\n",
        "\n",
        "    # Calcola padding simmetrico: (left, right, top, bottom)\n",
        "    pad_left = pad_W // 2\n",
        "    pad_right = pad_W - pad_left\n",
        "    pad_top = pad_H // 2\n",
        "    pad_bottom = pad_H - pad_top\n",
        "\n",
        "    padding = (pad_left, pad_right, pad_top, pad_bottom)\n",
        "    padded_img = F.pad(img, padding, mode='replicate')\n",
        "    padded_cropped_imgs.append(padded_img)\n",
        "    padding = torch.tensor([pad_left, pad_right, pad_top, pad_bottom])\n",
        "    paddings.append(padding)\n",
        "\n",
        "  batch_dict = {\n",
        "    # \"rgb\": torch.stack([item['rgb'] for item in batch]).to(device),\n",
        "    \"cropped_img\": torch.stack(padded_cropped_imgs).to(device),\n",
        "    # \"depth\": torch.stack([item['depth'] for item in batch]).to(device),\n",
        "    \"pointcloud\": torch.stack([item['pointcloud'] for item in batch]).to(device),  # Lista di Tensor rimane\n",
        "    \"camera_intrinsics\": torch.stack([item['camera_intrinsics'] for item in batch]).to(device),\n",
        "    \"translation\": torch.stack([item['translation'] for item in batch]).to(device),\n",
        "    \"rotation\": torch.stack([item['rotation'] for item in batch]).to(device),\n",
        "    \"quaternion\": torch.stack([item['quaternion'] for item in batch]).to(device),\n",
        "    \"bbox_base\": torch.stack([item['bbox_base'] for item in batch]).to(device),\n",
        "    \"obj_id\": torch.stack([item['obj_id'] for item in batch]).to(device),\n",
        "    \"sample_id\": torch.stack([item['sample_id'] for item in batch]).to(device),\n",
        "    \"paddings\":torch.stack(paddings).to(device),\n",
        "  }\n",
        "\n",
        "  return batch_dict\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pointcloud_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=pointcloud_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=pointcloud_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "npEN1IO_zORZ",
      "metadata": {
        "id": "npEN1IO_zORZ"
      },
      "source": [
        "#### Architettura extension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YNwPPKqKPdK1",
      "metadata": {
        "id": "YNwPPKqKPdK1"
      },
      "source": [
        "##### Architettura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cnuyKkszy4Z3",
      "metadata": {
        "id": "cnuyKkszy4Z3"
      },
      "outputs": [],
      "source": [
        "class PointNetLayer(MessagePassing):\n",
        "    \"\"\"PointNet layer per estrazione feature geometriche\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_xyz=True):\n",
        "        super().__init__(aggr='max')\n",
        "        self.use_xyz = use_xyz\n",
        "        mlp_channels = in_channels + 3 if use_xyz else in_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(mlp_channels, out_channels),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_channels, out_channels),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, h, pos, edge_index):\n",
        "        h = h.view(-1, h.size(-1))\n",
        "        # h: [B, 1, N, C] oppure [B, N, C] -> Flatten to [B*N, C]\n",
        "        pos = pos.view(-1, 3)\n",
        "        return self.propagate(edge_index, h=h, pos=pos)\n",
        "\n",
        "    def message(self, h_j, pos_j, pos_i):\n",
        "        geometric_feat = pos_j - pos_i\n",
        "        if self.use_xyz:\n",
        "            geometric_feat = torch.cat([h_j, geometric_feat], dim=-1)\n",
        "        else:\n",
        "            geometric_feat = h_j\n",
        "        return self.mlp(geometric_feat)\n",
        "\n",
        "\n",
        "class SetAbstractionLayer(nn.Module):\n",
        "    \"\"\"Set Abstraction per PointNet++\"\"\"\n",
        "    def __init__(self, n_points, radius, n_neighbors, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.n_points = n_points\n",
        "        self.radius = radius\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.pointnet_layer = PointNetLayer(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, pos, h=None, batch_indices=None):\n",
        "        if h is None:\n",
        "          h = pos\n",
        "\n",
        "        # Sampling with proper batch handling\n",
        "        if self.n_points is not None:\n",
        "            pos = pos.view(-1, 3)  # -> [B, N, 3] -> [B*N, 3]\n",
        "            # Campiona n_points usando FPS\n",
        "            centroids_idx = fps(pos, batch_indices, ratio=self.n_points / pos.size(0))\n",
        "            centroids = pos[centroids_idx] # [N_centroids, 3]\n",
        "            centroids_batch = batch_indices[centroids_idx]# [N_centroids]\n",
        "        else:\n",
        "            centroids_idx = torch.arange(pos.size(0), device=pos.device)\n",
        "            centroids = pos\n",
        "            centroids_batch = batch_indices\n",
        "\n",
        "        edge_index = radius(pos, centroids, r=self.radius,\n",
        "                               batch_x=batch_indices, batch_y=centroids_batch,\n",
        "                               max_num_neighbors=self.n_neighbors) # [2, E]\n",
        "        # PointNet\n",
        "        aggregated_h = self.pointnet_layer(h, pos, edge_index) # [N, out_channels]\n",
        "        # Aggregate per centroids\n",
        "        new_h = torch.zeros(centroids.size(0), aggregated_h.size(1),\n",
        "                           device=aggregated_h.device, dtype=aggregated_h.dtype) # [N_centroids, out_channels]\n",
        "\n",
        "        for i, centroid_idx in enumerate(centroids_idx):\n",
        "            mask = edge_index[1] == i\n",
        "            if mask.any():\n",
        "                neighbor_feats = aggregated_h[edge_index[0][mask]]\n",
        "                new_h[i] = neighbor_feats.max(dim=0)[0]\n",
        "\n",
        "        return centroids, new_h, centroids_batch, edge_index # [N_centroids, 3] [N_centroids, out_channels] [N_centroids], [2, E]\n",
        "\n",
        "\n",
        "class GeometricFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Estrattore di feature geometriche da point cloud\n",
        "    Produce embeddings multi-scala per la fusion\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dims=[64, 128, 256]):\n",
        "        super().__init__()\n",
        "        self.feature_dims = feature_dims\n",
        "\n",
        "        # Multi-scale feature extraction\n",
        "        self.sa1 = SetAbstractionLayer(\n",
        "            n_points=800, radius=0.05, n_neighbors=32,\n",
        "            in_channels=3, out_channels=feature_dims[0]\n",
        "        )\n",
        "        self.sa2 = SetAbstractionLayer(\n",
        "            n_points=200, radius=0.1, n_neighbors=64,\n",
        "            in_channels=feature_dims[0], out_channels=feature_dims[1]\n",
        "        )\n",
        "        self.sa3 = SetAbstractionLayer(\n",
        "            n_points=50, radius=0.2, n_neighbors=64,\n",
        "            in_channels=feature_dims[1], out_channels=feature_dims[2]\n",
        "        )\n",
        "\n",
        "        # Feature projection layers per diversi livelli\n",
        "        self.feat_proj1 = nn.Sequential(\n",
        "            nn.Linear(feature_dims[0], feature_dims[0]),\n",
        "            nn.BatchNorm1d(feature_dims[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.feat_proj2 = nn.Sequential(\n",
        "            nn.Linear(feature_dims[1], feature_dims[1]),\n",
        "            nn.BatchNorm1d(feature_dims[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.feat_proj3 = nn.Sequential(\n",
        "            nn.Linear(feature_dims[2], feature_dims[2]),\n",
        "            nn.BatchNorm1d(feature_dims[2]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def upsampling_features(self, pos, h, edge_index):\n",
        "      # pos [B*N, 3], h [M, C], edge:index [2, E]\n",
        "\n",
        "      # edge_index[0] -> idx nei punti originali (pos)\n",
        "      # edge_index[1] -> idx nei centroidi\n",
        "\n",
        "      # Crea un tensore vuoto per i punti originali\n",
        "      all_feats = torch.zeros(pos.shape[0], h.shape[1], device=pos.device) # [B*N, C]\n",
        "\n",
        "      # Conta quanti centroidi sono associati a ciascun punto\n",
        "      counts = torch.zeros(pos.shape[0], 1, device=pos.device) # [B*N, 1]\n",
        "\n",
        "      # Somma le feature di ciascun centroide a tutti i punti vicini\n",
        "      all_feats.index_add_(0, edge_index[0], h[edge_index[1]])\n",
        "      counts.index_add_(0, edge_index[0], torch.ones_like(h[edge_index[1]][:, :1]))\n",
        "\n",
        "      # Media (evita divisione per zero)\n",
        "      counts = counts.clamp(min=1)\n",
        "      all_feats = all_feats / counts\n",
        "      return all_feats\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: dict containing 'pointcloud': [B, N, 3] point cloud coordinates\n",
        "        Returns:\n",
        "            multi_scale_features: dict con features a diverse risoluzioni\n",
        "        \"\"\"\n",
        "        pointcloud = batch['pointcloud']  # [B, N, 3]\n",
        "        B, N, _ = pointcloud.shape\n",
        "\n",
        "        # Flatten pointcloud and create batch indices\n",
        "        pos = pointcloud.view(-1, 3)  # [B*N, 3]\n",
        "        batch_indices = torch.arange(B, device=pos.device).repeat_interleave(N)  # [B*N]\n",
        "\n",
        "        # Extract multi-scale features with proper batch tracking\n",
        "        # [B*800, 3] [B*800, 64] [B*800] output shapes\n",
        "        pos1, h1, batch1, edge_index1 = self.sa1(pos, batch=batch_indices)      # [B*800, 64]\n",
        "        # [B*200, 3] [B*200, 128] [B*200] output shapes\n",
        "        pos2, h2, batch2, edge_index2 = self.sa2(pos1, h1, batch1)             # [B*200, 128]\n",
        "        # [B*50, 3] [B*50, 256] [B*50] output shapes\n",
        "        pos3, h3, batch3, edge_index3 = self.sa3(pos2, h2, batch2)             # [B*50, 256]\n",
        "\n",
        "        h1 = self.feat_proj1(h1)\n",
        "        h2 = self.feat_proj2(h2)\n",
        "        h3 = self.feat_proj3(h3)\n",
        "\n",
        "        h2 = self.upsampling_features(pos, h2, edge_index2)  # [B*800, 128]\n",
        "        h3 = self.upsampling_features(pos2, h3, edge_index3) # [B*200, 256]\n",
        "        h3 = self.upsampling_features(pos, h3, edge_index2) # [B*800, 256]\n",
        "\n",
        "        return {\n",
        "            'level1': {'pos': pos1, 'features': h1, 'batch': batch1},  # Fine details h1 [B*800, 64]\n",
        "            'level2': {'pos': pos2, 'features': h2, 'batch': batch2},  # Medium details h2 [B*800, 128]\n",
        "            'level3': {'pos': pos3, 'features': h3, 'batch': batch3},  # Coarse details h3 [B*800, 256]\n",
        "        }\n",
        "\n",
        "\n",
        "class PointProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Proietta i punti 3D nello spazio immagine usando parametri intrinseci fissi\n",
        "    Assume che i punti siano già nel sistema di coordinate della camera\n",
        "    \"\"\"\n",
        "    def __init__(self, fx=572.4114, fy=573.57043, cx=325.2611, cy=242.04899):\n",
        "        super().__init__()\n",
        "        # Fixed camera intrinsics matrix\n",
        "        self.register_buffer('camera_intrinsics', torch.tensor([\n",
        "            [fx, 0.0, cx],\n",
        "            [0.0, fy, cy],\n",
        "            [0.0, 0.0, 1.0]\n",
        "        ]))\n",
        "\n",
        "    def forward(self, points_3d, bbox_base, padding):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            points_3d: [N, 3] coordinate 3D dei punti nel sistema camera\n",
        "            bbox_base: [4] (x_min, y_min, crop_width, crop_height)\n",
        "            padding: [4] (pad_left, pad_right, pad_top, pad_bottom)\n",
        "        Returns:\n",
        "            pixel_coords: [N, 2] coordinate pixel (u, v) rispetto all'immagine croppata e paddata\n",
        "            valid_mask: [N] mask per punti validi (dentro l'immagine croppata e paddata)\n",
        "        \"\"\"\n",
        "        device = points_3d.device\n",
        "\n",
        "        # Estrai parametri bbox e padding\n",
        "        x_min, y_min, crop_width, crop_height = bbox_base.int()\n",
        "        pad_left, pad_right, pad_top, pad_bottom = padding.int()\n",
        "\n",
        "        # 1. Project to original image plane (640x480)\n",
        "        points_2d_homo = torch.matmul(points_3d, self.camera_intrinsics.T)\n",
        "        pixel_coords_original = points_2d_homo[:, :2] / (points_2d_homo[:, 2:3] + 1e-8)\n",
        "\n",
        "        # 2. Transform to cropped image coordinates\n",
        "        pixel_coords_crop = torch.zeros_like(pixel_coords_original)\n",
        "        pixel_coords_crop[:, 0] = pixel_coords_original[:, 0] - x_min  # u coordinate\n",
        "        pixel_coords_crop[:, 1] = pixel_coords_original[:, 1] - y_min  # v coordinate\n",
        "\n",
        "        # 3. Transform to padded image coordinates\n",
        "        pixel_coords_padded = torch.zeros_like(pixel_coords_crop)\n",
        "        pixel_coords_padded[:, 0] = pixel_coords_crop[:, 0] + pad_left\n",
        "        pixel_coords_padded[:, 1] = pixel_coords_crop[:, 1] + pad_top\n",
        "\n",
        "        # 4. Valid mask checks\n",
        "        # Check depth\n",
        "        valid_depth = points_3d[:, 2] > 0.1  # min depth\n",
        "\n",
        "        # Check if points are within original crop area\n",
        "        valid_in_original_crop = (\n",
        "            (pixel_coords_original[:, 0] >= x_min) &\n",
        "            (pixel_coords_original[:, 0] < x_min + crop_width) &\n",
        "            (pixel_coords_original[:, 1] >= y_min) &\n",
        "            (pixel_coords_original[:, 1] < y_min + crop_height)\n",
        "        )\n",
        "\n",
        "        # Check if points are within padded image bounds\n",
        "        padded_width = crop_width + pad_left + pad_right\n",
        "        padded_height = crop_height + pad_top + pad_bottom\n",
        "\n",
        "        valid_in_padded = (\n",
        "            (pixel_coords_padded[:, 0] >= 0) &\n",
        "            (pixel_coords_padded[:, 0] < padded_width) &\n",
        "            (pixel_coords_padded[:, 1] >= 0) &\n",
        "            (pixel_coords_padded[:, 1] < padded_height)\n",
        "        )\n",
        "\n",
        "        valid_mask = valid_depth & valid_in_original_crop & valid_in_padded\n",
        "\n",
        "        return pixel_coords_padded, valid_mask\n",
        "\n",
        "\n",
        "class PixelWiseFusionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Network principale per fusion pixel-wise di features geometriche e visive\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 geometric_dims=[64, 128, 256],\n",
        "                 image_backbone='resnet18',\n",
        "                 fx=572.4114, fy=573.57043, cx=325.2611, cy=242.04899):\n",
        "        super().__init__()\n",
        "\n",
        "        self.geometric_dims = geometric_dims\n",
        "        self.sample_img_features_dim = 256\n",
        "\n",
        "        # Geometric feature extractor\n",
        "        self.geometric_extractor = GeometricFeatureExtractor(geometric_dims)\n",
        "\n",
        "        # Point projector with fixed intrinsics\n",
        "        self.point_projector = PointProjector(fx, fy, cx, cy)\n",
        "\n",
        "        # Image feature extractor (CNN backbone)\n",
        "        if image_backbone == 'resnet18':\n",
        "            self.backbone = models.resnet18(weights=\"ResNet18_Weights.IMAGENET1K_V1\")\n",
        "            self.image_encoder = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "            image_feat_dim = 512\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Backbone {image_backbone} not implemented\")\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.features_reduction = nn.Sequential(\n",
        "            nn.Conv2d(image_feat_dim, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, self.sample_img_features_dim, 1),\n",
        "            nn.BatchNorm2d(self.sample_img_features_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: dictionary containing:\n",
        "                - cropped_img: [B, 3, H, W] immagini RGB croppate e paddate\n",
        "                - pointcloud: [B, N, 3] coordinate 3D\n",
        "                - bbox_base: [B, 4] bbox parameters\n",
        "                - paddings: [B, 4] padding parameters\n",
        "        Returns:\n",
        "            fused_features: [B, fusion_dim, H, W] feature fuse pixel-wise\n",
        "        \"\"\"\n",
        "        B, _, H, W = batch['cropped_img'].shape\n",
        "        image = batch['cropped_img']\n",
        "\n",
        "        # Extract geometric features for all batch items\n",
        "        geometric_features_dict = self.geometric_extractor(batch)\n",
        "\n",
        "        # Extract image features\n",
        "        image_features = self.image_encoder(image)  # [B, 512, H_pad/32, W_pad/32]\n",
        "        image_features = self.features_reduction(image_features)  # [B, 256, H_pad/32, W_pad/32]\n",
        "        image_features = F.interpolate(image_features, size=(H, W), mode='bilinear', align_corners=False) # [B, 256, H_pad, W_pad]\n",
        "\n",
        "        # Initialize fusion feature maps for each level\n",
        "        fusion_maps = []\n",
        "\n",
        "        geometric_features = torch.cat([level_data['features'] for level_data in geometric_features_dict.values()], dim=1) # [B*N, 448]\n",
        "\n",
        "        level_data = geometric_features_dict['level1']\n",
        "        points_3d = level_data['pos']  # All points for all batches [B*N, 3]\n",
        "        points_batch = level_data['batch']  # Batch assignment for each point [B*N]\n",
        "\n",
        "        # Process each batch separately\n",
        "        for b in range(B):\n",
        "            # Get points belonging to this batch\n",
        "            batch_mask = points_batch == b\n",
        "\n",
        "            batch_points = points_3d[batch_mask]  # [N_b, 3]\n",
        "            batch_geom_feats = geometric_features[batch_mask]  # [N_b, 448]\n",
        "            bbox_base = batch['bbox_base'][b]  # [4]\n",
        "            padding = batch['paddings'][b]  # [4]\n",
        "\n",
        "            # Project points to padded image coordinates\n",
        "            pixel_coords, valid_mask = self.point_projector(batch_points, bbox_base, padding)\n",
        "\n",
        "            # Keep only valid points and features\n",
        "            valid_points = batch_points[valid_mask] # [N_valid, 3]\n",
        "            valid_geom_feats = batch_geom_feats[valid_mask] # [N_valid, feat_geom]\n",
        "            valid_pixels = pixel_coords[valid_mask] # [N_valid, 2]\n",
        "\n",
        "            if len(valid_points) == 0:\n",
        "                print(f\"[DEBUG] No valid points for batch {b} at level {level_name}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "                #########DA QUIIII\n",
        "\n",
        "                # Map geometric features to pixels\n",
        "                u_coords = torch.clamp(valid_pixels[:, 0].long(), 0, W-1)\n",
        "                v_coords = torch.clamp(valid_pixels[:, 1].long(), 0, H-1)\n",
        "\n",
        "                # Simple assignment (last point wins if multiple points map to same pixel)\n",
        "                for i in range(len(valid_geom_feats)):\n",
        "                    u, v = u_coords[i], v_coords[i]\n",
        "                    pixel_geom_features[:, v, u] = valid_geom_feats[i]\n",
        "\n",
        "                # Prepare features for fusion\n",
        "                # Geometric features: [geom_dim, H, W] -> [H*W, geom_dim]\n",
        "                pixel_geom_flat = pixel_geom_features.permute(1, 2, 0).reshape(H*W, self.geometric_dims[level_idx])\n",
        "\n",
        "                # Image features for this batch: [img_dim, H, W] -> [H*W, img_dim]\n",
        "                image_feat_flat = image_features[b].permute(1, 2, 0).reshape(H*W, image_features.size(1))\n",
        "\n",
        "                # Concatenate geometric and image features\n",
        "                combined_feats = torch.cat([pixel_geom_flat, image_feat_flat], dim=1)  # [H*W, geom_dim + img_dim]\n",
        "\n",
        "                # Apply fusion layer\n",
        "                fused_feats = self.fusion_layers[level_name](combined_feats)  # [H*W, fusion_dim]\n",
        "\n",
        "                # Reshape back to spatial dimensions\n",
        "                level_fusion_features[b] = fused_feats.reshape(H, W, self.fusion_dim).permute(2, 0, 1)  # [fusion_dim, H, W]\n",
        "\n",
        "            fusion_maps.append(level_fusion_features)\n",
        "\n",
        "        # Aggregate multi-level features\n",
        "        multi_level_features = torch.cat(fusion_maps, dim=1)  # [B, fusion_dim*3, H, W]\n",
        "        final_features = self.feature_aggregator(multi_level_features)\n",
        "\n",
        "        if self.output_classes is not None:\n",
        "            output = self.output_head(final_features)\n",
        "            return final_features, output\n",
        "\n",
        "        return final_features\n",
        "\n",
        "class PoseEstimationPipeline(nn.Module):\n",
        "    \"\"\"\n",
        "    Pipeline per stima di posa dense seguendo il paper DenseFusion:\n",
        "    - Feature pixel-wise -> Global features via MLP + AvgPool\n",
        "    - Concatenazione global + pixel-wise features\n",
        "    - Predizione per-pixel di posa + confidenza\n",
        "    - Argmax per selezione finale\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 geometric_dims=[64, 128, 256],\n",
        "                 fusion_dim=256,\n",
        "                 fx=572.4114, fy=573.57043, cx=325.2611, cy=242.04899,\n",
        "                 img_width=640, img_height=480):\n",
        "        super().__init__()\n",
        "\n",
        "        # Fusion network (senza output_classes)\n",
        "        self.fusion_network = PixelWiseFusionNetwork(\n",
        "            geometric_dims=geometric_dims,\n",
        "            fusion_dim=fusion_dim,\n",
        "            output_classes=None,  # Non serve per pose estimation\n",
        "            fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "            img_width=img_width, img_height=img_height\n",
        "        )\n",
        "\n",
        "        # MLP per ottenere global features dalle feature pixel-wise\n",
        "        self.global_feature_mlp = nn.Sequential(\n",
        "            nn.Conv2d(fusion_dim, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Global pooling per ridurre le dimensioni spaziali\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # MLP finale per predizioni per-pixel\n",
        "        # Input: fusion_dim (pixel features) + 128 (global features)\n",
        "        self.final_mlp_quat = nn.Sequential(\n",
        "            nn.Conv2d(fusion_dim + 128, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.final_mlp_rot = nn.Sequential(\n",
        "            nn.Conv2d(fusion_dim + 128, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.final_mlp_conf = nn.Sequential(\n",
        "            nn.Conv2d(fusion_dim + 128, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Head per rotazione (quaternion per ogni pixel)\n",
        "        self.rotation_head = nn.Conv2d(128, 4, 1)  # [B, 4, H, W]\n",
        "\n",
        "        # Head per traslazione (per ogni pixel)\n",
        "        self.translation_head = nn.Conv2d(128, 3, 1)  # [B, 3, H, W]\n",
        "\n",
        "        # Head per confidenza (per ogni pixel)\n",
        "        self.confidence_head = nn.Conv2d(128, 1, 1)  # [B, 1, H, W]\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch_data: dict contenente point_cloud, image, batch\n",
        "        Returns:\n",
        "            rotation_quaternion: [B, 4] quaternion finale (dopo argmax)\n",
        "            translation: [B, 3] traslazione finale (dopo argmax)\n",
        "            pixel_predictions: dict con tutte le predizioni pixel-wise per debugging/loss\n",
        "        \"\"\"\n",
        "        B = batch_data['cropped_img'].shape[0]\n",
        "\n",
        "        # 1. Estrai feature fuse pixel-wise\n",
        "        pixel_features = self.fusion_network(batch_data)  # [B, fusion_dim, H, W]\n",
        "        H, W = pixel_features.shape[2], pixel_features.shape[3]\n",
        "\n",
        "        # 2. Calcola global features\n",
        "        global_feat_intermediate = self.global_feature_mlp(pixel_features)  # [B, 128, H, W]\n",
        "        global_features = self.avg_pool(global_feat_intermediate)  # [B, 128, 1, 1]\n",
        "\n",
        "        # 3. Espandi global features per ogni pixel\n",
        "        global_features_expanded = global_features.expand(-1, -1, H, W)  # [B, 128, H, W]\n",
        "\n",
        "        # 4. Concatena pixel features + global features\n",
        "        combined_features = torch.cat([pixel_features, global_features_expanded], dim=1)  # [B, fusion_dim+128, H, W]\n",
        "\n",
        "        # 5. MLP finale\n",
        "        # final_features = self.final_mlp(combined_features)  # [B, 128, H, W]\n",
        "\n",
        "        # 6. Predizioni per ogni pixel\n",
        "        pixel_rotations = self.rotation_head(self.final_mlp_quat(combined_features))  # [B, 4, H, W]\n",
        "        pixel_translations = self.translation_head(self.final_mlp_rot(combined_features))  # [B, 3, H, W]\n",
        "        pixel_confidences = self.confidence_head(self.final_mlp_conf(combined_features))  # [B, 1, H, W]\n",
        "\n",
        "        # 7. Applica sigmoid alla confidenza per normalizzarla in [0,1]\n",
        "        pixel_confidences = torch.sigmoid(pixel_confidences)\n",
        "\n",
        "        # 8. Normalizza i quaternion per ogni pixel\n",
        "        pixel_rotations_norm = F.normalize(pixel_rotations, p=2, dim=1)  # [B, 4, H, W]\n",
        "\n",
        "        # 9. Argmax sulla confidenza per selezionare il pixel migliore\n",
        "        confidence_flat = pixel_confidences.view(B, -1)  # [B, H*W]\n",
        "        best_pixel_indices = torch.argmax(confidence_flat, dim=1)  # [B]\n",
        "\n",
        "        # 10. Estrai le predizioni finali usando gli indici del best pixel\n",
        "        final_rotations = torch.zeros(B, 4, device=pixel_rotations.device)\n",
        "        final_translations = torch.zeros(B, 3, device=pixel_translations.device)\n",
        "        final_confidences = torch.zeros(B, device=pixel_confidences.device)\n",
        "\n",
        "        for b in range(B):\n",
        "            best_idx = best_pixel_indices[b]\n",
        "            h_idx = best_idx // W\n",
        "            w_idx = best_idx % W\n",
        "\n",
        "            final_rotations[b] = pixel_rotations_norm[b, :, h_idx, w_idx]\n",
        "            final_translations[b] = pixel_translations[b, :, h_idx, w_idx]\n",
        "            final_confidences[b] = pixel_confidences[b, 0, h_idx, w_idx]\n",
        "\n",
        "        # Dizionario con tutte le predizioni pixel-wise (utile per loss e debugging)\n",
        "        pixel_predictions = {\n",
        "            'rotations': pixel_rotations_norm,  # [B, 4, H, W]\n",
        "            'translations': pixel_translations,  # [B, 3, H, W]\n",
        "            'confidences': pixel_confidences,   # [B, 1, H, W]\n",
        "            'best_pixel_indices': best_pixel_indices,  # [B]\n",
        "            'best_confidences': final_confidences  # [B]\n",
        "        }\n",
        "\n",
        "        return final_rotations, final_translations, pixel_predictions\n",
        "\n",
        "    def get_pose_at_pixel(self, batch_data, pixel_coords):\n",
        "        \"\"\"\n",
        "        Utility function per ottenere la posa predetta a coordinate pixel specifiche\n",
        "\n",
        "        Args:\n",
        "            batch_data: input batch\n",
        "            pixel_coords: [B, 2] coordinate (u, v) per ogni batch item\n",
        "        Returns:\n",
        "            rotations: [B, 4] quaternion alle coordinate specificate\n",
        "            translations: [B, 3] traslazioni alle coordinate specificate\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            B = batch_data['cropped_img'].shape[0]\n",
        "\n",
        "            # Forward per ottenere tutte le predizioni pixel-wise\n",
        "            _, _, pixel_predictions = self.forward(batch_data)\n",
        "\n",
        "            pixel_rotations = pixel_predictions['rotations']  # [B, 4, H, W]\n",
        "            pixel_translations = pixel_predictions['translations']  # [B, 3, H, W]\n",
        "\n",
        "            # Estrai predizioni alle coordinate specificate\n",
        "            rotations = torch.zeros(B, 4, device=pixel_rotations.device)\n",
        "            translations = torch.zeros(B, 3, device=pixel_translations.device)\n",
        "\n",
        "            for b in range(B):\n",
        "                u, v = pixel_coords[b]\n",
        "                u, v = int(u.clamp(0, pixel_rotations.shape[3]-1)), int(v.clamp(0, pixel_rotations.shape[2]-1))\n",
        "                rotations[b] = pixel_rotations[b, :, v, u]\n",
        "                translations[b] = pixel_translations[b, :, v, u]\n",
        "\n",
        "            return rotations, translations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ADDMetric:\n",
        "    \"\"\" ADD metric to evaluate prediction of the NN comparing it with the CAD model.\n",
        "        returns the average distance between the predicted pose and the ground truth pose.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, test_loader, models_3D_dir, symmetric_objects=None, device='cuda', experiment=None, config=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model (torch model): model to evaluate\n",
        "            test_loader (dataloader): test loader\n",
        "            models_3D_dir (str): path to the directory containing 3D models\n",
        "            symmetric_objects (list, optional): list of object_id that are symmetric. Defaults to None.\n",
        "            device (str, optional): device to use. Defaults to 'cuda'.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.test_loader = test_loader\n",
        "        self.models_3D_dir = models_3D_dir\n",
        "        self.symmetric_objects = symmetric_objects or []\n",
        "        self.device = device\n",
        "        self.models_points_dict = self.load_models_points(self.models_3D_dir)\n",
        "        self.dict_diameters = self.load_diameters()\n",
        "        self.experiment = experiment\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "    def load_diameters(self):\n",
        "        dict_diameters = {}\n",
        "        with open('/content/datasets/linemod/DenseFusion/Linemod_preprocessed/models/models_info.yml', 'r') as f:\n",
        "            fl = yaml.load(f, Loader=yaml.CLoader)\n",
        "\n",
        "        for k,v in fl.items():\n",
        "            dict_diameters[f'{k:02d}'] = v['diameter']/1000.0\n",
        "        return dict_diameters\n",
        "\n",
        "\n",
        "    def load_models_points(self, models_dir):\n",
        "        \"\"\"Load the 3D model points (vertices) for the LINEMOD dataset in a dictionary {class_name: points}.\n",
        "\n",
        "        Args:\n",
        "            models_dir (_type_): path of the .ply files\n",
        "\n",
        "        Returns:\n",
        "            model_points_dict: dictionary of model points for each object class\n",
        "        \"\"\"\n",
        "        model_points_dict = {}\n",
        "        class_names = [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]\n",
        "\n",
        "        for obj_id in class_names:\n",
        "            model_path = os.path.join(models_dir, f'obj_{obj_id}.ply')\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    # Carica il modello 3D\n",
        "                    mesh = trimesh.load(model_path)\n",
        "\n",
        "                    # Estrai punti dalla superficie o usa vertices\n",
        "                    if hasattr(mesh, 'vertices') and mesh.vertices is not None:\n",
        "                        points = torch.tensor(mesh.vertices/1000.0, dtype=torch.float32).to(device)\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    # Check for NaN or infinite values\n",
        "                    if torch.any(torch.isnan(points)) or torch.any(torch.isinf(points)):\n",
        "                        # Remove NaN/Inf points\n",
        "                        valid_mask = ~(torch.any(torch.isnan(points), dim=1) | torch.any(torch.isinf(points), dim=1))\n",
        "                        points = points[valid_mask].to(device)\n",
        "\n",
        "                    if len(points) == 0:\n",
        "                        print(f\"No valid points found for object {obj_id}\")\n",
        "                        continue\n",
        "                    model_points_dict[obj_id] = points\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading model {model_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return model_points_dict\n",
        "\n",
        "    def compute_add(self, pred_pose, gt_pose, object_id, threshold=0.1):\n",
        "        \"\"\"\n",
        "        Compute ADD metric for a single prediction\n",
        "\n",
        "        Args:\n",
        "            pred_pose (tuple): (translation, rotation_matrix)\n",
        "            gt_pose (tuple): (translation, rotation_matrix)\n",
        "            object_id (str): ID of the object\n",
        "            threshold (float): threshold to consider the prediction correct\n",
        "        \"\"\"\n",
        "        pred_trans, pred_rot = pred_pose\n",
        "        gt_trans, gt_rot = gt_pose\n",
        "\n",
        "        # Check for NaN values in poses\n",
        "        if torch.any(torch.isnan(pred_trans)) or torch.any(torch.isnan(pred_rot)):\n",
        "            print(f\"NaN values found in predicted pose in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        if torch.any(torch.isnan(gt_trans)) or torch.any(torch.isnan(gt_rot)):\n",
        "            print(f\"NaN values found in ground truth pose in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        try:\n",
        "        # Convert Torch quaternion [x, y, z, w] -> numpy.quaternion(w, x, y, z)\n",
        "            pred_quat = np.quaternion(pred_rot[3].item(), pred_rot[0].item(), pred_rot[1].item(), pred_rot[2].item())\n",
        "            gt_quat = np.quaternion(gt_rot[3].item(), gt_rot[0].item(), gt_rot[1].item(), gt_rot[2].item())\n",
        "\n",
        "            # Get rotation matrices (3x3 numpy)\n",
        "            pred_rot = torch.tensor(quaternion.as_rotation_matrix(pred_quat), dtype=torch.float32, device=pred_rot.device)\n",
        "            gt_rot = torch.tensor(quaternion.as_rotation_matrix(gt_quat), dtype=torch.float32, device=gt_rot.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting quaternion to rotation matrix for object {object_id}: {e}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        # Take 3D model points\n",
        "        if object_id not in self.models_points_dict:\n",
        "            print(f\"Object with object id {object_id} is not present in models_point_dict\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        model_points = self.models_points_dict[object_id]  # Shape: (N, 3)\n",
        "\n",
        "        # Ensure proper shapes\n",
        "        if pred_rot.shape != (3, 3) or gt_rot.shape != (3, 3):\n",
        "            print(f\"Invalid shape for rotation in object {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "        try:\n",
        "            # Transform points with the predicted pose\n",
        "            pred_points = torch.matmul(model_points, pred_rot.T) + pred_trans.reshape(1, 3)\n",
        "\n",
        "            # Transform points with the gt pose\n",
        "            gt_points = torch.matmul(model_points, gt_rot.T) + gt_trans.reshape(1, 3)\n",
        "\n",
        "            # Check for NaN in transformed points\n",
        "            if torch.any(torch.isnan(pred_points)) or torch.any(torch.isnan(gt_points)):\n",
        "                print(f\"NaN values found in transformed points in object {object_id}\")\n",
        "                return torch.nan, False\n",
        "\n",
        "            # Compute ADD\n",
        "            if object_id in self.symmetric_objects:\n",
        "                # For symmetric objects, compute ADD-S\n",
        "                distances = []\n",
        "                for pred_point in pred_points:\n",
        "                    diffs = gt_points - pred_point.reshape(1, 3)\n",
        "                    point_distances = torch.linalg.norm(diffs, axis=1)\n",
        "                    min_dist = torch.min(point_distances)\n",
        "                    distances.append(min_dist)\n",
        "                distances = torch.tensor(distances)\n",
        "            else:\n",
        "                # For non-symmetric objects, compute standard ADD\n",
        "                distances = torch.linalg.norm(pred_points - gt_points, axis=1)\n",
        "\n",
        "            avg_distance = torch.mean(distances)\n",
        "\n",
        "            # Check if the prediction is correct\n",
        "            is_correct = 1.0 if avg_distance < threshold else 0.0\n",
        "\n",
        "            return avg_distance, is_correct\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore dirante il calcolo ADD per l'oggetto {object_id}\")\n",
        "            return torch.nan, False\n",
        "\n",
        "    def evaluate_model_with_add(self):\n",
        "        \"\"\"Evaluate the model using the ADD metric.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        results = {}\n",
        "        all_distances = []\n",
        "        all_correct = []\n",
        "        total_processed = 0\n",
        "        total_skipped = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(self.test_loader, desc='Evaluating ADD')):\n",
        "                images = batch['cropped_img'].to(self.device)\n",
        "                gt_trans = batch['translation'].to(device)\n",
        "                gt_rot = batch['quaternion'].to(device)\n",
        "                object_ids = batch['obj_id'].to(device)\n",
        "\n",
        "                # Predict\n",
        "                try:\n",
        "                    pred_rot, pred_trans, _ = self.model(batch)\n",
        "\n",
        "                except Exception as e:\n",
        "                    raise ValueError(f\"Error during model prediction: {e}\")\n",
        "\n",
        "                # Compute ADD for each object\n",
        "\n",
        "                for i in range(len(images)):\n",
        "                    obj_id_int = int(object_ids[i])\n",
        "                    obj_id = f\"{obj_id_int:02d}\"\n",
        "\n",
        "                    if obj_id in self.models_points_dict:\n",
        "                        pred_pose = (pred_trans[i], pred_rot[i])\n",
        "                        gt_pose = (gt_trans[i], gt_rot[i])\n",
        "\n",
        "                        distance, is_correct = self.compute_add(\n",
        "                            pred_pose, gt_pose, obj_id, threshold=0.1*self.dict_diameters[obj_id]\n",
        "                        )\n",
        "\n",
        "                        if not torch.isnan(distance):\n",
        "                            all_distances.append(distance)\n",
        "                            all_correct.append(is_correct)\n",
        "                            total_processed += 1\n",
        "\n",
        "                            if obj_id not in results:\n",
        "                                results[obj_id] = {'distances': [], 'correct': []}\n",
        "\n",
        "                            results[obj_id]['distances'].append(distance)\n",
        "                            results[obj_id]['correct'].append(is_correct)\n",
        "                        else:\n",
        "                            total_skipped += 1\n",
        "                    else:\n",
        "                        total_skipped += 1\n",
        "\n",
        "        if len(all_distances) == 0:\n",
        "            print(\"No valid objects found for evaluation.\")\n",
        "            return torch.nan, torch.nan, {}\n",
        "\n",
        "        # Compute overall metrics\n",
        "        all_distances = torch.tensor(all_distances)\n",
        "        all_correct = torch.tensor(all_correct)\n",
        "        overall_add = torch.mean(all_distances)\n",
        "        overall_accuracy = torch.mean(all_correct)\n",
        "\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_metrics({\n",
        "                \"test_add_score\": overall_add,\n",
        "                \"test_accuracy\": overall_accuracy,\n",
        "                \"total_processed\": total_processed,\n",
        "                \"total_skipped\": total_skipped\n",
        "            })\n",
        "\n",
        "        print(f\"\\nOverall ADD: {overall_add:.4f}\")\n",
        "        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "        # Print per-object results\n",
        "        print(\"\\nPer-object results:\")\n",
        "        for obj_id, obj_results in results.items():\n",
        "            obj_add = torch.mean(torch.tensor(obj_results['distances']))\n",
        "            obj_acc = torch.mean(torch.tensor(obj_results['correct']))\n",
        "            num_samples = len(obj_results['distances'])\n",
        "            print(f\"Object {obj_id}: ADD={obj_add:.4f}, Acc={obj_acc:.4f}, Samples={num_samples}\")\n",
        "\n",
        "        return overall_add, overall_accuracy, results"
      ],
      "metadata": {
        "id": "aSRrBAcVXIIG"
      },
      "id": "aSRrBAcVXIIG",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # per visualizzare il progresso\n",
        "# Funzione per invertire la normalizzazione\n",
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Inverte la normalizzazione di un'immagine tensorizzata [C, H, W]\n",
        "    \"\"\"\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1).to(tensor.device)\n",
        "    std = torch.tensor(std).view(-1, 1, 1).to(tensor.device)\n",
        "    return tensor * std + mean\n",
        "\n",
        "# Parametri della normalizzazione usata durante il training\n",
        "mean = [0.3348, 0.3165, 0.3105]\n",
        "std = [0.2521, 0.2496, 0.2502]\n",
        "\n",
        "# Assicurati di avere istanziato il PointProjector con la camera intrinsics\n",
        "projector = PointProjector().to(device)\n",
        "\n",
        "for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Processing batches\")):\n",
        "    pointclouds = batch[\"pointcloud\"]        # [B, N, 3]\n",
        "    bbox_bases = batch[\"bbox_base\"]          # [B, 4]\n",
        "    paddings = batch[\"paddings\"]             # [B, 4]\n",
        "    cropped_imgs = batch[\"cropped_img\"]      # [B, 3, H, W]\n",
        "\n",
        "    B = pointclouds.shape[0]\n",
        "\n",
        "    for i in range(B):\n",
        "        points_3d = pointclouds[i]            # [N, 3]\n",
        "        bbox_base = bbox_bases[i]             # [4]\n",
        "        padding = paddings[i]                 # [4]\n",
        "        cropped_img = cropped_imgs[i]         # [3, H, W]\n",
        "\n",
        "        # Proietta i punti nel piano immagine 2D\n",
        "        pixel_coords, valid_mask = projector(points_3d, bbox_base, padding)\n",
        "\n",
        "        # Logging base\n",
        "        n_valid = valid_mask.sum().item()\n",
        "        if n_valid != 800:\n",
        "            print(f\"[Batch {batch_idx}, Sample {i}] ⚠️ Valid points: {n_valid}/800\")\n",
        "\n",
        "        # (Opzionale) Visualizza risultati per debugging\n",
        "        visualize = False\n",
        "        if visualize:\n",
        "            img_denorm = denormalize(cropped_img, mean, std)\n",
        "            img_np = img_denorm.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "            projected_uv = pixel_coords[valid_mask].detach().cpu().numpy()\n",
        "            # plt.figure(figsize=(5, 5))\n",
        "            # plt.imshow(img_np)\n",
        "            # plt.scatter(projected_uv[:, 0], projected_uv[:, 1], s=2, c='red')\n",
        "            # plt.title(f\"Batch {batch_idx}, Sample {i}\")\n",
        "            # plt.axis(\"off\")\n",
        "            # plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sEkMUqruaNHw",
        "outputId": "0c5548b2-335b-45cd-c7fc-fbe1d2343954"
      },
      "id": "sEkMUqruaNHw",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 1381/1381 [09:25<00:00,  2.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VOczHg2RPnxz",
      "metadata": {
        "id": "VOczHg2RPnxz"
      },
      "source": [
        "##### Pose estimation trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HvdGSMKA_pHY",
      "metadata": {
        "id": "HvdGSMKA_pHY"
      },
      "outputs": [],
      "source": [
        "class PoseEstimationTrainer:\n",
        "    \"\"\"Trainer class for the Pose Estimation model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, train_loader, val_loader, device='cuda', config=None, experiment=None):\n",
        "        \"\"\"summary\n",
        "\n",
        "        Args:\n",
        "            model (torch model): Model to be trained\n",
        "            train_loader (dataloader): train dataloader\n",
        "            val_loader (dataloader): validation dataloader\n",
        "            device (str, optional): cuda or cpu. Defaults to 'cuda'.\n",
        "            config (dict, optional): configuration file. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.config = config or {}\n",
        "\n",
        "        # Loss function e optimizer\n",
        "        self.criterion = PoseLoss(alpha=1.0, beta=1.0)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max= config['num_epochs'], eta_min=1e-6)\n",
        "        # self.grad_accum_steps = self.config.get('grad_accum_steps', 8)\n",
        "\n",
        "\n",
        "        # Tracking delle metriche\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.step = 0\n",
        "\n",
        "        # # Log model architecture to wandb\n",
        "        # if wandb.run is not None:\n",
        "        #     wandb.watch(self.model, log=\"all\", log_freq=50)\n",
        "\n",
        "        self.experiment = experiment\n",
        "        self.num_batches = len(self.train_loader)\n",
        "\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train the model for one epoch.\n",
        "\n",
        "        Returns:\n",
        "            avg_loss (float): average total loss of the epoch\n",
        "            avg_trans_loss (float): average translation loss of the epoch\n",
        "            avg_rot_loss (float): average rotation loss of the epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_trans_loss = 0\n",
        "        total_rot_loss = 0\n",
        "\n",
        "        # Numero totale di batch nel dataloader\n",
        "        # total_batches = len(self.train_loader)\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc='Training')\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            gt_trans = batch['translation']\n",
        "            gt_rot = batch['quaternion']\n",
        "            obj_id = batch['obj_id']\n",
        "\n",
        "            # Forward pass\n",
        "            pred_rot, pred_trans, pixel_predictions = self.model(batch)\n",
        "\n",
        "            # Compute loss (valori originali, non normalizzati)\n",
        "            loss, _, _ = self.criterion(pred_trans, pred_rot, gt_trans, gt_rot, obj_id)\n",
        "\n",
        "            # Normalizza SOLO per il backward (accumulo gradiente)\n",
        "            # loss_normalized = loss / self.grad_accum_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumula le loss originali per le statistiche\n",
        "            total_loss += loss.item()\n",
        "            # total_trans_loss += trans_loss.item()\n",
        "            # total_rot_loss += rot_loss.item()\n",
        "\n",
        "            # Step ogni grad_accum_steps o all'ultimo batch\n",
        "            # if (batch_idx + 1) % self.grad_accum_steps == 0 or (batch_idx + 1) == total_batches:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Logging (usa i valori originali)\n",
        "            if self.experiment is not None:\n",
        "                if batch_idx % 100 == 0:\n",
        "                    self.experiment.log_metrics({\n",
        "                        \"batch_loss\": loss.item(),  # Valore originale\n",
        "                        # \"batch_trans_loss\": trans_loss.item(),  # Valore originale\n",
        "                        # \"batch_rot_loss\": rot_loss.item(),  # Valore originale\n",
        "                        \"learning_rate\": self.optimizer.param_groups[0]['lr'],\n",
        "                        \"step\": self.step\n",
        "                    })\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "            # Update progress bar (valori originali)\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                # 'Trans': f'{trans_loss.item():.4f}',\n",
        "                # 'Rot': f'{rot_loss.item():.4f}',\n",
        "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "            })\n",
        "\n",
        "        # Calcola le medie sui batch totali (non sui step di accumulo)\n",
        "        avg_loss = total_loss / self.num_batches\n",
        "        avg_trans_loss = total_trans_loss / self.num_batches\n",
        "        avg_rot_loss = total_rot_loss / self.num_batches\n",
        "\n",
        "        return avg_loss, avg_trans_loss, avg_rot_loss\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model on the validation set after each epoch.\n",
        "\n",
        "        Returns:\n",
        "            avg_loss (float): average total loss of the epoch\n",
        "            avg_trans_loss (float): average translation loss of the epoch\n",
        "            avg_rot_loss (float): average rotation loss of the epoch\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_trans_loss = 0\n",
        "        total_rot_loss = 0\n",
        "        num_batches = len(self.val_loader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
        "                # images = batch['cropped_img']\n",
        "                gt_trans = batch['translation']\n",
        "                gt_rot = batch['quaternion'] # ['quaternion'] per quaternion ['rotation']\n",
        "                obj_id = batch['obj_id']\n",
        "\n",
        "                # Forward pass\n",
        "                pred_rot, pred_trans, pixel_predictions = self.model(batch)\n",
        "\n",
        "                # Calcola loss\n",
        "                loss, trans_loss, rot_loss = self.criterion(pred_trans, pred_rot, gt_trans, gt_rot, obj_id)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                # total_trans_loss += trans_loss.item()\n",
        "                # total_rot_loss += rot_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        # avg_trans_loss = total_trans_loss / num_batches\n",
        "        # avg_rot_loss = total_rot_loss / num_batches\n",
        "\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_metrics({\n",
        "                \"val_loss\": avg_loss,\n",
        "                # \"val_trans_loss\": avg_trans_loss,\n",
        "                # \"val_rot_loss\": avg_rot_loss,\n",
        "                \"epoch\": len(self.train_losses)\n",
        "            })\n",
        "\n",
        "        return avg_loss, _, _,\n",
        "        # avg_trans_loss, avg_rot_loss\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        \"\"\"Train the model for the specified number of epochs.\n",
        "\n",
        "        Args:\n",
        "            num_epochs (int): number of epochs to train the model for\n",
        "        \"\"\"\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "        if self.experiment is not None:\n",
        "            with self.experiment.train():\n",
        "              watch(self.model)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "              print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "              # Training\n",
        "              train_loss, train_trans_loss, train_rot_loss = self.train_epoch()\n",
        "\n",
        "              # Validation\n",
        "              val_loss, val_trans_loss, val_rot_loss = self.validate()\n",
        "\n",
        "              # Scheduler step\n",
        "              self.scheduler.step()\n",
        "\n",
        "              # Save metrics\n",
        "              self.train_losses.append(train_loss)\n",
        "              self.val_losses.append(val_loss)\n",
        "\n",
        "              # Log epoch metrics to wandb\n",
        "              if self.experiment is not None:\n",
        "                  self.experiment.log_metrics({\n",
        "                      \"epoch\": epoch + 1,\n",
        "                      \"train_loss\": train_loss,\n",
        "                      # \"train_trans_loss\": train_trans_loss,\n",
        "                      # \"train_rot_loss\": train_rot_loss,\n",
        "                      \"val_loss\": val_loss,\n",
        "                      # \"val_trans_loss\": val_trans_loss,\n",
        "                      # \"val_rot_loss\": val_rot_loss,\n",
        "                  })\n",
        "\n",
        "              print(f'Train Loss: {train_loss:.4f}')\n",
        "              print(f'Val Loss: {val_loss:.4f}')\n",
        "              # print(f'Train Loss: {train_loss:.4f} (Trans: {train_trans_loss:.4f}, Rot: {train_rot_loss:.4f})')\n",
        "              # print(f'Val Loss: {val_loss:.4f} (Trans: {val_trans_loss:.4f}, Rot: {val_rot_loss:.4f})')\n",
        "\n",
        "              # Save best model\n",
        "              if val_loss < self.best_val_loss:\n",
        "                  self.best_val_loss = val_loss\n",
        "\n",
        "                  os.makedirs(f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/extension/\", exist_ok=True)\n",
        "\n",
        "                  lr = self.optimizer.param_groups[0]['lr']\n",
        "                  batch_size = self.config.get('batch_size', 32)\n",
        "                  best_model_path = (\n",
        "                      f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/extension/{config['name_saved_file']}_{config['backbone']}\" # quaternion\n",
        "                      f\"_bs{batch_size}.pth\"\n",
        "                  )\n",
        "\n",
        "                  # Save the model\n",
        "                  torch.save({\n",
        "                      'epoch': epoch,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'train_loss': train_loss,\n",
        "                      'val_loss': val_loss,\n",
        "                      'config': self.config\n",
        "                  }, best_model_path)\n",
        "\n",
        "                  # Log model to wandb\n",
        "                  if self.experiment is not None:\n",
        "                      self.experiment.log_metric(\"best_val_loss\", val_loss)\n",
        "\n",
        "                  print(f'Saved best model with val_loss: {val_loss:.4f}')\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    def plot_losses(self):\n",
        "        \"\"\"Plot the training and validation losses.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Log plot to wandb\n",
        "        if self.experiment is not None:\n",
        "            self.experiment.log_image(image_data= plt, name= \"Plot losses\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n8Pao_Smdyqi",
      "metadata": {
        "id": "n8Pao_Smdyqi"
      },
      "outputs": [],
      "source": [
        "class PoseLoss(nn.Module):\n",
        "    \"\"\"Loss function combines translation and rotation\n",
        "        returns a weighted sum of the translation and rotation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0, beta=1.0):\n",
        "        \"\"\"Initialize alpha and beta values.\n",
        "\n",
        "        Args:\n",
        "            alpha (float, optional): Wieght (importance) to give to translation. Defaults to 1.0.\n",
        "            beta (float, optional): Wieght (importance) to give to rotation. Defaults to 1.0.\n",
        "        \"\"\"\n",
        "        super(PoseLoss, self).__init__()\n",
        "        self.alpha = alpha  # peso per translation loss\n",
        "        self.beta = beta    # peso per rotation loss\n",
        "        self.models_dir = \"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/models\"\n",
        "        self.models_dict = self.load_models_points(self.models_dir)\n",
        "\n",
        "\n",
        "    def forward(self, pred_trans, pred_rot, gt_trans, gt_rot, obj_id=None):\n",
        "        \"\"\"Initialize alpha and beta values.\n",
        "\n",
        "          Args:\n",
        "              pred_trans (matrix , optional): Predicted translation.\n",
        "              pred_rot (matrix, optional): Predicted rotation.\n",
        "              gt_trans (matrix, optional): Ground truth translation.\n",
        "              gt_rot (matrix, optional): Ground truth rotation.\n",
        "        \"\"\"\n",
        "        # Translation loss (MSE)\n",
        "        # trans_loss = F.mse_loss(pred_trans, gt_trans)\n",
        "\n",
        "        # Frobenius norm of the differences between matrices\n",
        "        # rot_loss = F.mse_loss(pred_rot, gt_rot.view(-1, 4)) # view(-1,4) per quaternion, view(-1, 3, 3) per rotation\n",
        "        # rot_loss = self.quaternion_loss(pred_rot, gt_rot.view(-1, 4)) # view(-1,4) per quaternion, view(-1, 3, 3) per rotation\n",
        "        total_loss = self.dense_loss(pred_trans, pred_rot, gt_trans, gt_rot, obj_id)\n",
        "\n",
        "\n",
        "        # Total Loss\n",
        "        # total_loss = self.alpha * trans_loss + self.beta * rot_loss\n",
        "\n",
        "        # return total_loss, trans_loss, rot_loss\n",
        "        return total_loss\n",
        "\n",
        "    def quaternion_loss(self, pred_q, gt_q):\n",
        "        pred_q = F.normalize(pred_q, dim=-1)\n",
        "        gt_q = F.normalize(gt_q, dim=-1)\n",
        "        dot = torch.sum(pred_q * gt_q, dim=-1).abs()\n",
        "        return 1 - dot.mean()\n",
        "\n",
        "    def dense_loss(self, pred_trans, pred_rot, gt_trans, gt_rot, obj_id=None):\n",
        "\n",
        "        pred_rot = pred_rot.detach().cpu().numpy()\n",
        "        gt_rot = gt_rot.detach().cpu().numpy() # Also detach gt_rot for consistency if quaternion library is used\n",
        "\n",
        "        # Now convert the arrays of quaternion objects to rotation matrices\n",
        "        pred_rot = R.from_quat(pred_rot).as_matrix()\n",
        "        gt_rot = R.from_quat(gt_rot).as_matrix()\n",
        "\n",
        "\n",
        "        # Convert back to torch tensors on the correct device\n",
        "        pred_rot_matrix = torch.tensor(pred_rot, dtype=torch.float32, device=device)\n",
        "        gt_rot_matrix = torch.tensor(gt_rot, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Ensure obj_id is used correctly to retrieve model points for each item in the batch\n",
        "        batch_size = pred_trans.shape[0]\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            current_obj_id_int = int(obj_id[i].item()) # Get the integer object ID for this sample\n",
        "            # Convert integer ID to string key format (e.g., 2 -> \"02\")\n",
        "            current_obj_id_str = f\"{current_obj_id_int:02d}\"\n",
        "\n",
        "            if current_obj_id_str not in self.models_dict:\n",
        "                print(f\"Warning: Model points not found for object ID {current_obj_id_str}. Skipping loss calculation for this sample.\")\n",
        "                # Add a small loss or skip this sample based on desired behavior\n",
        "                total_loss += torch.tensor(0.0, device=pred_trans.device) # Add zero loss\n",
        "                continue\n",
        "\n",
        "            model_points = self.models_dict[current_obj_id_str].to(pred_trans.device) # Ensure points are on the same device\n",
        "            num_points = model_points.shape[0]\n",
        "\n",
        "            # Transform points with the predicted pose for the current sample\n",
        "            # Ensure translation is broadcast correctly over the points\n",
        "            pred_points = torch.matmul(model_points, pred_rot_matrix[i].T) + pred_trans[i].reshape(1, 3) # [N, 3]\n",
        "\n",
        "            # Transform points with the gt pose for the current sample\n",
        "            gt_points = torch.matmul(model_points, gt_rot_matrix[i].T) + gt_trans[i].reshape(1, 3) # [N, 3]\n",
        "\n",
        "            # Calculate ADD-S/ADD loss for the current sample\n",
        "            diff = pred_points - gt_points\n",
        "            add_loss_per_point = torch.linalg.norm(diff, dim=1) # Norm along the feature dimension\n",
        "            add_loss_sample = torch.mean(add_loss_per_point) # Average error over points\n",
        "\n",
        "            total_loss += add_loss_sample\n",
        "\n",
        "        # Average loss over the batch\n",
        "        avg_batch_loss = total_loss / batch_size\n",
        "\n",
        "        return avg_batch_loss, _, _\n",
        "\n",
        "    def load_models_points(self, models_dir):\n",
        "        \"\"\"Load the 3D model points (vertices) for the LINEMOD dataset in a dictionary {class_name: points}.\n",
        "\n",
        "        Args:\n",
        "            models_dir (_type_): path of the .ply files\n",
        "\n",
        "        Returns:\n",
        "            model_points_dict: dictionary of model points for each object class\n",
        "        \"\"\"\n",
        "        model_points_dict = {}\n",
        "        class_names = [\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]\n",
        "\n",
        "        for obj_id in class_names:\n",
        "            model_path = os.path.join(models_dir, f'obj_{obj_id}.ply')\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    # Carica il modello 3D\n",
        "                    mesh = trimesh.load(model_path)\n",
        "\n",
        "                    # Estrai punti dalla superficie o usa vertices\n",
        "                    if hasattr(mesh, 'vertices') and mesh.vertices is not None:\n",
        "                        points = torch.tensor(mesh.vertices/1000.0, dtype=torch.float32).to(device)\n",
        "                        sample_points = fps(points, None, ratio=301/points.size(0), random_start=False)[:300]\n",
        "                        points = points[sample_points]\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    # Check for NaN or infinite values\n",
        "                    if torch.any(torch.isnan(points)) or torch.any(torch.isinf(points)):\n",
        "                        # Remove NaN/Inf points\n",
        "                        valid_mask = ~(torch.any(torch.isnan(points), dim=1) | torch.any(torch.isinf(points), dim=1))\n",
        "                        points = points[valid_mask].to(device)\n",
        "\n",
        "\n",
        "                    if len(points) == 0:\n",
        "                        print(f\"No valid points found for object {obj_id}\")\n",
        "                        continue\n",
        "                    model_points_dict[obj_id] = points\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading model {model_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return model_points_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotPose(pathImage, translation_gt, rotation_gt, translation_pred, rotation_pred, experiment=None):\n",
        "\n",
        "    image = cv2.imread(pathImage)\n",
        "    transparent_image = image.copy()\n",
        "\n",
        "    rotat_gt = rotation_gt.to(device).float()\n",
        "    trans_gt = translation_gt.to(device).float()\n",
        "    rotat_pred = rotation_pred.to(device).float()\n",
        "    trans_pred = translation_pred.to(device).float()\n",
        "\n",
        "    image_id = pathImage.split(\"/\")[-1].split(\".\")[0]\n",
        "    label = pathImage.split(\"/\")[-3]\n",
        "    camera_intrinsics = torch.tensor([572.4114, 0.0, 325.2611, 0.0, 573.57043, 242.04899, 0.0, 0.0, 1.0]).reshape(3, 3).to(device)\n",
        "\n",
        "    meshModel = trimesh.load(f\"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/models/obj_{label}.ply\")\n",
        "    vertices = torch.tensor(meshModel.vertices / 1000, dtype=torch.float32).to(device)\n",
        "    min_corner = vertices.min(dim=0).values\n",
        "    max_corner = vertices.max(dim=0).values\n",
        "\n",
        "    bounding_box_3d = torch.tensor([\n",
        "        [min_corner[0], min_corner[1], min_corner[2]],\n",
        "        [max_corner[0], min_corner[1], min_corner[2]],\n",
        "        [max_corner[0], max_corner[1], min_corner[2]],\n",
        "        [min_corner[0], max_corner[1], min_corner[2]],\n",
        "        [min_corner[0], min_corner[1], max_corner[2]],\n",
        "        [max_corner[0], min_corner[1], max_corner[2]],\n",
        "        [max_corner[0], max_corner[1], max_corner[2]],\n",
        "        [min_corner[0], max_corner[1], max_corner[2]],\n",
        "    ], dtype=torch.float32).to(device)\n",
        "\n",
        "    if rotat_gt.numel() == 4:\n",
        "        rotat_gt = torch.tensor(\n",
        "            quaternion.as_rotation_matrix(np.quaternion(*rotat_gt.cpu().numpy())),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "    else:\n",
        "        rotat_gt = rotat_gt.reshape(3, 3)\n",
        "\n",
        "    if rotat_pred.numel() == 4:\n",
        "        rotat_pred = torch.tensor(\n",
        "            quaternion.as_rotation_matrix(np.quaternion(*rotat_pred.cpu().numpy())),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "    else:\n",
        "        rotat_pred = rotat_pred.reshape(3, 3)\n",
        "\n",
        "    axes_3d = torch.tensor([\n",
        "        [0, 0, 0],\n",
        "        [0.15, 0, 0],\n",
        "        [0, 0.15, 0],\n",
        "        [0, 0, 0.15]\n",
        "    ], dtype=torch.float32).to(device)\n",
        "\n",
        "    axes_cam_gt = (rotat_gt @ axes_3d.T).T + trans_gt\n",
        "    bounding_box_3d_cam_gt = (rotat_gt @ bounding_box_3d.T).T + trans_gt\n",
        "\n",
        "    axes_2d_gt = (camera_intrinsics @ axes_cam_gt.T).T\n",
        "    axes_2d_gt = axes_2d_gt[:, :2] / axes_2d_gt[:, 2:3]\n",
        "    bounding_box_2d_gt = (camera_intrinsics @ bounding_box_3d_cam_gt.T).T\n",
        "    bounding_box_2d_gt = (bounding_box_2d_gt[:, :2] / bounding_box_2d_gt[:, 2:3]).int()\n",
        "\n",
        "    p_gt = [tuple(el.cpu().numpy()) for el in bounding_box_2d_gt]\n",
        "    edges = [(0,1), (1,2), (2,3), (3,0), (0,4), (1,5), (2,6), (3,7), (4,5), (5,6), (6,7), (7,4)]\n",
        "    for el in edges:\n",
        "        cv2.line(image, p_gt[el[0]], p_gt[el[1]], (0,0,255), 5)\n",
        "\n",
        "    p0_gt = tuple(axes_2d_gt[0].cpu().int().numpy())\n",
        "    p1_gt = tuple(axes_2d_gt[1].cpu().int().numpy())\n",
        "    p2_gt = tuple(axes_2d_gt[2].cpu().int().numpy())\n",
        "    p3_gt = tuple(axes_2d_gt[3].cpu().int().numpy())\n",
        "\n",
        "    cv2.arrowedLine(image, p0_gt, p1_gt, (0, 0, 255), 2)\n",
        "    cv2.arrowedLine(image, p0_gt, p2_gt, (0, 255, 0), 2)\n",
        "    cv2.arrowedLine(image, p0_gt, p3_gt, (255, 0, 0), 2)\n",
        "\n",
        "    axes_cam_pred = (rotat_pred @ axes_3d.T).T + trans_pred\n",
        "    bounding_box_3d_cam_pred = (rotat_pred @ bounding_box_3d.T).T + trans_pred\n",
        "\n",
        "    axes_2d_pred = (camera_intrinsics @ axes_cam_pred.T).T\n",
        "    axes_2d_pred = axes_2d_pred[:, :2] / axes_2d_pred[:, 2:3]\n",
        "    bounding_box_2d_pred = (camera_intrinsics @ bounding_box_3d_cam_pred.T).T\n",
        "    bounding_box_2d_pred = (bounding_box_2d_pred[:, :2] / bounding_box_2d_pred[:, 2:3]).int()\n",
        "\n",
        "    p_pred = [tuple(el.cpu().numpy()) for el in bounding_box_2d_pred]\n",
        "    for el in edges:\n",
        "        cv2.line(image, p_pred[el[0]], p_pred[el[1]], (255, 0, 0), 5)\n",
        "\n",
        "    p0_pred = tuple(axes_2d_pred[0].cpu().int().numpy())\n",
        "    p1_pred = tuple(axes_2d_pred[1].cpu().int().numpy())\n",
        "    p2_pred = tuple(axes_2d_pred[2].cpu().int().numpy())\n",
        "    p3_pred = tuple(axes_2d_pred[3].cpu().int().numpy())\n",
        "\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p1_pred, (0, 0, 255), 2)\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p2_pred, (0, 255, 0), 2)\n",
        "    cv2.arrowedLine(transparent_image, p0_pred, p3_pred, (255, 0, 0), 2)\n",
        "\n",
        "    overlapImage = cv2.addWeighted(transparent_image, 0.5, image, 1, 0)\n",
        "    img = cv2.cvtColor(overlapImage, cv2.COLOR_BGR2RGB)\n",
        "    # plt.imshow(img)\n",
        "    if experiment is not None:\n",
        "      experiment.log_image(image_data=img, name= f\"{label}_{image_id}\")\n",
        "    # plt.title(\"Object Pose Estimation (prediction is transparent)\")\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "lWbhayrhaDIZ"
      },
      "id": "lWbhayrhaDIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "v0hBawiB9Lbl",
      "metadata": {
        "id": "v0hBawiB9Lbl"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lYCNTw73zeCZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYCNTw73zeCZ",
        "outputId": "dbe10d6b-ac20-42c6-bef6-98978d8d71d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Configuration: {'project_name': 'pointnet', 'experiment_name': 'add_loss', 'batch_size': 8, 'num_epochs': 25, 'learning_rate': 0.0001, 'weight_decay': 1e-05, 'backbone': 'resnet18', 'hidden_dim': 512, 'img_size': 224, 'alpha': 1.0, 'beta': 1.0, 'add_threshold': 0.1, 'symmetric_objects': ['10'], 'name_saved_file': 'add_loss_', 'geometric_dims': [64, 128, 256], 'fusion_dim': 128}\n",
            "Total parameters: 12,620,720\n",
            "Trainable parameters: 931,208\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:15<00:00,  1.14it/s, Loss=0.1378, LR=1.00e-04]\n",
            "Validation: 100%|██████████| 296/296 [02:45<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2800\n",
            "Val Loss: 0.2064\n",
            "Saved best model with val_loss: 0.2064\n",
            "\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:20<00:00,  1.13it/s, Loss=0.1802, LR=9.96e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1562\n",
            "Val Loss: 0.2326\n",
            "\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:26<00:00,  1.13it/s, Loss=0.1362, LR=9.84e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1502\n",
            "Val Loss: 0.2028\n",
            "Saved best model with val_loss: 0.2028\n",
            "\n",
            "Epoch 4/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:31<00:00,  1.12it/s, Loss=0.1552, LR=9.65e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:48<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1474\n",
            "Val Loss: 0.1928\n",
            "Saved best model with val_loss: 0.1928\n",
            "\n",
            "Epoch 5/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:31<00:00,  1.12it/s, Loss=0.1197, LR=9.39e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1453\n",
            "Val Loss: 0.1921\n",
            "Saved best model with val_loss: 0.1921\n",
            "\n",
            "Epoch 6/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:27<00:00,  1.12it/s, Loss=0.1159, LR=9.05e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:48<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1432\n",
            "Val Loss: 0.1791\n",
            "Saved best model with val_loss: 0.1791\n",
            "\n",
            "Epoch 7/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:27<00:00,  1.13it/s, Loss=0.1315, LR=8.66e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1424\n",
            "Val Loss: 0.1890\n",
            "\n",
            "Epoch 8/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:35<00:00,  1.12it/s, Loss=0.1435, LR=8.21e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1406\n",
            "Val Loss: 0.1831\n",
            "\n",
            "Epoch 9/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:33<00:00,  1.12it/s, Loss=0.1231, LR=7.70e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1389\n",
            "Val Loss: 0.1863\n",
            "\n",
            "Epoch 10/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:27<00:00,  1.13it/s, Loss=0.1451, LR=7.16e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1387\n",
            "Val Loss: 0.1915\n",
            "\n",
            "Epoch 11/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:35<00:00,  1.12it/s, Loss=0.1586, LR=6.58e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1373\n",
            "Val Loss: 0.1893\n",
            "\n",
            "Epoch 12/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:35<00:00,  1.12it/s, Loss=0.1252, LR=5.98e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1366\n",
            "Val Loss: 0.1861\n",
            "\n",
            "Epoch 13/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:34<00:00,  1.12it/s, Loss=0.1237, LR=5.36e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1359\n",
            "Val Loss: 0.1848\n",
            "\n",
            "Epoch 14/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:33<00:00,  1.12it/s, Loss=0.1587, LR=4.74e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1350\n",
            "Val Loss: 0.1821\n",
            "\n",
            "Epoch 15/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:29<00:00,  1.12it/s, Loss=0.1336, LR=4.12e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:45<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1340\n",
            "Val Loss: 0.1832\n",
            "\n",
            "Epoch 16/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       | 392/1381 [05:48<14:42,  1.12it/s, Loss=0.1250, LR=3.52e-05]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] No valid points for batch 3 at level level3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:34<00:00,  1.12it/s, Loss=0.1381, LR=3.52e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1336\n",
            "Val Loss: 0.1816\n",
            "\n",
            "Epoch 17/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:33<00:00,  1.12it/s, Loss=0.1184, LR=2.94e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1332\n",
            "Val Loss: 0.1913\n",
            "\n",
            "Epoch 18/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:34<00:00,  1.12it/s, Loss=0.1270, LR=2.40e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1321\n",
            "Val Loss: 0.1849\n",
            "\n",
            "Epoch 19/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:35<00:00,  1.12it/s, Loss=0.1447, LR=1.89e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1317\n",
            "Val Loss: 0.1887\n",
            "\n",
            "Epoch 20/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:43<00:00,  1.11it/s, Loss=0.1363, LR=1.44e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1308\n",
            "Val Loss: 0.1894\n",
            "\n",
            "Epoch 21/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  94%|█████████▍| 1299/1381 [19:30<01:13,  1.12it/s, Loss=0.1397, LR=1.05e-05]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] No valid points for batch 7 at level level3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:44<00:00,  1.11it/s, Loss=0.1250, LR=1.05e-05]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1302\n",
            "Val Loss: 0.1890\n",
            "\n",
            "Epoch 22/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:42<00:00,  1.11it/s, Loss=0.1257, LR=7.12e-06]\n",
            "Validation: 100%|██████████| 296/296 [02:47<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1295\n",
            "Val Loss: 0.1906\n",
            "\n",
            "Epoch 23/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:43<00:00,  1.11it/s, Loss=0.1446, LR=4.48e-06]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1302\n",
            "Val Loss: 0.1896\n",
            "\n",
            "Epoch 24/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:40<00:00,  1.11it/s, Loss=0.1558, LR=2.56e-06]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1298\n",
            "Val Loss: 0.1926\n",
            "\n",
            "Epoch 25/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1381/1381 [20:42<00:00,  1.11it/s, Loss=0.1371, LR=1.39e-06]\n",
            "Validation: 100%|██████████| 296/296 [02:46<00:00,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1298\n",
            "Val Loss: 0.1914\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with ADD metric...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating ADD: 100%|██████████| 296/296 [06:14<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall ADD: 0.1699\n",
            "Overall Accuracy: 0.0017\n",
            "\n",
            "Per-object results:\n",
            "Object 06: ADD=0.1659, Acc=0.0000, Samples=177\n",
            "Object 11: ADD=0.1667, Acc=0.0000, Samples=183\n",
            "Object 15: ADD=0.1879, Acc=0.0000, Samples=184\n",
            "Object 01: ADD=0.2007, Acc=0.0000, Samples=186\n",
            "Object 04: ADD=0.1582, Acc=0.0000, Samples=180\n",
            "Object 10: ADD=0.0844, Acc=0.0213, Samples=188\n",
            "Object 09: ADD=0.1574, Acc=0.0000, Samples=188\n",
            "Object 08: ADD=0.1858, Acc=0.0000, Samples=178\n",
            "Object 05: ADD=0.1634, Acc=0.0000, Samples=179\n",
            "Object 14: ADD=0.1945, Acc=0.0000, Samples=184\n",
            "Object 02: ADD=0.1795, Acc=0.0000, Samples=182\n",
            "Object 12: ADD=0.1784, Acc=0.0000, Samples=186\n",
            "Object 13: ADD=0.1882, Acc=0.0000, Samples=173\n",
            "\n",
            "Final Results:\n",
            "ADD Score: 0.1699\n",
            "Accuracy: 0.0017\n",
            "Plot salvati su comet_ml in project: pointnet, experiment: add_loss\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    \"project_name\": \"pointnet\",\n",
        "    \"experiment_name\": \"add_loss\",\n",
        "    \"batch_size\": 8,\n",
        "    \"num_epochs\": 25,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"backbone\": \"resnet18\",\n",
        "    \"hidden_dim\": 512,\n",
        "    \"img_size\": 224,\n",
        "    \"alpha\": 1.0,\n",
        "    \"beta\": 1.0,\n",
        "    \"add_threshold\": 0.1,\n",
        "    \"symmetric_objects\": [\"10\"],\n",
        "    \"name_saved_file\": \"add_loss_\",\n",
        "    \"geometric_dims\" : [64,128,256],\n",
        "    \"fusion_dim\" : 128\n",
        "}\n",
        "\n",
        "MODELS_DIR = \"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/models\"\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Configuration: {config}\")\n",
        "\n",
        "# Modello\n",
        "model = PoseEstimationPipeline(\n",
        "         geometric_dims=config[\"geometric_dims\"],\n",
        "         fusion_dim=config['fusion_dim']\n",
        "     ).to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "experiment = comet_ml.start(\n",
        "    api_key=\"qt32gGMiNUQ0MX8Kc1jhRdvlo\",\n",
        "    project_name=config['project_name'],\n",
        "    experiment_config=comet_ml.ExperimentConfig(\n",
        "        name=config[\"experiment_name\"],\n",
        "        parse_args=False)\n",
        ")\n",
        "\n",
        "experiment.log_parameters(config)\n",
        "\n",
        "# # # --------------------------\n",
        "# from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# # Numero di campioni desiderati nel subset\n",
        "# subset_size = 10\n",
        "# subset_indices = list(range(subset_size))\n",
        "\n",
        "# # Creiamo i subset dei dataset originali\n",
        "# train_subset = Subset(train_loader.dataset, subset_indices)\n",
        "# val_subset = Subset(val_loader.dataset, subset_indices)\n",
        "# test_subset = Subset(test_loader.dataset, subset_indices)\n",
        "\n",
        "# # Creiamo i nuovi DataLoader a partire dai subset\n",
        "# train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=pointcloud_collate_fn)\n",
        "# val_loader = DataLoader(val_subset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=pointcloud_collate_fn)\n",
        "# test_loader = DataLoader(test_subset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=pointcloud_collate_fn)\n",
        "# # ----------------------\n",
        "\n",
        "trainer = PoseEstimationTrainer(model, train_loader, val_loader, device=DEVICE, config=config, experiment= experiment)\n",
        "trainer.train(num_epochs=config[\"num_epochs\"])\n",
        "\n",
        "!mkdir -p /content/checkpoints\n",
        "!cp -r /content/drive/MyDrive/6D_pose_estimation/checkpoints /content\n",
        "\n",
        "\n",
        "checkpoint = torch.load(f\"/content/drive/MyDrive/6D_pose_estimation/checkpoints/extension/{config['name_saved_file']}_{config['backbone']}_bs{config['batch_size']}.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "\n",
        "add_metric = ADDMetric(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    models_3D_dir=MODELS_DIR,\n",
        "    symmetric_objects=config[\"symmetric_objects\"],\n",
        "    device=DEVICE,\n",
        "    experiment= experiment,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Evaluating with ADD metric...\")\n",
        "add_score, accuracy, detailed_results = add_metric.evaluate_model_with_add()\n",
        "\n",
        "\n",
        "print(f\"\\nFinal Results:\\nADD Score: {add_score:.4f}\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "test_batch = next(iter(test_loader))\n",
        "\n",
        "for idx, batch in enumerate(test_loader):\n",
        "    gt_trans = batch['translation']\n",
        "    gt_rot = batch['rotation']\n",
        "    object_ids = batch['obj_id']\n",
        "    sample_id = batch[\"sample_id\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_rot, pred_trans, _ = model(batch)\n",
        "\n",
        "      for i in range(len(object_ids)):\n",
        "        if i == 0:\n",
        "          img_path = f\"/content/datasets/linemod/DenseFusion/Linemod_preprocessed/data/{sample_id[i][0]:02d}/rgb/{sample_id[i][1]:04d}.png\"\n",
        "\n",
        "          plotPose(img_path, gt_trans[i], gt_rot[i], pred_trans[i], pred_rot[i], experiment=experiment)\n",
        "print(f\"Plot salvati su comet_ml in project: {config['project_name']}, experiment: {config['experiment_name']}\")\n",
        "\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NlWOXgmlMmuM",
      "metadata": {
        "id": "NlWOXgmlMmuM"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "\n",
        "# Elimina variabili grandi\n",
        "del model\n",
        "del trainer\n",
        "gc.collect()\n",
        "\n",
        "# Libera memoria cache della GPU\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Forza la liberazione della memoria GPU (più aggressivo)\n",
        "try:\n",
        "    torch.cuda.ipc_collect()\n",
        "except:\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6b41fabe",
        "GoX7b-8FF3nd",
        "cqZgXdA6n4FU",
        "15fb7108",
        "912e3ec7",
        "J_nF3utavmIA",
        "3276b3ba",
        "s1sD-RYfaBiS",
        "80313348",
        "Abg6c53ONiO7",
        "NUExAwEnOeBe",
        "9Ga14jl3CwXC",
        "BCVCG6kzC17y",
        "sdfVKnprWbXR",
        "W7wK5uUcMCb7",
        "C8JiNwv2p-ww",
        "VOczHg2RPnxz",
        "v0hBawiB9Lbl"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}