{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c391c5e",
   "metadata": {},
   "source": [
    "Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DenseFusion Folder (Which includes a portion of the LimeMOD dataset)\n",
    "\n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip Linemod_preprocessed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6715c",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d56b",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fe60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d913bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"{path}/DenseFusion/Linemod_preprocessed/data/01/rgb/0000.png\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): #Â used to load and preprocess data\n",
    "    def __init__(self, dataset_root, split='train', train_ratio=0.8, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_root (str): Path to the dataset directory.\n",
    "            split (str): 'train' or 'test'.\n",
    "            train_ratio (float): Percentage of data used for training (default 80%).\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.dataset_root = dataset_root\n",
    "        self.split = split\n",
    "        self.train_ratio = train_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Get list of all samples (folder_id, sample_id)\n",
    "        self.samples = self.get_all_samples()\n",
    "\n",
    "        # Check if samples were found\n",
    "        if not self.samples:\n",
    "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
    "\n",
    "        # Split into training and test sets\n",
    "        self.train_samples, self.test_samples = train_test_split(\n",
    "            self.samples, train_size=self.train_ratio, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        # Select the appropriate split\n",
    "        self.samples = self.train_samples if split == 'train' else self.test_samples\n",
    "\n",
    "        # Define image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def get_all_samples(self):\n",
    "        \"\"\"Retrieve the list of all available sample indices from all folders.\"\"\"\n",
    "        samples = []\n",
    "        for folder_id in range(1, 16):  # Assuming folders are named 01 to 15\n",
    "            folder_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\")\n",
    "            #print(folder_path)\n",
    "            if os.path.exists(folder_path):\n",
    "                # get id of the images\n",
    "                sample_ids = sorted([int(f.split('.')[0]) for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "                samples.extend([(folder_id, sid) for sid in sample_ids])  # Store (folder_id, sample_id)\n",
    "        return samples\n",
    "    \n",
    "    def load_config(self, folder_id):\n",
    "        \"\"\"Load YAML configuration files for camera intrinsics and object info for a specific folder.\"\"\"\n",
    "        camera_intrinsics_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", 'info.yml')\n",
    "        objects_info_path = os.path.join(self.dataset_root, 'models', f\"models_info.yml\")\n",
    "\n",
    "        with open(camera_intrinsics_path, 'r') as f:\n",
    "            camera_intrinsics = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        with open(objects_info_path, 'r') as f:\n",
    "            objects_info = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return camera_intrinsics, objects_info\n",
    "\n",
    "    #Define here some usefull functions to access the data\n",
    "    def load_image(self, img_path):\n",
    "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.transform(img)\n",
    "    \n",
    "    def load_depth(self, depth_path):\n",
    "        \"\"\"Load a depth image and convert to tensor.\"\"\"\n",
    "        depth = np.array(Image.open(depth_path))\n",
    "        return torch.tensor(depth, dtype=torch.float32)\n",
    "    \n",
    "    def load_point_cloud(self, depth, intrinsics):\n",
    "        \"\"\"Convert depth image to point cloud using Open3D.\"\"\"\n",
    "        intrinsics = intrinsics[0]['cam_K'] # take intrinsincs of the first image\n",
    "        h, w = depth.shape\n",
    "        # focal lengths and principal centers\n",
    "        fx, fy, cx, cy = intrinsics[0], intrinsics[4], intrinsics[2], intrinsics[5]\n",
    "\n",
    "        # Generate 3D points\n",
    "        xmap, ymap = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        z = depth / 1000.0  # Convert to meters\n",
    "        x = (xmap - cx) * z / fx\n",
    "        y = (ymap - cy) * z / fy\n",
    "\n",
    "        points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "        return point_cloud\n",
    "\n",
    "    def load_6d_pose(self, folder_id, sample_id):\n",
    "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
    "        pose_file = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"gt.yml\")\n",
    "\n",
    "        # Load the ground truth poses from the gt.yml file\n",
    "        with open(pose_file, 'r') as f:\n",
    "            pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # The pose data is a dictionary where each key corresponds to a frame with pose info\n",
    "        # We assume sample_id corresponds to the key in pose_data\n",
    "        if sample_id not in pose_data:\n",
    "            raise KeyError(f\"Sample ID {sample_id} not found in gt.yml for folder {folder_id}.\")\n",
    "\n",
    "        pose = pose_data[sample_id][0]  # There's only one pose per sample\n",
    "\n",
    "        # Extract translation and rotation\n",
    "        translation = np.array(pose['cam_t_m2c'], dtype=np.float32)  # [3] ---> (x,y,z)\n",
    "        rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
    "        bbox = np.array(pose['obj_bb'], dtype=np.float32) #[4] ---> x_min, y_min, width, height\n",
    "        obj_id = np.array(pose['obj_id'], dtype=np.float32) #[1] ---> label\n",
    "\n",
    "        x_min, y_min, width, height = bbox\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "        # values given in the file are the top left corner\n",
    "        bbox = np.array([x_min, y_min, x_max, y_max], dtype=np.float32) #x_min, y_min, x_max, y_max\n",
    "\n",
    "        return translation, rotation, bbox, obj_id\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the total number of samples in the selected split.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Load a dataset sample.\n",
    "        folder_id, sample_id = self.samples[idx]\n",
    "\n",
    "        # Load the correct camera intrinsics and object info for this folder\n",
    "        camera_intrinsics, objects_info = self.load_config(folder_id)\n",
    "\n",
    "        img_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"rgb/{sample_id:04d}.png\")\n",
    "        depth_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"depth/{sample_id:04d}.png\")\n",
    "\n",
    "        img = self.load_image(img_path)\n",
    "        depth = self.load_depth(depth_path)\n",
    "        point_cloud = self.load_point_cloud(depth.numpy(), camera_intrinsics)\n",
    "        point_cloud = torch.tensor(np.asarray(point_cloud.points), dtype=torch.float32)\n",
    "        translation, rotation, bbox, obj_id = self.load_6d_pose(folder_id, sample_id)\n",
    "\n",
    "        #Dictionary with all the data\n",
    "        return {\n",
    "            \"rgb\": img,\n",
    "            \"depth\": torch.tensor(depth, dtype=torch.float32),\n",
    "            \"point_cloud\": point_cloud,\n",
    "            \"camera_intrinsics\": camera_intrinsics[0]['cam_K'],\n",
    "            \"objects_info\": objects_info,\n",
    "            \"translation\": torch.tensor(translation),\n",
    "            \"rotation\": torch.tensor(rotation),\n",
    "            \"bbox\": torch.tensor(bbox),\n",
    "            \"obj_id\": torch.tensor(obj_id)\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"./Linemod_preprocessed\"\n",
    "\n",
    "train_dataset = CustomDataset(dataset_root, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\") # 12640\n",
    "print(f\"Training Loader samples: {len(train_loader)}\") #Â 12640/batch_size\n",
    "\n",
    "test_dataset = CustomDataset(dataset_root, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Testing samples: {len(test_dataset)}\") # 3160\n",
    "print(f\"Test Loader samples: {len(test_loader)}\") # 3160/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_num_batches = 10 #Â take 10 batches from training (each batch has 4 samples)\n",
    "test_subset_num_batches = 5 # take 5 batches from testing\n",
    "train_subset = list(itertools.islice(train_loader, train_subset_num_batches))\n",
    "test_subset=list(itertools.islice(test_loader, test_subset_num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from the train loader (4 images)\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract relevant data\n",
    "rgb_images = batch[\"rgb\"]         # (B, 3, H, W)\n",
    "bboxes = batch[\"bbox\"]            # (B, 4) in pixel coords: x_min, y_min, x_max, y_max\n",
    "obj_ids = batch[\"obj_id\"]         # (B,)\n",
    "\n",
    "# Convert to numpy and rearrange channels\n",
    "rgb_images = rgb_images.permute(0, 2, 3, 1).numpy()  # (B, H, W, 3)\n",
    "bboxes = bboxes.numpy()\n",
    "obj_ids = obj_ids.numpy()\n",
    "\n",
    "# Plot settings\n",
    "batch_size = rgb_images.shape[0]\n",
    "cols = min(4, batch_size)\n",
    "rows = (batch_size + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    ax = axes[i]\n",
    "    img = rgb_images[i]\n",
    "    x_min, y_min, x_max, y_max = bboxes[i]\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min #---> [x_min, y_min, width, height]\n",
    "    obj_id = obj_ids[i]\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min),   # (x_min, y_min)\n",
    "        width,              # width\n",
    "        height,              # height\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add object ID as label\n",
    "    ax.text(\n",
    "        x_min,\n",
    "        y_min - 10,\n",
    "        f'ID: {int(obj_id)}',\n",
    "        color='yellow',\n",
    "        fontsize=10,\n",
    "        backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "# Hide unused axes if batch_size < cols * rows\n",
    "for j in range(batch_size, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
