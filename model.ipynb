{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde563b",
   "metadata": {},
   "source": [
    "Set some variables to conditionally run some codes. First download the project and change directory to ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNT_DRIVE = False\n",
    "COMET_ML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17727590",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/6DPose_Estimation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdb879",
   "metadata": {},
   "source": [
    "Install all dependencies of PyTorch dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%env TORCH=$torch.__version__\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7f2c7",
   "metadata": {},
   "source": [
    "Install all packages, you may need to restart the runtime before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "print(\"Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "import shutil\n",
    "import ultralytics\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.patches as patches\n",
    "import wandb\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import v2\n",
    "import trimesh\n",
    "\n",
    "# install PyTorch Geometric after installation and restart\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import knn_interpolate, MessagePassing\n",
    "from torch_geometric.nn.pool import fps, radius\n",
    "\n",
    "# import comet-ml\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import watch\n",
    "\n",
    "from utils.data_exploration import load_image\n",
    "from utils.installation_checker import check_torch_geometric\n",
    "\n",
    "from data.CustomDatasetPose import IMG_WIDTH, IMG_HEIGHT\n",
    "\n",
    "# check if everything works\n",
    "check_torch_geometric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba70be",
   "metadata": {},
   "source": [
    "Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.init import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b752f3",
   "metadata": {},
   "source": [
    "Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.init import set_device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806248",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (which includes a portion of the LimeMOD dataset)\n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    !cp /content/drive/MyDrive/6DPose_Estimation /content/ # move to content for faster access to files\n",
    "    %cd /content/6DPose_Estimation/datasets/linemod/DenseFusion\n",
    "\n",
    "!unzip Linemod_preprocessed.zip\n",
    "!rm Linemod_preprocessed.zip\n",
    "%cd ../../../ # change directory to 6D_pose_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fb52",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773553",
   "metadata": {},
   "source": [
    "## Modify Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7f9e4",
   "metadata": {},
   "source": [
    "Copy ground truth files to ```Linemod_preprocessed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import get_class_names\n",
    "from utils.preprocessing import copy_gt_file, change_02gt, quaternion_gt\n",
    "\n",
    "folder_names = get_class_names()\n",
    "copy_gt_file(folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add824a",
   "metadata": {},
   "source": [
    "Change ```02_gt.yml``` to take only one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_02gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed/02_gt.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e793c4",
   "metadata": {},
   "source": [
    "Add quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c090358",
   "metadata": {},
   "outputs": [],
   "source": [
    "quaternion_gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3ec7",
   "metadata": {},
   "source": [
    "Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121399",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(label=1, object=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d93753",
   "metadata": {},
   "source": [
    "Check if camera intrinsics is same for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/data\"\n",
    "\n",
    "from utils.data_exploration import check_cam_K_equal\n",
    "\n",
    "cam_K = check_cam_K_equal(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ce9fc",
   "metadata": {},
   "source": [
    "## Define CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CustomDatasetPose import CustomDatasetPose\n",
    "\n",
    "dataset_root = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/\"\n",
    "\n",
    "train_dataset = CustomDatasetPose(dataset_root, split=\"train\", device=device, cam_K = cam_K)\n",
    "image_mean, image_std = train_dataset.get_image_mean_std()\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDatasetPose(dataset_root, split=\"validation\", device=device, cam_K = cam_K, img_mean = image_mean, img_std = image_std)\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetPose(dataset_root, split=\"test\", device=device, cam_K = cam_K, img_mean = image_mean, img_std = image_std)\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276b3ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869154a1",
   "metadata": {},
   "source": [
    "Structure the data for YOLO such that\n",
    "```\n",
    "datasets/\n",
    "├── data.yaml\n",
    "│\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │\n",
    "│   └── labels/\n",
    "│  \n",
    "├── val/\n",
    "│\n",
    "└── test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into training, validation and testing set\n",
    "train_samples = train_dataset.get_samples_id()\n",
    "validation_samples = val_dataset.get_samples_id()\n",
    "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253d1e",
   "metadata": {},
   "source": [
    "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
    "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
    "\n",
    "The working directory is in the ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76389b",
   "metadata": {},
   "source": [
    "Create YOLO yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec539f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import create_YOLO_yaml, create_dataset_YOLO\n",
    "\n",
    "number_classes, class_names = create_YOLO_yaml(path, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba3d0",
   "metadata": {},
   "source": [
    "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to have easily access to the index\n",
    "index_dict = dict()\n",
    "for index, el in enumerate(class_names):\n",
    "    index_dict[int(el)] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c7b5",
   "metadata": {},
   "source": [
    "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects, but just consider the object of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_df = create_dataset_YOLO(number_classes, train_samples, validation_samples, test_samples, index_dict, path, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b20d",
   "metadata": {},
   "source": [
    "Visualize dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_dataset_distribution\n",
    "\n",
    "load_dataset_distribution(counter_df, index_dict, number_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b007448",
   "metadata": {},
   "source": [
    "Visualize depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_image\n",
    "\n",
    "folder = \"02\"\n",
    "object_name = \"0101\"\n",
    "img = load_depth_image(f\"./datasets/linemod/DenseFusion/Linemod_preprocessed/data/{folder}/depth/{object_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8a540",
   "metadata": {},
   "source": [
    "Plot the patch of first object of the image, it reads from the ground truth file containing also multiple objects in one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_patch\n",
    "\n",
    "load_depth_patch(path, folder, object_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab338446",
   "metadata": {},
   "source": [
    "Get data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"Training loader: {len(train_loader)}\")\n",
    "print(f\"Validation loader: {len(val_loader)}\")\n",
    "print(f\"Test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b86ad",
   "metadata": {},
   "source": [
    "Plot one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef53038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import plot_batch_data\n",
    "\n",
    "plot_batch_data(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80313348",
   "metadata": {},
   "source": [
    "## Training Object Detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_YOLO import train_YOLO\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "IMG_SIZE = 640\n",
    "\n",
    "train_YOLO(path, epochs, batch_size, device, IMG_SIZE) # train model and save it to checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d7a82",
   "metadata": {},
   "source": [
    "Validate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18df963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_YOLO import evaluate_YOLO\n",
    "\n",
    "evaluate_YOLO(path, epochs, batch_size, IMG_SIZE, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425354d6",
   "metadata": {},
   "source": [
    "## Pose Estimator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465dd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.PosePredictorModel import PosePredictorModel\n",
    "from PoseEstimationTrainer import PoseEstimationTrainer\n",
    "from models.ADDMetric import ADDMetric\n",
    "from utils.pose_plot import plotPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84448d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"project_name\": \"baseline_quaternion\",\n",
    "    \"experiment_name\": \"mse_loss_step_optim\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 25,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"hidden_dim\": 512,\n",
    "    \"img_size\": 224,\n",
    "    \"alpha\": 1.0,\n",
    "    \"beta\": 1.0,\n",
    "    \"add_threshold\": 0.1,\n",
    "    \"symmetric_objects\": [\"10\"],\n",
    "    \"name_saved_file\": \"mse_loss_step\"\n",
    "}\n",
    "\n",
    "MODELS_DIR = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/models\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "# # --------------------------\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# # Number of samples in subset\n",
    "# subset_size = 10\n",
    "# subset_indices = list(range(subset_size))\n",
    "\n",
    "# # create subset of original dataset\n",
    "# train_subset = Subset(train_loader.dataset, subset_indices)\n",
    "# val_subset = Subset(val_loader.dataset, subset_indices)\n",
    "# test_subset = Subset(test_loader.dataset, subset_indices)\n",
    "\n",
    "# # create new DataLoader from subset\n",
    "# train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "# val_loader = DataLoader(val_subset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "# test_loader = DataLoader(test_subset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "# # ----------------------\n",
    "\n",
    "\n",
    "# Model\n",
    "model = PosePredictorModel(\n",
    "    backbone=config[\"backbone\"],\n",
    "    hidden_dim=config[\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "experiment = comet_ml.start(\n",
    "    api_key=\"<YOUR_API>\",\n",
    "    project_name=config['project_name'],\n",
    "    experiment_config=comet_ml.ExperimentConfig(\n",
    "        name=config[\"experiment_name\"],\n",
    "        parse_args=False)\n",
    ")\n",
    "\n",
    "experiment.log_parameters(config)\n",
    "\n",
    "trainer = PoseEstimationTrainer(model, train_loader, val_loader, device=device, config=config, experiment=experiment)\n",
    "trainer.train(num_epochs=config[\"num_epochs\"])\n",
    "\n",
    "checkpoint = torch.load(f\"{path}/checkpoints/baseline/{config['name_saved_file']}_{config['backbone']}_bs{config['batch_size']}.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "add_metric = ADDMetric(\n",
    "    model=model,\n",
    "    class_names=class_names,\n",
    "    test_loader=test_loader,\n",
    "    models_3D_dir=MODELS_DIR,\n",
    "    symmetric_objects=config[\"symmetric_objects\"],\n",
    "    device=DEVICE,\n",
    "    experiment=experiment,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Evaluating with ADD metric...\")\n",
    "add_score, accuracy, detailed_results = add_metric.evaluate_model_with_add()\n",
    "\n",
    "\n",
    "print(f\"\\nFinal Results:\\nADD Score: {add_score:.4f}\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "test_batch = next(iter(test_loader))\n",
    "\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    images = batch['rgb'].to(device)\n",
    "    gt_trans = batch['translation']\n",
    "    gt_rot = batch['rotation']\n",
    "    object_ids = batch['obj_id']\n",
    "    sample_id = batch[\"sample_id\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_trans, pred_rot = model(images)\n",
    "        pred_trans = pred_trans\n",
    "        pred_rot = pred_rot\n",
    "       \n",
    "        for i in range(len(images)):\n",
    "            if i == 0:\n",
    "                img_path = f\"{path}/datasets/linemod/DenseFusion/Linemod_preprocessed/data/{sample_id[i][0]:02d}/rgb/{sample_id[i][1]:04d}.png\"\n",
    "\n",
    "                plotPose(img_path, gt_trans[i], gt_rot[i], pred_trans[i], pred_rot[i], experiment, cam_K)\n",
    "print(f\"Plot saved on comet_ml in project: {config['project_name']}, experiment: {config['experiment_name']}\")\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dd776",
   "metadata": {},
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3463ed4",
   "metadata": {},
   "source": [
    "Compare images in ```rgb``` and ```mask``` and analyze if there are images that are only in one of the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4215cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import compare_rgb_mask_in_data\n",
    "\n",
    "compare_rgb_mask_in_data(\"./datasets/linemod/DenseFusion/Linemod_preprocessed/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ff0bd",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741495dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CustomDataset import CustomDataset\n",
    "\n",
    "dataset_root = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/\"\n",
    "\n",
    "train_dataset = CustomDataset(dataset_root, split='train', device=device, cam_K=cam_K)\n",
    "image_mean, image_std = train_dataset.get_image_mean_std()\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "\n",
    "val_dataset = CustomDataset(dataset_root, split='validation', device=device, cam_K = cam_K, img_mean = image_mean, img_std = image_std)\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "\n",
    "test_dataset = CustomDataset(dataset_root, split='test', device=device, cam_K = cam_K, img_mean = image_mean, img_std = image_std)\n",
    "print(f'Testing samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2427a65",
   "metadata": {},
   "source": [
    "Get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af010d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CustomDataLoader import pointcloud_collate_fn\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pointcloud_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=pointcloud_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=pointcloud_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923e6a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
