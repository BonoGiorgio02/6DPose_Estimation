{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde563b",
   "metadata": {},
   "source": [
    "Set some variables to conditionally run some codes. First download the project and change directory to ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNT_DRIVE = False\n",
    "COMET_ML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17727590",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/6DPose_Estimation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdb879",
   "metadata": {},
   "source": [
    "Install all dependencies of PyTorch dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%env TORCH=$torch.__version__\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7f2c7",
   "metadata": {},
   "source": [
    "Install all packages, you may need to restart the runtime before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "print(\"Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "import shutil\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.patches as patches\n",
    "import wandb\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import v2\n",
    "import trimesh\n",
    "\n",
    "# install PyTorch Geometric after installation and restart\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import knn_interpolate, MessagePassing\n",
    "from torch_geometric.nn.pool import fps, radius\n",
    "\n",
    "# import comet-ml\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import watch\n",
    "\n",
    "from utils.data_exploration import load_image\n",
    "from utils.installation_checker import check_torch_geometric\n",
    "\n",
    "from data.CustomDatasetPose import IMG_WIDTH, IMG_HEIGHT\n",
    "\n",
    "# check if everything works\n",
    "check_torch_geometric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b752f3",
   "metadata": {},
   "source": [
    "Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device_setter import set_device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806248",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (which includes a portion of the LimeMOD dataset)\n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    !cp /content/drive/MyDrive/6DPose_Estimation /content/ # move to content for faster access to files\n",
    "    %cd /content/6DPose_Estimation/datasets/linemod/DenseFusion\n",
    "\n",
    "!unzip Linemod_preprocessed.zip\n",
    "!rm Linemod_preprocessed.zip\n",
    "%cd ../../../ # change directory to 6D_pose_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fb52",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773553",
   "metadata": {},
   "source": [
    "## Modify Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7f9e4",
   "metadata": {},
   "source": [
    "Copy ground truth files to ```Linemod_preprocessed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import get_class_names\n",
    "from utils.preprocessing import copy_gt_file, change_02gt, quaternion_gt\n",
    "\n",
    "folder_names = get_class_names()\n",
    "copy_gt_file(folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add824a",
   "metadata": {},
   "source": [
    "Change ```02_gt.yml``` to take only one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_02gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed/02_gt.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e793c4",
   "metadata": {},
   "source": [
    "Add quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c090358",
   "metadata": {},
   "outputs": [],
   "source": [
    "quaternion_gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3ec7",
   "metadata": {},
   "source": [
    "Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121399",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(label=1, object=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d93753",
   "metadata": {},
   "source": [
    "Check if camera intrinsics is same for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/data\"\n",
    "\n",
    "from utils.data_exploration import check_cam_K_equal\n",
    "\n",
    "cam_K = check_cam_K_equal(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ce9fc",
   "metadata": {},
   "source": [
    "## Define CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CustomDatasetPose import CustomDatasetPose\n",
    "\n",
    "dataset_root = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/\"\n",
    "\n",
    "train_dataset = CustomDatasetPose(dataset_root, split=\"train\", device=device, cam_K = cam_K)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDatasetPose(dataset_root, split=\"validation\", device=device, cam_K = cam_K)\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetPose(dataset_root, split=\"test\", device=device, cam_K = cam_K)\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276b3ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869154a1",
   "metadata": {},
   "source": [
    "Structure the data for YOLO such that\n",
    "```\n",
    "datasets/\n",
    "├── data.yaml\n",
    "│\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │\n",
    "│   └── labels/\n",
    "│  \n",
    "├── val/\n",
    "│\n",
    "└── test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into training, validation and testing set\n",
    "train_samples = train_dataset.get_samples_id()\n",
    "validation_samples = val_dataset.get_samples_id()\n",
    "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253d1e",
   "metadata": {},
   "source": [
    "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
    "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
    "\n",
    "The working directory is in the ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76389b",
   "metadata": {},
   "source": [
    "Create YOLO yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec539f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import create_YOLO_yaml, create_dataset_YOLO\n",
    "\n",
    "number_classes, class_names = create_YOLO_yaml(path, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba3d0",
   "metadata": {},
   "source": [
    "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to have easily access to the index\n",
    "index_dict = dict()\n",
    "for index, el in enumerate(class_names):\n",
    "    index_dict[int(el)] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c7b5",
   "metadata": {},
   "source": [
    "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects, but just consider the object of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_df = create_dataset_YOLO(number_classes, train_samples, validation_samples, test_samples, index_dict, path, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b20d",
   "metadata": {},
   "source": [
    "Visualize dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_dataset_distribution\n",
    "\n",
    "load_dataset_distribution(counter_df, index_dict, number_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b007448",
   "metadata": {},
   "source": [
    "Visualize depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_image\n",
    "\n",
    "folder = \"02\"\n",
    "object_name = \"0101\"\n",
    "img = load_depth_image(f\"{path}/datasets/linemod/DenseFusion/Linemod_preprocessed/data/{folder}/depth/{object_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8a540",
   "metadata": {},
   "source": [
    "Plot the patch of first object of the image, it reads from the ground truth file containing also multiple objects in one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_patch\n",
    "\n",
    "load_depth_patch(path, folder, object_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab338446",
   "metadata": {},
   "source": [
    "Get data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"Training loader: {len(train_loader)}\")\n",
    "print(f\"Validation loader: {len(val_loader)}\")\n",
    "print(f\"Test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b86ad",
   "metadata": {},
   "source": [
    "Plot one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef53038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import plot_batch_data\n",
    "\n",
    "plot_batch_data(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80313348",
   "metadata": {},
   "source": [
    "## Training Object Detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = f\"{path}/checkpoints/yolo11n.pt\"\n",
    "model = YOLO(model_path)\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "IMG_SIZE = 640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396f8e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03517373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model will automatically scale the image and related bounding box according to imgsz\n",
    "results = model.train(data=f\"{path}/datasets/data.yaml\", epochs=epochs, batch=batch_size, device=device,\n",
    "        imgsz=IMG_SIZE,\n",
    "        augment=True,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        hsv_h=0.4,\n",
    "        hsv_s=0.4,\n",
    "        hsv_v=0.4,\n",
    "        degrees=120,\n",
    "        translate=0.1,\n",
    "        scale=0.5,\n",
    "        shear=20,\n",
    "        perspective=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c08d2b",
   "metadata": {},
   "source": [
    "Copy model file to ```checkpoints```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(f\"./runs/detect/train/weights/best.pt\", f\"../../../checkpoints/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d7a82",
   "metadata": {},
   "source": [
    "Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../checkpoints/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "results = model.val(\n",
    "        data=f\"{path}/datasets/data.yaml\",\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6419a45",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../checkpoints/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "results = model.val(\n",
    "        data=f\"{path}/datasets/data.yaml\",\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=device,\n",
    "        split=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425354d6",
   "metadata": {},
   "source": [
    "## Pose Estimator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"./YOLO/datasets_cropped\", exist_ok=True)\n",
    "\n",
    "# input: path YOLO/datasets\n",
    "# output: save images\n",
    "\n",
    "def mask_outside_bbox(path=\"./YOLO/datasets\", color=(0, 0, 0)):\n",
    "\n",
    "    for folder in os.scandir(path):\n",
    "        if folder.is_dir():\n",
    "            os.makedirs(f\"./YOLO/datasets_cropped/{folder.name}\", exist_ok=True)\n",
    "            path_dir= os.path.join(path, folder.name)\n",
    "            images_path = os.path.join(path_dir, \"images\")\n",
    "            labels_path = os.path.join(path_dir, \"labels\")\n",
    "\n",
    "            for img_name, label_name in zip(sorted(os.listdir(images_path)), sorted(os.listdir(labels_path))):\n",
    "                img_path = os.path.join(images_path, img_name)\n",
    "                label_path = os.path.join(labels_path, label_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                with open(label_path, \"r\") as f:\n",
    "                    _, x_center, y_center, w, h = map(float,f.readline().split(\" \"))\n",
    "                f.close()\n",
    "                \n",
    "                img_height, img_width, _ = img.shape\n",
    "                \n",
    "                x1 = round((x_center - w / 2) * img_width)\n",
    "                y1 = round((y_center - h / 2) * img_height)\n",
    "                x2 = round((x_center + w / 2) * img_width)\n",
    "                y2 = round((y_center + h / 2) * img_height)\n",
    "                \n",
    "                img_masked = np.full_like(img, color)\n",
    "                img_masked[y1:y2, x1:x2] = img[y1:y2, x1:x2]\n",
    "                \n",
    "                cv2.imwrite(f\"./YOLO/datasets_cropped/{folder.name}/{img_name}\",img=img_masked)   \n",
    "             \n",
    "mask_outside_bbox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063b816",
   "metadata": {},
   "source": [
    "Copy the gt.yaml and info.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c76097",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in class_names:\n",
    "    shutil.copy(f\"./DenseFusion/Linemod_preprocessed/data/{el}/gt.yml\", f\"./YOLO/datasets_cropped/{el}_gt.yml\")\n",
    "    shutil.copy(f\"./DenseFusion/Linemod_preprocessed/data/{el}/info.yml\", f\"./YOLO/datasets_cropped/{el}_info.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetPose(Dataset): # used to load and preprocess data\n",
    "    def __init__(self, dataset_root, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_root (str): Path to the dataset directory.\n",
    "            split (str): 'train', 'validation' or 'test'.\n",
    "        \"\"\"\n",
    "        self.dataset_root = dataset_root\n",
    "        if split == \"train\":\n",
    "            self.split = split\n",
    "        elif split == \"validation\":\n",
    "            self.split = \"val\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "\n",
    "        # Get list of all samples (folder_id, sample_id)\n",
    "        self.samples = self.get_all_samples()\n",
    "\n",
    "        # Check if samples were found\n",
    "        if not self.samples:\n",
    "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
    "\n",
    "        # Define image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def get_samples_id(self):\n",
    "        return self.samples\n",
    "\n",
    "    def get_all_samples(self):\n",
    "        \"\"\"Retrieve the list of all available sample indices from all folders.\"\"\"\n",
    "        for folder in [\"train\",\"val\",\"test\"]:\n",
    "            if folder == self.split:\n",
    "                folder_path = os.path.join(self.dataset_root, f\"{folder}\")\n",
    "                #print(folder_path)\n",
    "                if os.path.exists(folder_path):\n",
    "                    # get name of files <folder id>_<image>\n",
    "                    sample_ids = sorted([f.split('.')[0] for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "        return sample_ids\n",
    "\n",
    "    #Define here some usefull functions to access the data\n",
    "    def load_image(self, img_path):\n",
    "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.transform(img)\n",
    "\n",
    "    def load_6d_pose(self, sample_id):\n",
    "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
    "        label = int(sample_id.split(\"_\")[0])\n",
    "        objectId = int(sample_id.split(\"_\")[1])\n",
    "        pose_file = os.path.join(self.dataset_root, f\"{label:02d}_gt.yml\")\n",
    "\n",
    "        # Load the ground truth poses from the gt.yml file\n",
    "        with open(pose_file, 'r') as f:\n",
    "            pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # The pose data is a dictionary where each key corresponds to a frame with pose info\n",
    "        # We assume sample_id corresponds to the key in pose_data\n",
    "        if objectId not in pose_data:\n",
    "            raise KeyError(f\"Sample ID {objectId} not found in {label:02d}_gt.yml.\")\n",
    "\n",
    "        for pose in pose_data[objectId]: # There can be more than one pose per sample, but take the one of label=folder_id\n",
    "            # Extract translation and rotation\n",
    "            if (int(pose['obj_id']) == int(label)):\n",
    "                translation = np.array(pose['cam_t_m2c'], dtype=np.float32)  # [3] ---> (x,y,z)\n",
    "                rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
    "                # bbox is top left corner and width and height info, YOLO needs center coordinates and width and height\n",
    "                x_min, y_min, width, height = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
    "                # compute initial center\n",
    "                x_center = x_min + width/2\n",
    "                y_center = y_min + height/2\n",
    "                \n",
    "                # store coordinates of the center and width and height of the bounding box normalized to the\n",
    "                # image width=640 pixels and height=480 pixels\n",
    "                bbox = np.array([x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT], dtype=np.float32)\n",
    "\n",
    "                obj_id = np.array(pose['obj_id'], dtype=np.float32) # [1] ---> label\n",
    "                break\n",
    "\n",
    "        return translation, rotation, bbox, obj_id\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the total number of samples in the selected split.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Load a dataset sample.\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        img_path = os.path.join(self.dataset_root, f\"{self.split}\", f\"{sample}.png\")\n",
    "\n",
    "        img = self.load_image(img_path)\n",
    "        translation, rotation, bbox, obj_id = self.load_6d_pose(sample)\n",
    "\n",
    "        #Dictionary with all the data\n",
    "        return {\n",
    "            \"rgb\": img,\n",
    "            \"translation\": torch.tensor(translation),\n",
    "            \"rotation\": torch.tensor(rotation),\n",
    "            \"bbox\": torch.tensor(bbox),\n",
    "            \"obj_id\": torch.tensor(obj_id),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_pose = \"./YOLO/datasets_cropped\"\n",
    "\n",
    "train_dataset = CustomDatasetPose(dataset_root_pose, split=\"train\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDatasetPose(dataset_root_pose, split=\"validation\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetPose(dataset_root_pose, split=\"test\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa40cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimatorQuat6D(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super(PoseEstimatorQuat6D, self).__init__()\n",
    "\n",
    "        # Load backbone (without last fully connected layer)\n",
    "        backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        in_features = backbone.fc.in_features\n",
    "\n",
    "        # 7 output (4 quaternion, 3 translation)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.fc(x)\n",
    "\n",
    "        quat = output[:, :4]\n",
    "        quat = quat / torch.norm(quat, dim=1, keepdim=True)  # Normalize quaternion\n",
    "\n",
    "        trans = output[:, 4:]\n",
    "        return quat, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, translation, rotation):\n",
    "    quat, trans = outputs\n",
    "    # rotation is list of type quaternion, make it tensor of float32 not float64 (double)\n",
    "    rotation = torch.tensor(np.stack([el.components for el in rotation]).astype(np.float32)).to(device)\n",
    "    loss_quat = F.mse_loss(quat, rotation)\n",
    "    loss_trans = F.mse_loss(trans, translation)\n",
    "\n",
    "    return loss_quat + loss_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac57e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "\n",
    "def train(model, epoch, dataloader, criterion, optimizer=Adam, device=device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, translation, rotation = data[\"rgb\"], data[\"translation\"], data[\"rotation\"]\n",
    "        img, translation, rotation = img.to(device), translation.to(device), quaternion.from_rotation_matrix(rotation)\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, translation, rotation)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 2 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} '\n",
    "                f'({100. * batch_idx / len(dataloader):.0f}%)]\\t Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[0.001]\n",
    "batch_size=[2]\n",
    "num_epoch=10\n",
    "for lr in learning_rate:\n",
    "    for batch in batch_size:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "        model = PoseEstimatorQuat6D().to(device)\n",
    "        print(model)\n",
    "        for epoch in range(1, num_epoch):\n",
    "            train(model, epoch, train_dataloader, criterion, optimizer=Adam(model.parameters(), lr=lr), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f1857",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quaternion\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed985b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPose(pathImage, translation_gt, rotation_gt, translation_pred, rotation_pred):\n",
    "    '''\n",
    "        Input:\n",
    "            path for image (in DenseFusion)\n",
    "            ground truth translation tensor (in millimeters)\n",
    "            ground truth rotation tensor (either matrix or quaternion)\n",
    "            predicted translation tensor (in millimeters)\n",
    "            predicted rotation tensor (either matrix or quaternion)\n",
    "    '''\n",
    "\n",
    "    # read image\n",
    "    image = cv2.imread(pathImage)\n",
    "    transparent_image = image.copy() # copy of the image to work on a transparent image (for the second reference system)\n",
    "\n",
    "    # read translation and rotation\n",
    "    rotat_gt = rotation_gt.numpy() # transform tensor to numpy array\n",
    "    trans_gt = translation_gt.numpy()/1000 # in meters\n",
    "    rotat_pred = rotation_pred.numpy()\n",
    "    trans_pred = translation_pred.numpy()/1000\n",
    "\n",
    "    # read camera intrinsics\n",
    "    label = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    image_id = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
    "    with open(f\"./YOLO/datasets_cropped/{label}_info.yml\", 'r') as f:\n",
    "            camera_info = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    camera_intrinsics = np.array(camera_info[int(image_id)][\"cam_K\"]).reshape(3,3)\n",
    "\n",
    "    # read 3D model\n",
    "    meshModel = trimesh.load(f\"./DenseFusion/Linemod_preprocessed/models/obj_{label}.ply\")\n",
    "    vertices = meshModel.vertices/1000 # it has 3 columns, for X, Y, Z, use unit of measurement of translation\n",
    "    # compute corners\n",
    "    min_corner = vertices.min(axis=0) # find for each column the smallest value\n",
    "    max_corner = vertices.max(axis=0)\n",
    "    \n",
    "    bounding_box_3d = np.array([[min_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], max_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], max_corner[2]],])\n",
    "\n",
    "    # convert quaternion to rotation matrix, if input was quaternion\n",
    "    if rotat_gt.size == 4:\n",
    "        rotat_gt = quaternion.as_rotation_matrix(np.quaternion(*rotat_gt))\n",
    "    else:\n",
    "        rotat_gt = rotat_gt.reshape(3,3)\n",
    "    if rotat_pred.size == 4:\n",
    "        rotat_pred = quaternion.as_rotation_matrix(np.quaternion(*rotat_pred))\n",
    "    else:\n",
    "        rotat_pred = rotat_pred.reshape(3,3)\n",
    "\n",
    "    # build 3D axes according to object coordinate system, same unit of measurement of translation, so in meters\n",
    "    axes_3d = np.array([\n",
    "        [0, 0, 0],      # origin, in the object coordinate system\n",
    "        [0.15, 0, 0],   # how long the arrow should be in the X coordinate\n",
    "        [0, 0.15, 0],   # how long the arrow should be in the Y coordinate\n",
    "        [0, 0, 0.15]    # how long the arrow should be in the Z coordinate\n",
    "    ])\n",
    "\n",
    "    # transform the object coordinate system to the camera coordinate system\n",
    "    # rotat_gt is 3x3, so axes_3d has to be transposed, then add to origin, and coordinates the translation\n",
    "    axes_cam_gt = (rotat_gt @ axes_3d.T).T + trans_gt\n",
    "    # bounding box\n",
    "    bounding_box_3d_cam_gt = (rotat_gt @ bounding_box_3d.T).T + trans_gt\n",
    "    # project 3D axes to 2D\n",
    "    axes_2d_gt = (camera_intrinsics @ axes_cam_gt.T).T # camera_intrinsics is 3x3, while axes_cam_gt 4x3, axes_2d_gt 4x3\n",
    "    axes_2d_gt = axes_2d_gt[:, :2] / axes_2d_gt[:, 2:3] # take first 2 columns and normalize by depth\n",
    "    # bounding box\n",
    "    bounding_box_2d_gt = (camera_intrinsics @ bounding_box_3d_cam_gt.T).T\n",
    "    bounding_box_2d_gt = (bounding_box_2d_gt[:, :2] / bounding_box_2d_gt[:, 2:3]).astype(int)\n",
    "    # get point coordinates\n",
    "    p_gt = [tuple(el) for el in bounding_box_2d_gt]\n",
    "    # define edges using two points, access with index\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    # draw edges\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_gt[el[0]], p_gt[el[1]], (0,0,255), 5)\n",
    "\n",
    "    p0_gt = tuple(axes_2d_gt[0].astype(int)) # take origin coordinates\n",
    "    p1_gt = tuple(axes_2d_gt[1].astype(int))\n",
    "    p2_gt = tuple(axes_2d_gt[2].astype(int))\n",
    "    p3_gt = tuple(axes_2d_gt[3].astype(int))\n",
    "\n",
    "    # color is in BGR format, set tickness=2\n",
    "    cv2.arrowedLine(image, p0_gt, p1_gt, (0, 0, 255), 2) # X is red\n",
    "    cv2.arrowedLine(image, p0_gt, p2_gt, (0, 255, 0), 2) # Y is green\n",
    "    cv2.arrowedLine(image, p0_gt, p3_gt, (255, 0, 0), 2) # Z is blue\n",
    "\n",
    "    # for predicted\n",
    "    axes_cam_pred = (rotat_pred @ axes_3d.T).T + trans_pred\n",
    "    bounding_box_3d_cam_pred = (rotat_pred @ bounding_box_3d.T).T + trans_pred\n",
    "\n",
    "    axes_2d_pred = (camera_intrinsics @ axes_cam_pred.T).T\n",
    "    axes_2d_pred = axes_2d_pred[:, :2] / axes_2d_pred[:, 2:3]\n",
    "    bounding_box_2d_pred = (camera_intrinsics @ bounding_box_3d_cam_pred.T).T\n",
    "    bounding_box_2d_pred = (bounding_box_2d_pred[:, :2] / bounding_box_2d_pred[:, 2:3]).astype(int)\n",
    "\n",
    "    p_pred = [tuple(el) for el in bounding_box_2d_pred]\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_pred[el[0]], p_pred[el[1]], (255,0,0), 5)\n",
    "\n",
    "    p0_pred = tuple(axes_2d_pred[0].astype(int))\n",
    "    p1_pred = tuple(axes_2d_pred[1].astype(int))\n",
    "    p2_pred = tuple(axes_2d_pred[2].astype(int))\n",
    "    p3_pred = tuple(axes_2d_pred[3].astype(int))\n",
    "\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p1_pred, (0, 0, 255), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p2_pred, (0, 255, 0), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p3_pred, (255, 0, 0), 2)\n",
    "\n",
    "    # show image after merging the two images\n",
    "    overlapImage = cv2.addWeighted(transparent_image, 0.5, image, 1, 0)\n",
    "    plt.imshow(cv2.cvtColor(overlapImage, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Object Pose Estimation (prediction is transparent)\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
