{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde563b",
   "metadata": {},
   "source": [
    "Set some variables to conditionally run some codes. First download the project and change directory to ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNT_DRIVE = False\n",
    "COMET_ML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17727590",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/6DPose_Estimation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdb879",
   "metadata": {},
   "source": [
    "Install all dependencies of PyTorch dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%env TORCH=$torch.__version__\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7f2c7",
   "metadata": {},
   "source": [
    "Install all packages, you may need to restart the runtime before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "print(\"Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "import shutil\n",
    "import ultralytics\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.patches as patches\n",
    "import wandb\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import v2\n",
    "import trimesh\n",
    "\n",
    "# install PyTorch Geometric after installation and restart\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import knn_interpolate, MessagePassing\n",
    "from torch_geometric.nn.pool import fps, radius\n",
    "\n",
    "# import comet-ml\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import watch\n",
    "\n",
    "from utils.data_exploration import load_image\n",
    "from utils.installation_checker import check_torch_geometric\n",
    "\n",
    "from data.CustomDatasetPose import IMG_WIDTH, IMG_HEIGHT\n",
    "\n",
    "# check if everything works\n",
    "check_torch_geometric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b752f3",
   "metadata": {},
   "source": [
    "Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device_setter import set_device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806248",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (which includes a portion of the LimeMOD dataset)\n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    !cp /content/drive/MyDrive/6DPose_Estimation /content/ # move to content for faster access to files\n",
    "    %cd /content/6DPose_Estimation/datasets/linemod/DenseFusion\n",
    "\n",
    "!unzip Linemod_preprocessed.zip\n",
    "!rm Linemod_preprocessed.zip\n",
    "%cd ../../../ # change directory to 6D_pose_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fb52",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773553",
   "metadata": {},
   "source": [
    "## Modify Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7f9e4",
   "metadata": {},
   "source": [
    "Copy ground truth files to ```Linemod_preprocessed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import get_class_names\n",
    "from utils.preprocessing import copy_gt_file, change_02gt, quaternion_gt\n",
    "\n",
    "folder_names = get_class_names()\n",
    "copy_gt_file(folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add824a",
   "metadata": {},
   "source": [
    "Change ```02_gt.yml``` to take only one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_02gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed/02_gt.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e793c4",
   "metadata": {},
   "source": [
    "Add quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c090358",
   "metadata": {},
   "outputs": [],
   "source": [
    "quaternion_gt(\"./datasets/linemod/DenseFusion/Linemod_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3ec7",
   "metadata": {},
   "source": [
    "Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121399",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(label=1, object=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d93753",
   "metadata": {},
   "source": [
    "Check if camera intrinsics is same for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/data\"\n",
    "\n",
    "from utils.data_exploration import check_cam_K_equal\n",
    "\n",
    "cam_K = check_cam_K_equal(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ce9fc",
   "metadata": {},
   "source": [
    "## Define CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.CustomDatasetPose import CustomDatasetPose\n",
    "\n",
    "dataset_root = \"./datasets/linemod/DenseFusion/Linemod_preprocessed/\"\n",
    "\n",
    "train_dataset = CustomDatasetPose(dataset_root, split=\"train\", device=device, cam_K = cam_K)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDatasetPose(dataset_root, split=\"validation\", device=device, cam_K = cam_K)\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetPose(dataset_root, split=\"test\", device=device, cam_K = cam_K)\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276b3ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869154a1",
   "metadata": {},
   "source": [
    "Structure the data for YOLO such that\n",
    "```\n",
    "datasets/\n",
    "├── data.yaml\n",
    "│\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │\n",
    "│   └── labels/\n",
    "│  \n",
    "├── val/\n",
    "│\n",
    "└── test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into training, validation and testing set\n",
    "train_samples = train_dataset.get_samples_id()\n",
    "validation_samples = val_dataset.get_samples_id()\n",
    "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253d1e",
   "metadata": {},
   "source": [
    "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
    "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
    "\n",
    "The working directory is in the ```6DPose_Estimation```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76389b",
   "metadata": {},
   "source": [
    "Create YOLO yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec539f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import create_YOLO_yaml, create_dataset_YOLO\n",
    "\n",
    "number_classes, class_names = create_YOLO_yaml(path, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba3d0",
   "metadata": {},
   "source": [
    "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to have easily access to the index\n",
    "index_dict = dict()\n",
    "for index, el in enumerate(class_names):\n",
    "    index_dict[int(el)] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c7b5",
   "metadata": {},
   "source": [
    "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects, but just consider the object of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_df = create_dataset_YOLO(number_classes, train_samples, validation_samples, test_samples, index_dict, path, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b20d",
   "metadata": {},
   "source": [
    "Visualize dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_dataset_distribution\n",
    "\n",
    "load_dataset_distribution(counter_df, index_dict, number_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b007448",
   "metadata": {},
   "source": [
    "Visualize depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_image\n",
    "\n",
    "folder = \"02\"\n",
    "object_name = \"0101\"\n",
    "img = load_depth_image(f\"{path}/datasets/linemod/DenseFusion/Linemod_preprocessed/data/{folder}/depth/{object_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8a540",
   "metadata": {},
   "source": [
    "Plot the patch of first object of the image, it reads from the ground truth file containing also multiple objects in one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import load_depth_patch\n",
    "\n",
    "load_depth_patch(path, folder, object_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab338446",
   "metadata": {},
   "source": [
    "Get data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"Training loader: {len(train_loader)}\")\n",
    "print(f\"Validation loader: {len(val_loader)}\")\n",
    "print(f\"Test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b86ad",
   "metadata": {},
   "source": [
    "Plot one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef53038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import plot_batch_data\n",
    "\n",
    "plot_batch_data(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80313348",
   "metadata": {},
   "source": [
    "## Training Object Detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_YOLO import train_YOLO\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "IMG_SIZE = 640\n",
    "\n",
    "train_YOLO(path, epochs, batch_size, device, IMG_SIZE) # train model and save it to checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d7a82",
   "metadata": {},
   "source": [
    "Validate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18df963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_YOLO import evaluate_YOLO\n",
    "\n",
    "evaluate_YOLO(path, epochs, batch_size, IMG_SIZE, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425354d6",
   "metadata": {},
   "source": [
    "## Pose Estimator Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f1857",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quaternion\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed985b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPose(pathImage, translation_gt, rotation_gt, translation_pred, rotation_pred):\n",
    "    '''\n",
    "        Input:\n",
    "            path for image (in DenseFusion)\n",
    "            ground truth translation tensor (in millimeters)\n",
    "            ground truth rotation tensor (either matrix or quaternion)\n",
    "            predicted translation tensor (in millimeters)\n",
    "            predicted rotation tensor (either matrix or quaternion)\n",
    "    '''\n",
    "\n",
    "    # read image\n",
    "    image = cv2.imread(pathImage)\n",
    "    transparent_image = image.copy() # copy of the image to work on a transparent image (for the second reference system)\n",
    "\n",
    "    # read translation and rotation\n",
    "    rotat_gt = rotation_gt.numpy() # transform tensor to numpy array\n",
    "    trans_gt = translation_gt.numpy()/1000 # in meters\n",
    "    rotat_pred = rotation_pred.numpy()\n",
    "    trans_pred = translation_pred.numpy()/1000\n",
    "\n",
    "    # read camera intrinsics\n",
    "    label = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    image_id = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
    "    with open(f\"./YOLO/datasets_cropped/{label}_info.yml\", 'r') as f:\n",
    "            camera_info = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    camera_intrinsics = np.array(camera_info[int(image_id)][\"cam_K\"]).reshape(3,3)\n",
    "\n",
    "    # read 3D model\n",
    "    meshModel = trimesh.load(f\"./DenseFusion/Linemod_preprocessed/models/obj_{label}.ply\")\n",
    "    vertices = meshModel.vertices/1000 # it has 3 columns, for X, Y, Z, use unit of measurement of translation\n",
    "    # compute corners\n",
    "    min_corner = vertices.min(axis=0) # find for each column the smallest value\n",
    "    max_corner = vertices.max(axis=0)\n",
    "    \n",
    "    bounding_box_3d = np.array([[min_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], max_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], max_corner[2]],])\n",
    "\n",
    "    # convert quaternion to rotation matrix, if input was quaternion\n",
    "    if rotat_gt.size == 4:\n",
    "        rotat_gt = quaternion.as_rotation_matrix(np.quaternion(*rotat_gt))\n",
    "    else:\n",
    "        rotat_gt = rotat_gt.reshape(3,3)\n",
    "    if rotat_pred.size == 4:\n",
    "        rotat_pred = quaternion.as_rotation_matrix(np.quaternion(*rotat_pred))\n",
    "    else:\n",
    "        rotat_pred = rotat_pred.reshape(3,3)\n",
    "\n",
    "    # build 3D axes according to object coordinate system, same unit of measurement of translation, so in meters\n",
    "    axes_3d = np.array([\n",
    "        [0, 0, 0],      # origin, in the object coordinate system\n",
    "        [0.15, 0, 0],   # how long the arrow should be in the X coordinate\n",
    "        [0, 0.15, 0],   # how long the arrow should be in the Y coordinate\n",
    "        [0, 0, 0.15]    # how long the arrow should be in the Z coordinate\n",
    "    ])\n",
    "\n",
    "    # transform the object coordinate system to the camera coordinate system\n",
    "    # rotat_gt is 3x3, so axes_3d has to be transposed, then add to origin, and coordinates the translation\n",
    "    axes_cam_gt = (rotat_gt @ axes_3d.T).T + trans_gt\n",
    "    # bounding box\n",
    "    bounding_box_3d_cam_gt = (rotat_gt @ bounding_box_3d.T).T + trans_gt\n",
    "    # project 3D axes to 2D\n",
    "    axes_2d_gt = (camera_intrinsics @ axes_cam_gt.T).T # camera_intrinsics is 3x3, while axes_cam_gt 4x3, axes_2d_gt 4x3\n",
    "    axes_2d_gt = axes_2d_gt[:, :2] / axes_2d_gt[:, 2:3] # take first 2 columns and normalize by depth\n",
    "    # bounding box\n",
    "    bounding_box_2d_gt = (camera_intrinsics @ bounding_box_3d_cam_gt.T).T\n",
    "    bounding_box_2d_gt = (bounding_box_2d_gt[:, :2] / bounding_box_2d_gt[:, 2:3]).astype(int)\n",
    "    # get point coordinates\n",
    "    p_gt = [tuple(el) for el in bounding_box_2d_gt]\n",
    "    # define edges using two points, access with index\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    # draw edges\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_gt[el[0]], p_gt[el[1]], (0,0,255), 5)\n",
    "\n",
    "    p0_gt = tuple(axes_2d_gt[0].astype(int)) # take origin coordinates\n",
    "    p1_gt = tuple(axes_2d_gt[1].astype(int))\n",
    "    p2_gt = tuple(axes_2d_gt[2].astype(int))\n",
    "    p3_gt = tuple(axes_2d_gt[3].astype(int))\n",
    "\n",
    "    # color is in BGR format, set tickness=2\n",
    "    cv2.arrowedLine(image, p0_gt, p1_gt, (0, 0, 255), 2) # X is red\n",
    "    cv2.arrowedLine(image, p0_gt, p2_gt, (0, 255, 0), 2) # Y is green\n",
    "    cv2.arrowedLine(image, p0_gt, p3_gt, (255, 0, 0), 2) # Z is blue\n",
    "\n",
    "    # for predicted\n",
    "    axes_cam_pred = (rotat_pred @ axes_3d.T).T + trans_pred\n",
    "    bounding_box_3d_cam_pred = (rotat_pred @ bounding_box_3d.T).T + trans_pred\n",
    "\n",
    "    axes_2d_pred = (camera_intrinsics @ axes_cam_pred.T).T\n",
    "    axes_2d_pred = axes_2d_pred[:, :2] / axes_2d_pred[:, 2:3]\n",
    "    bounding_box_2d_pred = (camera_intrinsics @ bounding_box_3d_cam_pred.T).T\n",
    "    bounding_box_2d_pred = (bounding_box_2d_pred[:, :2] / bounding_box_2d_pred[:, 2:3]).astype(int)\n",
    "\n",
    "    p_pred = [tuple(el) for el in bounding_box_2d_pred]\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_pred[el[0]], p_pred[el[1]], (255,0,0), 5)\n",
    "\n",
    "    p0_pred = tuple(axes_2d_pred[0].astype(int))\n",
    "    p1_pred = tuple(axes_2d_pred[1].astype(int))\n",
    "    p2_pred = tuple(axes_2d_pred[2].astype(int))\n",
    "    p3_pred = tuple(axes_2d_pred[3].astype(int))\n",
    "\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p1_pred, (0, 0, 255), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p2_pred, (0, 255, 0), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p3_pred, (255, 0, 0), 2)\n",
    "\n",
    "    # show image after merging the two images\n",
    "    overlapImage = cv2.addWeighted(transparent_image, 0.5, image, 1, 0)\n",
    "    plt.imshow(cv2.cvtColor(overlapImage, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Object Pose Estimation (prediction is transparent)\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
