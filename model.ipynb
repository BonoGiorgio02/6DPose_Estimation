{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c391c5e",
   "metadata": {},
   "source": [
    "Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DenseFusion Folder (Which includes a portion of the LimeMOD dataset) \n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ../Linemod_preprocessed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6715c",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d56b",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fe60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "import shutil\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "IMG_WIDTH = 640\n",
    "IMG_HEIGHT = 480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3ec7",
   "metadata": {},
   "source": [
    "Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d913bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"{path}/Linemod_preprocessed/data/01/rgb/0000.png\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # used to load and preprocess data\n",
    "    def __init__(self, dataset_root, split='train', train_ratio=0.7, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_root (str): Path to the dataset directory.\n",
    "            split (str): 'train', 'validation' or 'test'.\n",
    "            train_ratio (float): Percentage of data used for training (default 70%).\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.dataset_root = dataset_root\n",
    "        self.split = split\n",
    "        self.train_ratio = train_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Get list of all samples (folder_id, sample_id)\n",
    "        self.samples = self.get_all_samples()\n",
    "\n",
    "        # Check if samples were found\n",
    "        if not self.samples:\n",
    "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
    "\n",
    "        # Split into training and validation+test sets\n",
    "        labels = [el[0] for el in self.samples]\n",
    "        self.train_samples, self.val_test_samples = train_test_split(\n",
    "            self.samples, train_size=self.train_ratio, random_state=self.seed, stratify=labels\n",
    "        )\n",
    "\n",
    "        # split validation+test set (by default 30% of the original dataset) into validation and test sets\n",
    "        labels = [el[0] for el in self.val_test_samples]\n",
    "        self.val_samples, self.test_samples = train_test_split(self.val_test_samples, train_size=0.5, random_state=self.seed, stratify=labels)\n",
    "\n",
    "        # Select the appropriate split\n",
    "        if split == \"train\":\n",
    "            self.samples = self.train_samples\n",
    "        elif split == \"validation\":\n",
    "            self.samples = self.val_samples\n",
    "        else:\n",
    "            self.samples = self.test_samples\n",
    "\n",
    "        # Define image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def get_samples_id(self):\n",
    "        return self.samples\n",
    "\n",
    "    def get_all_samples(self):\n",
    "        \"\"\"Retrieve the list of all available sample indices from all folders.\"\"\"\n",
    "        samples = []\n",
    "        for folder_id in range(1, 16):  # Assuming folders are named 01 to 15\n",
    "            folder_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"rgb\")\n",
    "            #print(folder_path)\n",
    "            if os.path.exists(folder_path):\n",
    "                # get id of the images\n",
    "                sample_ids = sorted([int(f.split('.')[0]) for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "                samples.extend([(folder_id, sid) for sid in sample_ids])  # Store (folder_id, sample_id)\n",
    "        return samples\n",
    "    \n",
    "    def load_config(self, folder_id):\n",
    "        \"\"\"Load YAML configuration files for camera intrinsics and object info for a specific folder.\"\"\"\n",
    "        camera_intrinsics_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", 'info.yml')\n",
    "        objects_info_path = os.path.join(self.dataset_root, 'models', f\"models_info.yml\")\n",
    "\n",
    "        with open(camera_intrinsics_path, 'r') as f:\n",
    "            camera_intrinsics = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        with open(objects_info_path, 'r') as f:\n",
    "            objects_info = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return camera_intrinsics, objects_info\n",
    "\n",
    "    #Define here some usefull functions to access the data\n",
    "    def load_image(self, img_path):\n",
    "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.transform(img)\n",
    "    \n",
    "    def load_depth(self, depth_path):\n",
    "        \"\"\"Load a depth image and convert to tensor.\"\"\"\n",
    "        depth = np.array(Image.open(depth_path))\n",
    "        return torch.tensor(depth, dtype=torch.float32)\n",
    "    \n",
    "    def load_point_cloud(self, depth, intrinsics):\n",
    "        \"\"\"Convert depth image to point cloud using Open3D.\"\"\"\n",
    "        intrinsics = intrinsics[0]['cam_K'] # take intrinsincs of the first image\n",
    "        h, w = depth.shape\n",
    "        # focal lengths and principal centers\n",
    "        fx, fy, cx, cy = intrinsics[0], intrinsics[4], intrinsics[2], intrinsics[5]\n",
    "\n",
    "        # Generate 3D points\n",
    "        xmap, ymap = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        z = depth / 1000.0  # Convert to meters\n",
    "        x = (xmap - cx) * z / fx\n",
    "        y = (ymap - cy) * z / fy\n",
    "\n",
    "        points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "        return point_cloud\n",
    "\n",
    "    def load_6d_pose(self, folder_id, sample_id):\n",
    "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
    "        pose_file = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", \"gt.yml\")\n",
    "\n",
    "        # Load the ground truth poses from the gt.yml file\n",
    "        with open(pose_file, 'r') as f:\n",
    "            pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # The pose data is a dictionary where each key corresponds to a frame with pose info\n",
    "        # We assume sample_id corresponds to the key in pose_data\n",
    "        if sample_id not in pose_data:\n",
    "            raise KeyError(f\"Sample ID {sample_id} not found in gt.yml for folder {folder_id}.\")\n",
    "\n",
    "        for pose in pose_data[sample_id]: # There can be more than one pose per sample, but take the one of label=folder_id\n",
    "            # Extract translation and rotation\n",
    "            if (int(pose['obj_id']) == int(folder_id)):\n",
    "                translation = np.array(pose['cam_t_m2c'], dtype=np.float32)  # [3] ---> (x,y,z)\n",
    "                rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
    "                # bbox is top left corner and width and height info, YOLO needs center coordinates and width and height\n",
    "                x_min, y_min, width, height = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
    "                # compute initial center\n",
    "                x_center = x_min + width/2\n",
    "                y_center = y_min + height/2\n",
    "\n",
    "                # move the center when outside image and adjust width and height accordingly\n",
    "                if x_center < 0:\n",
    "                    width += 2 * x_center # x_center negative, subtract its absolute value * 2 from width\n",
    "                    x_center = 0\n",
    "                elif x_center > IMG_WIDTH:\n",
    "                    width -= 2 * (x_center - IMG_WIDTH)\n",
    "                    x_center = IMG_WIDTH\n",
    "\n",
    "                if y_center < 0:\n",
    "                    height += 2 * y_center # y_center negative, subtract its absolute value * 2 from height\n",
    "                    y_center = 0\n",
    "                elif y_center > IMG_HEIGHT:\n",
    "                    height -= 2 * (y_center - IMG_HEIGHT)\n",
    "                    y_center = IMG_HEIGHT\n",
    "                \n",
    "                # ensure width and height are not negative, this happens when bounding box is completely outside image (it should never happen)\n",
    "                width = max(0, width)\n",
    "                height = max(0, height)\n",
    "                \n",
    "                # store coordinates of the center and width and height of the bounding box normalized to the\n",
    "                # image width=640 pixels and height=480 pixels\n",
    "                bbox = np.array([x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT], dtype=np.float32)\n",
    "\n",
    "                obj_id = np.array(pose['obj_id'], dtype=np.float32) # [1] ---> label\n",
    "                break\n",
    "\n",
    "        return translation, rotation, bbox, obj_id\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the total number of samples in the selected split.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Load a dataset sample.\n",
    "        folder_id, sample_id = self.samples[idx]\n",
    "\n",
    "        # Load the correct camera intrinsics and object info for this folder\n",
    "        camera_intrinsics, objects_info = self.load_config(folder_id)\n",
    "\n",
    "        img_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"rgb/{sample_id:04d}.png\")\n",
    "        depth_path = os.path.join(self.dataset_root, 'data', f\"{folder_id:02d}\", f\"depth/{sample_id:04d}.png\")\n",
    "\n",
    "        img = self.load_image(img_path)\n",
    "        depth = self.load_depth(depth_path)\n",
    "        point_cloud = self.load_point_cloud(depth.numpy(), camera_intrinsics)\n",
    "        point_cloud = torch.tensor(np.asarray(point_cloud.points), dtype=torch.float32)\n",
    "        translation, rotation, bbox, obj_id = self.load_6d_pose(folder_id, sample_id)\n",
    "\n",
    "        #Dictionary with all the data\n",
    "        return {\n",
    "            \"rgb\": img,\n",
    "            \"depth\": torch.tensor(depth, dtype=torch.float32),\n",
    "            \"point_cloud\": point_cloud,\n",
    "            \"camera_intrinsics\": camera_intrinsics[0]['cam_K'],\n",
    "            \"objects_info\": objects_info,\n",
    "            \"translation\": torch.tensor(translation),\n",
    "            \"rotation\": torch.tensor(rotation),\n",
    "            \"bbox\": torch.tensor(bbox),\n",
    "            \"obj_id\": torch.tensor(obj_id)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"./Linemod_preprocessed\"\n",
    "\n",
    "train_dataset = CustomDataset(dataset_root, split=\"train\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDataset(dataset_root, split=\"validation\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDataset(dataset_root, split=\"test\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276b3ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869154a1",
   "metadata": {},
   "source": [
    "Structure the data such that\n",
    "```\n",
    "datasets/\n",
    "├── data.yaml\n",
    "│\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │\n",
    "│   └── labels/\n",
    "│  \n",
    "├── val/\n",
    "│\n",
    "└── test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into training, validation and testing set\n",
    "train_samples = train_dataset.get_samples_id()\n",
    "validation_samples = val_dataset.get_samples_id()\n",
    "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253d1e",
   "metadata": {},
   "source": [
    "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
    "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
    "\n",
    "The working directory is in the ```DenseFusion```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder to contain the dataset for YOLO model\n",
    "os.makedirs(\"../YOLO/datasets\", exist_ok=True)\n",
    "\n",
    "# count number of distinct classes\n",
    "number_classes = 0\n",
    "class_names = []\n",
    "for el in os.scandir(\"./Linemod_preprocessed/data\"):\n",
    "    # if entry is a directory and its name is an integer value (this is just to avoid counting non directories or other directories)\n",
    "    if (el.is_dir() and el.name.isdigit()):\n",
    "        class_names.append(el.name)\n",
    "        number_classes += 1\n",
    "\n",
    "# get string of all class names\n",
    "class_names.sort() # sort the names\n",
    "names = \"[\"\n",
    "for index, el in enumerate(class_names):\n",
    "    # if last element don't add comma\n",
    "    if index == number_classes-1:\n",
    "        names += f\"'{str(el)}'\"\n",
    "    else:\n",
    "        names += f\"'{str(el)}',\"\n",
    "names += \"]\"\n",
    "\n",
    "# create data.yaml (as class names use ids of the folder)\n",
    "content = f\"\"\"train: ./train/images\n",
    "val: ./val/images\n",
    "test: ./test/images\n",
    "\n",
    "nc: {number_classes}\n",
    "names: {names}\"\"\"\n",
    "# write to file\n",
    "with open(\"../YOLO/datasets/data.yaml\", \"w\") as fout:\n",
    "    fout.write(content)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba3d0",
   "metadata": {},
   "source": [
    "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to have easily access to the index\n",
    "index_dict = dict()\n",
    "for index, el in enumerate(class_names):\n",
    "    index_dict[int(el)] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c7b5",
   "metadata": {},
   "source": [
    "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects, but just consider the object of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# create images and labels\n",
    "# dataset = [train_samples, validation_samples, test_samples]\n",
    "folder_names = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# count also the number of instances of each class\n",
    "classes = range(0, number_classes)\n",
    "counter_df = pd.DataFrame()\n",
    "for idx in range(3):\n",
    "    if idx == 0:\n",
    "        dataset = train_samples\n",
    "    elif idx == 1:\n",
    "        dataset = validation_samples\n",
    "    else:\n",
    "        dataset = test_samples\n",
    "    print(f\"------------------------------{folder_names[idx].upper()}------------------------------\")\n",
    "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/labels\", exist_ok=True)\n",
    "    classCount = {label_object: 0 for label_object in index_dict.keys()} # initialize dictionary for counting\n",
    "    total = 0 # used to normalize count\n",
    "    for el in tqdm(dataset, desc=\"Moving...\"):\n",
    "        # el is (folderId, sampleId)\n",
    "        _, _, bbox, obj_id = train_dataset.load_6d_pose(el[0], el[1])\n",
    "        # copy image into the new folder\n",
    "        # avoid overwriting the files, so concat also the name of the folderId to the destination file\n",
    "        shutil.copy(f\"./Linemod_preprocessed/data/{el[0]:02d}/rgb/{el[1]:04d}.png\", f\"../YOLO/datasets/{folder_names[idx]}/images/{el[0]:02d}_{el[1]:04d}.png\")\n",
    "        # create label file with the same name as the image\n",
    "        with open(f\"../YOLO/datasets/{folder_names[idx]}/labels/{el[0]:02d}_{el[1]:04d}.txt\", \"w\") as fout:\n",
    "            # bbox is a list of values in the form of [x_center, y_center, width, height] and obj_id a list of class labels\n",
    "            # where each label is in the format 01-15\n",
    "            classCount[int(obj_id)] += 1\n",
    "            total += 1\n",
    "            content = f\"{index_dict[int(obj_id)]} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\"\n",
    "            fout.write(content)\n",
    "        fout.close()\n",
    "    \n",
    "    # store in the dataframe\n",
    "    values = pd.array(list(classCount.values()))/total\n",
    "    counter_df[folder_names[idx]] = values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3273181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of labels in training, validation and test set\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,6),sharey=True)\n",
    "for index, column in enumerate(counter_df.columns):\n",
    "    axes[index].barh([str(el) for el in index_dict.keys()], counter_df[column],color=\"orange\", edgecolor='gray')\n",
    "    axes[index].set_title(column.capitalize())\n",
    "    # add line that represents the uniform distribution of the labels\n",
    "    axes[index].axvline(x=1/number_classes, color=\"blue\")\n",
    "    axes[index].text(x=1/number_classes,y=-0.5,s=f\"{1/number_classes: .5f}\", color=\"blue\")\n",
    "\n",
    "fig.supxlabel(\"Frequency\")\n",
    "fig.supylabel(\"Labels\")\n",
    "plt.subplots_adjust(left=0.07, wspace=0.1)\n",
    "plt.suptitle(\"Labels Distribution over the Training, Validation and Test sets\")\n",
    "plt.savefig(\"../../../images/YOLO_dataset_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b007448",
   "metadata": {},
   "source": [
    "Visualize depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./Linemod_preprocessed/data/02/depth/0000.png\"\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Plot image with bounding box\n",
    "\n",
    "# Load the ground truth poses from the gt.yml file\n",
    "with open(\"./Linemod_preprocessed/data/02/gt.yml\", 'r') as f:\n",
    "  pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "pose = pose_data[0][1] # access image 0 (start counting from 0) and get second object in that image (in case of multiple objects)\n",
    "\n",
    "bbox = np.array(pose['obj_bb'], dtype=np.float32) #[4]\n",
    "obj_id = np.array(pose['obj_id'], dtype=np.float32) #[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a rectangle patch\n",
    "rect = patches.Rectangle(\n",
    "    (bbox[0], bbox[1]),  # (x, y)\n",
    "    bbox[2],             # width\n",
    "    bbox[3],             # height\n",
    "    linewidth=2,\n",
    "    edgecolor='red',\n",
    "    facecolor='none'\n",
    ")\n",
    "\n",
    "# Add the rectangle to the plot\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Optionally add object ID label (write a bit above the top left corner)\n",
    "ax.text(bbox[0], bbox[1] - 10, f'ID: {int(obj_id)}', color='yellow', fontsize=12, backgroundcolor='black')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"Training loader: {len(train_loader)}\")\n",
    "print(f\"Validation loader: {len(val_loader)}\")\n",
    "print(f\"Test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Get only the first 1 batch\n",
    "train_subset_num_batches = 1\n",
    "val_subset_num_batches = 1\n",
    "test_subset_num_batches = 1\n",
    "train_subset = list(itertools.islice(train_loader, train_subset_num_batches))\n",
    "val_subset = list(itertools.islice(val_loader, val_subset_num_batches))\n",
    "test_subset = list(itertools.islice(test_loader, test_subset_num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Get one batch from the train loader (4 images)\n",
    "batch = next(iter(train_loader)) # it uses load_6d_pose, so one pose per object\n",
    "\n",
    "# Extract relevant data\n",
    "rgb_images = batch[\"rgb\"]         # (B, 3, H, W)\n",
    "bboxes = batch[\"bbox\"]            # (B, 4) in pixel coords: x_min, y_min, x_max, y_max\n",
    "obj_ids = batch[\"obj_id\"]         # (B,)\n",
    "\n",
    "# Convert to numpy and rearrange channels\n",
    "rgb_images = rgb_images.permute(0, 2, 3, 1).numpy()  # (B, H, W, 3)\n",
    "bboxes = bboxes.numpy()\n",
    "obj_ids = obj_ids.numpy()\n",
    "\n",
    "# Plot settings\n",
    "batch_size = rgb_images.shape[0]\n",
    "cols = min(4, batch_size)\n",
    "rows = (batch_size + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    ax = axes[i]\n",
    "    img = rgb_images[i]\n",
    "    # each element is [x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT]\n",
    "    x_center, y_center, width, height = bboxes[i]\n",
    "    # remove normalization\n",
    "    x_center = x_center*IMG_WIDTH\n",
    "    y_center = y_center*IMG_HEIGHT\n",
    "    width = width*IMG_WIDTH\n",
    "    height = height*IMG_HEIGHT\n",
    "    x_min = x_center-(width/2)\n",
    "    y_min = y_center-(height/2)\n",
    "    obj_id = obj_ids[i]\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min),   # (x_min, y_min)\n",
    "        width,              # width\n",
    "        height,              # height\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add object ID as label\n",
    "    ax.text(\n",
    "        x_min,\n",
    "        y_min - 10,\n",
    "        f'ID: {int(obj_id)}',\n",
    "        color='yellow',\n",
    "        fontsize=10,\n",
    "        backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "# Hide unused axes if batch_size < cols * rows\n",
    "for j in range(batch_size, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80313348",
   "metadata": {},
   "source": [
    "## Training Object Detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf706c86",
   "metadata": {},
   "source": [
    "Check if CUDA available, otherwise try with MPS and then CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e775da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Cuda not available, use mps\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d00f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../YOLO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03517373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"../../../checkpoints/yolo11n.pt\"\n",
    "model = YOLO(model_path)\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "IMG_SIZE = 640\n",
    "\n",
    "# model will automatically scale the image and related bounding box according to imgsz\n",
    "results = model.train(data=f\"{path}/datasets/data.yaml\", epochs=epochs, batch=batch_size, device=device,\n",
    "        imgsz=IMG_SIZE,\n",
    "        augment=True,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        hsv_h=0.4,\n",
    "        hsv_s=0.4,\n",
    "        hsv_v=0.4,\n",
    "        degrees=120,\n",
    "        translate=0.1,\n",
    "        scale=0.5,\n",
    "        shear=20,\n",
    "        perspective=0.0001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c08d2b",
   "metadata": {},
   "source": [
    "Copy model file to ```checkpoints```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(f\"./runs/detect/train/weights/best.pt\", f\"../../../checkpoints/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d7a82",
   "metadata": {},
   "source": [
    "Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../checkpoints/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "results = model.val(\n",
    "        data=f\"{path}/datasets/data.yaml\",\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6419a45",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pathTest = f\"{path}/datasets/test/images\"\n",
    "filenames = os.listdir(pathTest)\n",
    "random.shuffle(filenames)\n",
    "\n",
    "for filename in filenames:\n",
    "    path = os.path.join(pathTest, filename)\n",
    "    results = model.predict(path)\n",
    "    for result in results:\n",
    "        result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
