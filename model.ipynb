{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266fc8f6",
   "metadata": {},
   "source": [
    "# 6D Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fabe",
   "metadata": {},
   "source": [
    "## Set up the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88aaa0",
   "metadata": {},
   "source": [
    "We will work with a portion of this dataset, which you can find here: https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde563b",
   "metadata": {},
   "source": [
    "Set some variables to conditionally run some codes. First download the project and change directory to ```6D_pose_estimation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNT_DRIVE = False\n",
    "WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17727590",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/6D_pose_estimation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdb879",
   "metadata": {},
   "source": [
    "Install all dependencies of PyTorch dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%env TORCH=$torch.__version__\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7f2c7",
   "metadata": {},
   "source": [
    "Install all packages, you may need to restart the runtime before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "print(\"Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "import itertools\n",
    "import shutil\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import wandb\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import v2\n",
    "import trimesh\n",
    "\n",
    "# install PyTorch Geometric after installation and restart\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import knn_interpolate, MessagePassing\n",
    "from torch_geometric.nn.pool import fps, radius\n",
    "\n",
    "from utils.data_exploration import load_image\n",
    "\n",
    "IMG_WIDTH = 640\n",
    "IMG_HEIGHT = 480\n",
    "\n",
    "# check if everything works\n",
    "try:\n",
    "    from torch_geometric.nn.pool import fps\n",
    "    print(\"PyTorch Geoemtric correctly installed\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Check if you have restarted runtime after installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b752f3",
   "metadata": {},
   "source": [
    "Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Cuda not available, use mps\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42fbe8",
   "metadata": {},
   "source": [
    "Connect to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    os.makedirs(\"./wandb\", exist_ok=True)\n",
    "    %env WANDB_DIR=\"./wandb\"\n",
    "    wandb.login(key=\"<YOUR_KEY>\")\n",
    "    wandb.init(project=\"6D_pose_estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806248",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset (LineMOD)\n",
    "# Download LineMOD dataset\n",
    "# create directory structure without errors\n",
    "!mkdir -p datasets/linemod/\n",
    "%cd datasets/linemod/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DenseFusion/\n",
    "%cd DenseFusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (which includes a portion of the LimeMOD dataset)\n",
    "!gdown --folder \"https://drive.google.com/drive/folders/19ivHpaKm9dOrr12fzC8IDFczWRPFxho7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip Linemod_preprocessed.zip\n",
    "!rm Linemod_preprocessed.zip\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fb52",
   "metadata": {},
   "source": [
    "Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7108",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3ec7",
   "metadata": {},
   "source": [
    "Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121399",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image(label=1, object=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.get_samples import get_samples\n",
    "\n",
    "# divide dataset into training, validation and testing set for training YOLO\n",
    "train_samples = get_samples(split=\"train\")\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "\n",
    "validation_samples = get_samples(split=\"validation\")\n",
    "print(f\"Validation samples: {len(validation_samples)}\")\n",
    "\n",
    "test_samples = get_samples(split=\"test\") # test folder is optional for training YOLO\n",
    "print(f\"Testing samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e75ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3276b3ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869154a1",
   "metadata": {},
   "source": [
    "Structure the data such that\n",
    "```\n",
    "datasets/\n",
    "├── data.yaml\n",
    "│\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │\n",
    "│   └── labels/\n",
    "│  \n",
    "├── val/\n",
    "│\n",
    "└── test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into training, validation and testing set\n",
    "train_samples = train_dataset.get_samples_id()\n",
    "validation_samples = val_dataset.get_samples_id()\n",
    "test_samples = test_dataset.get_samples_id() # test folder is optional for training YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253d1e",
   "metadata": {},
   "source": [
    "Create a new folder containing all the info, we just need the rgb image and a text file with the label and bounding box.\n",
    "The ```Linemod_preprocessed``` is not removed, as it contains info about translation and rotation that are needed for pose estimation, but not for object detection model.\n",
    "\n",
    "The working directory is in the ```DenseFusion```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder to contain the dataset for YOLO model\n",
    "os.makedirs(\"../YOLO/datasets\", exist_ok=True)\n",
    "\n",
    "# count number of distinct classes\n",
    "number_classes = 0\n",
    "class_names = []\n",
    "for el in os.scandir(\"./Linemod_preprocessed/data\"):\n",
    "    # if entry is a directory and its name is an integer value (this is just to avoid counting non directories or other directories)\n",
    "    if (el.is_dir() and el.name.isdigit()):\n",
    "        class_names.append(el.name)\n",
    "        number_classes += 1\n",
    "\n",
    "# get string of all class names\n",
    "class_names.sort() # sort the names\n",
    "names = \"[\"\n",
    "for index, el in enumerate(class_names):\n",
    "    # if last element don't add comma\n",
    "    if index == number_classes-1:\n",
    "        names += f\"'{str(el)}'\"\n",
    "    else:\n",
    "        names += f\"'{str(el)}',\"\n",
    "names += \"]\"\n",
    "\n",
    "# create data.yaml (as class names use ids of the folder)\n",
    "content = f\"\"\"train: ./train/images\n",
    "val: ./val/images\n",
    "test: ./test/images\n",
    "\n",
    "nc: {number_classes}\n",
    "names: {names}\"\"\"\n",
    "# write to file\n",
    "with open(\"../YOLO/datasets/data.yaml\", \"w\") as fout:\n",
    "    fout.write(content)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba3d0",
   "metadata": {},
   "source": [
    "While creating the folder structure, we have to change the class id by using the index in the array written in the ```data.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to have easily access to the index\n",
    "index_dict = dict()\n",
    "for index, el in enumerate(class_names):\n",
    "    index_dict[int(el)] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c7b5",
   "metadata": {},
   "source": [
    "Create the folders. Note that each image may contain multiple objects. For instance in ```data/02/gt.yml``` for one image there are multiple objects, but just consider the object of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# create images and labels\n",
    "# dataset = [train_samples, validation_samples, test_samples]\n",
    "folder_names = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# count also the number of instances of each class\n",
    "classes = range(0, number_classes)\n",
    "counter_df = pd.DataFrame()\n",
    "for idx in range(3):\n",
    "    if idx == 0:\n",
    "        dataset = train_samples\n",
    "    elif idx == 1:\n",
    "        dataset = validation_samples\n",
    "    else:\n",
    "        dataset = test_samples\n",
    "    print(f\"------------------------------{folder_names[idx].upper()}------------------------------\")\n",
    "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"../YOLO/datasets/{folder_names[idx]}/labels\", exist_ok=True)\n",
    "    classCount = {label_object: 0 for label_object in index_dict.keys()} # initialize dictionary for counting\n",
    "    total = 0 # used to normalize count\n",
    "    for el in tqdm(dataset, desc=\"Moving...\"):\n",
    "        # el is (folderId, sampleId)\n",
    "        _, _, bbox, obj_id = train_dataset.load_6d_pose(el[0], el[1])\n",
    "        # copy image into the new folder\n",
    "        # avoid overwriting the files, so concat also the name of the folderId to the destination file\n",
    "        shutil.copy(f\"./Linemod_preprocessed/data/{el[0]:02d}/rgb/{el[1]:04d}.png\", f\"../YOLO/datasets/{folder_names[idx]}/images/{el[0]:02d}_{el[1]:04d}.png\")\n",
    "        # create label file with the same name as the image\n",
    "        with open(f\"../YOLO/datasets/{folder_names[idx]}/labels/{el[0]:02d}_{el[1]:04d}.txt\", \"w\") as fout:\n",
    "            # bbox is a list of values in the form of [x_center, y_center, width, height] and obj_id a list of class labels\n",
    "            # where each label is in the format 01-15\n",
    "            classCount[int(obj_id)] += 1\n",
    "            total += 1\n",
    "            content = f\"{index_dict[int(obj_id)]} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\"\n",
    "            fout.write(content)\n",
    "        fout.close()\n",
    "    \n",
    "    # store in the dataframe\n",
    "    values = pd.array(list(classCount.values()))/total\n",
    "    counter_df[folder_names[idx]] = values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3273181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of labels in training, validation and test set\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,6),sharey=True)\n",
    "for index, column in enumerate(counter_df.columns):\n",
    "    axes[index].barh([str(el) for el in index_dict.keys()], counter_df[column],color=\"orange\", edgecolor='gray')\n",
    "    axes[index].set_title(column.capitalize())\n",
    "    # add line that represents the uniform distribution of the labels\n",
    "    axes[index].axvline(x=1/number_classes, color=\"blue\")\n",
    "    axes[index].text(x=1/number_classes,y=-0.5,s=f\"{1/number_classes: .5f}\", color=\"blue\")\n",
    "\n",
    "fig.supxlabel(\"Frequency\")\n",
    "fig.supylabel(\"Labels\")\n",
    "plt.subplots_adjust(left=0.07, wspace=0.1)\n",
    "plt.suptitle(\"Labels Distribution over the Training, Validation and Test sets\")\n",
    "plt.savefig(\"../../../images/YOLO_dataset_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede843b",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b007448",
   "metadata": {},
   "source": [
    "Visualize depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./Linemod_preprocessed/data/02/depth/0000.png\"\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Plot image with bounding box\n",
    "\n",
    "# Load the ground truth poses from the gt.yml file\n",
    "with open(\"./Linemod_preprocessed/data/02/gt.yml\", 'r') as f:\n",
    "  pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "pose = pose_data[0][1] # access image 0 (start counting from 0) and get second object in that image (in case of multiple objects)\n",
    "\n",
    "bbox = np.array(pose['obj_bb'], dtype=np.float32) #[4]\n",
    "obj_id = np.array(pose['obj_id'], dtype=np.float32) #[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a rectangle patch\n",
    "rect = patches.Rectangle(\n",
    "    (bbox[0], bbox[1]),  # (x, y)\n",
    "    bbox[2],             # width\n",
    "    bbox[3],             # height\n",
    "    linewidth=2,\n",
    "    edgecolor='red',\n",
    "    facecolor='none'\n",
    ")\n",
    "\n",
    "# Add the rectangle to the plot\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Optionally add object ID label (write a bit above the top left corner)\n",
    "ax.text(bbox[0], bbox[1] - 10, f'ID: {int(obj_id)}', color='yellow', fontsize=12, backgroundcolor='black')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"Training loader: {len(train_loader)}\")\n",
    "print(f\"Validation loader: {len(val_loader)}\")\n",
    "print(f\"Test loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Get only the first 1 batch\n",
    "train_subset_num_batches = 1\n",
    "val_subset_num_batches = 1\n",
    "test_subset_num_batches = 1\n",
    "train_subset = list(itertools.islice(train_loader, train_subset_num_batches))\n",
    "val_subset = list(itertools.islice(val_loader, val_subset_num_batches))\n",
    "test_subset = list(itertools.islice(test_loader, test_subset_num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Get one batch from the train loader (4 images)\n",
    "batch = next(iter(train_loader)) # it uses load_6d_pose, so one pose per object\n",
    "\n",
    "# Extract relevant data\n",
    "rgb_images = batch[\"rgb\"]         # (B, 3, H, W)\n",
    "bboxes = batch[\"bbox\"]            # (B, 4) in pixel coords: x_min, y_min, x_max, y_max\n",
    "obj_ids = batch[\"obj_id\"]         # (B,)\n",
    "\n",
    "# Convert to numpy and rearrange channels\n",
    "rgb_images = rgb_images.permute(0, 2, 3, 1).numpy()  # (B, H, W, 3)\n",
    "bboxes = bboxes.numpy()\n",
    "obj_ids = obj_ids.numpy()\n",
    "\n",
    "# Plot settings\n",
    "batch_size = rgb_images.shape[0]\n",
    "cols = min(4, batch_size)\n",
    "rows = (batch_size + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    ax = axes[i]\n",
    "    img = rgb_images[i]\n",
    "    # each element is [x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT]\n",
    "    x_center, y_center, width, height = bboxes[i]\n",
    "    # remove normalization\n",
    "    x_center = x_center*IMG_WIDTH\n",
    "    y_center = y_center*IMG_HEIGHT\n",
    "    width = width*IMG_WIDTH\n",
    "    height = height*IMG_HEIGHT\n",
    "    x_min = x_center-(width/2)\n",
    "    y_min = y_center-(height/2)\n",
    "    obj_id = obj_ids[i]\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min),   # (x_min, y_min)\n",
    "        width,              # width\n",
    "        height,              # height\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add object ID as label\n",
    "    ax.text(\n",
    "        x_min,\n",
    "        y_min - 10,\n",
    "        f'ID: {int(obj_id)}',\n",
    "        color='yellow',\n",
    "        fontsize=10,\n",
    "        backgroundcolor='black'\n",
    "    )\n",
    "\n",
    "# Hide unused axes if batch_size < cols * rows\n",
    "for j in range(batch_size, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80313348",
   "metadata": {},
   "source": [
    "## Training Object Detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf706c86",
   "metadata": {},
   "source": [
    "Check if CUDA available, otherwise try with MPS and then CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e775da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Cuda not available, use mps\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d00f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../YOLO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"../../../checkpoints/yolo11n.pt\"\n",
    "model = YOLO(model_path)\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "IMG_SIZE = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03517373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model will automatically scale the image and related bounding box according to imgsz\n",
    "results = model.train(data=f\"{path}/datasets/data.yaml\", epochs=epochs, batch=batch_size, device=device,\n",
    "        imgsz=IMG_SIZE,\n",
    "        augment=True,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        hsv_h=0.4,\n",
    "        hsv_s=0.4,\n",
    "        hsv_v=0.4,\n",
    "        degrees=120,\n",
    "        translate=0.1,\n",
    "        scale=0.5,\n",
    "        shear=20,\n",
    "        perspective=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c08d2b",
   "metadata": {},
   "source": [
    "Copy model file to ```checkpoints```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(f\"./runs/detect/train/weights/best.pt\", f\"../../../checkpoints/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d7a82",
   "metadata": {},
   "source": [
    "Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../checkpoints/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "results = model.val(\n",
    "        data=f\"{path}/datasets/data.yaml\",\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6419a45",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../checkpoints/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "results = model.val(\n",
    "        data=f\"{path}/datasets/data.yaml\",\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=device,\n",
    "        split=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425354d6",
   "metadata": {},
   "source": [
    "## Pose Estimator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "path = path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"./YOLO/datasets_cropped\", exist_ok=True)\n",
    "\n",
    "# input: path YOLO/datasets\n",
    "# output: save images\n",
    "\n",
    "def mask_outside_bbox(path=\"./YOLO/datasets\", color=(0, 0, 0)):\n",
    "\n",
    "    for folder in os.scandir(path):\n",
    "        if folder.is_dir():\n",
    "            os.makedirs(f\"./YOLO/datasets_cropped/{folder.name}\", exist_ok=True)\n",
    "            path_dir= os.path.join(path, folder.name)\n",
    "            images_path = os.path.join(path_dir, \"images\")\n",
    "            labels_path = os.path.join(path_dir, \"labels\")\n",
    "\n",
    "            for img_name, label_name in zip(sorted(os.listdir(images_path)), sorted(os.listdir(labels_path))):\n",
    "                img_path = os.path.join(images_path, img_name)\n",
    "                label_path = os.path.join(labels_path, label_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                with open(label_path, \"r\") as f:\n",
    "                    _, x_center, y_center, w, h = map(float,f.readline().split(\" \"))\n",
    "                f.close()\n",
    "                \n",
    "                img_height, img_width, _ = img.shape\n",
    "                \n",
    "                x1 = round((x_center - w / 2) * img_width)\n",
    "                y1 = round((y_center - h / 2) * img_height)\n",
    "                x2 = round((x_center + w / 2) * img_width)\n",
    "                y2 = round((y_center + h / 2) * img_height)\n",
    "                \n",
    "                img_masked = np.full_like(img, color)\n",
    "                img_masked[y1:y2, x1:x2] = img[y1:y2, x1:x2]\n",
    "                \n",
    "                cv2.imwrite(f\"./YOLO/datasets_cropped/{folder.name}/{img_name}\",img=img_masked)   \n",
    "             \n",
    "mask_outside_bbox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063b816",
   "metadata": {},
   "source": [
    "Copy the gt.yaml and info.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c76097",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in class_names:\n",
    "    shutil.copy(f\"./DenseFusion/Linemod_preprocessed/data/{el}/gt.yml\", f\"./YOLO/datasets_cropped/{el}_gt.yml\")\n",
    "    shutil.copy(f\"./DenseFusion/Linemod_preprocessed/data/{el}/info.yml\", f\"./YOLO/datasets_cropped/{el}_info.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetPose(Dataset): # used to load and preprocess data\n",
    "    def __init__(self, dataset_root, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_root (str): Path to the dataset directory.\n",
    "            split (str): 'train', 'validation' or 'test'.\n",
    "        \"\"\"\n",
    "        self.dataset_root = dataset_root\n",
    "        if split == \"train\":\n",
    "            self.split = split\n",
    "        elif split == \"validation\":\n",
    "            self.split = \"val\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "\n",
    "        # Get list of all samples (folder_id, sample_id)\n",
    "        self.samples = self.get_all_samples()\n",
    "\n",
    "        # Check if samples were found\n",
    "        if not self.samples:\n",
    "            raise ValueError(f\"No samples found in {self.dataset_root}. Check the dataset path and structure.\")\n",
    "\n",
    "        # Define image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def get_samples_id(self):\n",
    "        return self.samples\n",
    "\n",
    "    def get_all_samples(self):\n",
    "        \"\"\"Retrieve the list of all available sample indices from all folders.\"\"\"\n",
    "        for folder in [\"train\",\"val\",\"test\"]:\n",
    "            if folder == self.split:\n",
    "                folder_path = os.path.join(self.dataset_root, f\"{folder}\")\n",
    "                #print(folder_path)\n",
    "                if os.path.exists(folder_path):\n",
    "                    # get name of files <folder id>_<image>\n",
    "                    sample_ids = sorted([f.split('.')[0] for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "        return sample_ids\n",
    "\n",
    "    #Define here some usefull functions to access the data\n",
    "    def load_image(self, img_path):\n",
    "        \"\"\"Load an RGB image and convert to tensor.\"\"\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.transform(img)\n",
    "\n",
    "    def load_6d_pose(self, sample_id):\n",
    "        \"\"\"Load the 6D pose (translation and rotation) for the object in this sample.\"\"\"\n",
    "        label = int(sample_id.split(\"_\")[0])\n",
    "        objectId = int(sample_id.split(\"_\")[1])\n",
    "        pose_file = os.path.join(self.dataset_root, f\"{label:02d}_gt.yml\")\n",
    "\n",
    "        # Load the ground truth poses from the gt.yml file\n",
    "        with open(pose_file, 'r') as f:\n",
    "            pose_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # The pose data is a dictionary where each key corresponds to a frame with pose info\n",
    "        # We assume sample_id corresponds to the key in pose_data\n",
    "        if objectId not in pose_data:\n",
    "            raise KeyError(f\"Sample ID {objectId} not found in {label:02d}_gt.yml.\")\n",
    "\n",
    "        for pose in pose_data[objectId]: # There can be more than one pose per sample, but take the one of label=folder_id\n",
    "            # Extract translation and rotation\n",
    "            if (int(pose['obj_id']) == int(label)):\n",
    "                translation = np.array(pose['cam_t_m2c'], dtype=np.float32)  # [3] ---> (x,y,z)\n",
    "                rotation = np.array(pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)  # [3x3] ---> rotation matrix\n",
    "                # bbox is top left corner and width and height info, YOLO needs center coordinates and width and height\n",
    "                x_min, y_min, width, height = np.array(pose['obj_bb'], dtype=np.float32) # [4] ---> x_min, y_min, width, height\n",
    "                # compute initial center\n",
    "                x_center = x_min + width/2\n",
    "                y_center = y_min + height/2\n",
    "                \n",
    "                # store coordinates of the center and width and height of the bounding box normalized to the\n",
    "                # image width=640 pixels and height=480 pixels\n",
    "                bbox = np.array([x_center/IMG_WIDTH, y_center/IMG_HEIGHT, width/IMG_WIDTH, height/IMG_HEIGHT], dtype=np.float32)\n",
    "\n",
    "                obj_id = np.array(pose['obj_id'], dtype=np.float32) # [1] ---> label\n",
    "                break\n",
    "\n",
    "        return translation, rotation, bbox, obj_id\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the total number of samples in the selected split.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Load a dataset sample.\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        img_path = os.path.join(self.dataset_root, f\"{self.split}\", f\"{sample}.png\")\n",
    "\n",
    "        img = self.load_image(img_path)\n",
    "        translation, rotation, bbox, obj_id = self.load_6d_pose(sample)\n",
    "\n",
    "        #Dictionary with all the data\n",
    "        return {\n",
    "            \"rgb\": img,\n",
    "            \"translation\": torch.tensor(translation),\n",
    "            \"rotation\": torch.tensor(rotation),\n",
    "            \"bbox\": torch.tensor(bbox),\n",
    "            \"obj_id\": torch.tensor(obj_id),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_pose = \"./YOLO/datasets_cropped\"\n",
    "\n",
    "train_dataset = CustomDatasetPose(dataset_root_pose, split=\"train\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "val_dataset = CustomDatasetPose(dataset_root_pose, split=\"validation\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetPose(dataset_root_pose, split=\"test\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa40cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimatorQuat6D(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet18\", pretrained=True):\n",
    "        super(PoseEstimatorQuat6D, self).__init__()\n",
    "\n",
    "        # Load backbone (without last fully connected layer)\n",
    "        backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        in_features = backbone.fc.in_features\n",
    "\n",
    "        # 7 output (4 quaternion, 3 translation)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.fc(x)\n",
    "\n",
    "        quat = output[:, :4]\n",
    "        quat = quat / torch.norm(quat, dim=1, keepdim=True)  # Normalize quaternion\n",
    "\n",
    "        trans = output[:, 4:]\n",
    "        return quat, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, translation, rotation):\n",
    "    quat, trans = outputs\n",
    "    # rotation is list of type quaternion, make it tensor of float32 not float64 (double)\n",
    "    rotation = torch.tensor(np.stack([el.components for el in rotation]).astype(np.float32)).to(device)\n",
    "    loss_quat = F.mse_loss(quat, rotation)\n",
    "    loss_trans = F.mse_loss(trans, translation)\n",
    "\n",
    "    return loss_quat + loss_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac57e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import quaternion\n",
    "\n",
    "def train(model, epoch, dataloader, criterion, optimizer=Adam, device=device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, translation, rotation = data[\"rgb\"], data[\"translation\"], data[\"rotation\"]\n",
    "        img, translation, rotation = img.to(device), translation.to(device), quaternion.from_rotation_matrix(rotation)\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, translation, rotation)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 2 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} '\n",
    "                f'({100. * batch_idx / len(dataloader):.0f}%)]\\t Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[0.001]\n",
    "batch_size=[2]\n",
    "num_epoch=10\n",
    "for lr in learning_rate:\n",
    "    for batch in batch_size:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "        model = PoseEstimatorQuat6D().to(device)\n",
    "        print(model)\n",
    "        for epoch in range(1, num_epoch):\n",
    "            train(model, epoch, train_dataloader, criterion, optimizer=Adam(model.parameters(), lr=lr), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f1857",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quaternion\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed985b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPose(pathImage, translation_gt, rotation_gt, translation_pred, rotation_pred):\n",
    "    '''\n",
    "        Input:\n",
    "            path for image (in DenseFusion)\n",
    "            ground truth translation tensor (in millimeters)\n",
    "            ground truth rotation tensor (either matrix or quaternion)\n",
    "            predicted translation tensor (in millimeters)\n",
    "            predicted rotation tensor (either matrix or quaternion)\n",
    "    '''\n",
    "\n",
    "    # read image\n",
    "    image = cv2.imread(pathImage)\n",
    "    transparent_image = image.copy() # copy of the image to work on a transparent image (for the second reference system)\n",
    "\n",
    "    # read translation and rotation\n",
    "    rotat_gt = rotation_gt.numpy() # transform tensor to numpy array\n",
    "    trans_gt = translation_gt.numpy()/1000 # in meters\n",
    "    rotat_pred = rotation_pred.numpy()\n",
    "    trans_pred = translation_pred.numpy()/1000\n",
    "\n",
    "    # read camera intrinsics\n",
    "    label = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    image_id = pathImage.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
    "    with open(f\"./YOLO/datasets_cropped/{label}_info.yml\", 'r') as f:\n",
    "            camera_info = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    camera_intrinsics = np.array(camera_info[int(image_id)][\"cam_K\"]).reshape(3,3)\n",
    "\n",
    "    # read 3D model\n",
    "    meshModel = trimesh.load(f\"./DenseFusion/Linemod_preprocessed/models/obj_{label}.ply\")\n",
    "    vertices = meshModel.vertices/1000 # it has 3 columns, for X, Y, Z, use unit of measurement of translation\n",
    "    # compute corners\n",
    "    min_corner = vertices.min(axis=0) # find for each column the smallest value\n",
    "    max_corner = vertices.max(axis=0)\n",
    "    \n",
    "    bounding_box_3d = np.array([[min_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], min_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], min_corner[2]],\n",
    "                                [min_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], min_corner[1], max_corner[2]],\n",
    "                                [max_corner[0], max_corner[1], max_corner[2]],\n",
    "                                [min_corner[0], max_corner[1], max_corner[2]],])\n",
    "\n",
    "    # convert quaternion to rotation matrix, if input was quaternion\n",
    "    if rotat_gt.size == 4:\n",
    "        rotat_gt = quaternion.as_rotation_matrix(np.quaternion(*rotat_gt))\n",
    "    else:\n",
    "        rotat_gt = rotat_gt.reshape(3,3)\n",
    "    if rotat_pred.size == 4:\n",
    "        rotat_pred = quaternion.as_rotation_matrix(np.quaternion(*rotat_pred))\n",
    "    else:\n",
    "        rotat_pred = rotat_pred.reshape(3,3)\n",
    "\n",
    "    # build 3D axes according to object coordinate system, same unit of measurement of translation, so in meters\n",
    "    axes_3d = np.array([\n",
    "        [0, 0, 0],      # origin, in the object coordinate system\n",
    "        [0.15, 0, 0],   # how long the arrow should be in the X coordinate\n",
    "        [0, 0.15, 0],   # how long the arrow should be in the Y coordinate\n",
    "        [0, 0, 0.15]    # how long the arrow should be in the Z coordinate\n",
    "    ])\n",
    "\n",
    "    # transform the object coordinate system to the camera coordinate system\n",
    "    # rotat_gt is 3x3, so axes_3d has to be transposed, then add to origin, and coordinates the translation\n",
    "    axes_cam_gt = (rotat_gt @ axes_3d.T).T + trans_gt\n",
    "    # bounding box\n",
    "    bounding_box_3d_cam_gt = (rotat_gt @ bounding_box_3d.T).T + trans_gt\n",
    "    # project 3D axes to 2D\n",
    "    axes_2d_gt = (camera_intrinsics @ axes_cam_gt.T).T # camera_intrinsics is 3x3, while axes_cam_gt 4x3, axes_2d_gt 4x3\n",
    "    axes_2d_gt = axes_2d_gt[:, :2] / axes_2d_gt[:, 2:3] # take first 2 columns and normalize by depth\n",
    "    # bounding box\n",
    "    bounding_box_2d_gt = (camera_intrinsics @ bounding_box_3d_cam_gt.T).T\n",
    "    bounding_box_2d_gt = (bounding_box_2d_gt[:, :2] / bounding_box_2d_gt[:, 2:3]).astype(int)\n",
    "    # get point coordinates\n",
    "    p_gt = [tuple(el) for el in bounding_box_2d_gt]\n",
    "    # define edges using two points, access with index\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    # draw edges\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_gt[el[0]], p_gt[el[1]], (0,0,255), 5)\n",
    "\n",
    "    p0_gt = tuple(axes_2d_gt[0].astype(int)) # take origin coordinates\n",
    "    p1_gt = tuple(axes_2d_gt[1].astype(int))\n",
    "    p2_gt = tuple(axes_2d_gt[2].astype(int))\n",
    "    p3_gt = tuple(axes_2d_gt[3].astype(int))\n",
    "\n",
    "    # color is in BGR format, set tickness=2\n",
    "    cv2.arrowedLine(image, p0_gt, p1_gt, (0, 0, 255), 2) # X is red\n",
    "    cv2.arrowedLine(image, p0_gt, p2_gt, (0, 255, 0), 2) # Y is green\n",
    "    cv2.arrowedLine(image, p0_gt, p3_gt, (255, 0, 0), 2) # Z is blue\n",
    "\n",
    "    # for predicted\n",
    "    axes_cam_pred = (rotat_pred @ axes_3d.T).T + trans_pred\n",
    "    bounding_box_3d_cam_pred = (rotat_pred @ bounding_box_3d.T).T + trans_pred\n",
    "\n",
    "    axes_2d_pred = (camera_intrinsics @ axes_cam_pred.T).T\n",
    "    axes_2d_pred = axes_2d_pred[:, :2] / axes_2d_pred[:, 2:3]\n",
    "    bounding_box_2d_pred = (camera_intrinsics @ bounding_box_3d_cam_pred.T).T\n",
    "    bounding_box_2d_pred = (bounding_box_2d_pred[:, :2] / bounding_box_2d_pred[:, 2:3]).astype(int)\n",
    "\n",
    "    p_pred = [tuple(el) for el in bounding_box_2d_pred]\n",
    "    edges = [(0,1), (1,2), (2,3), (3,0), # bottom\n",
    "             (0,4), (1,5), (2,6), (3,7), # vertical\n",
    "             (4,5), (5,6), (6,7), (7,4) # top\n",
    "             ]\n",
    "    for el in edges:\n",
    "        cv2.line(image, p_pred[el[0]], p_pred[el[1]], (255,0,0), 5)\n",
    "\n",
    "    p0_pred = tuple(axes_2d_pred[0].astype(int))\n",
    "    p1_pred = tuple(axes_2d_pred[1].astype(int))\n",
    "    p2_pred = tuple(axes_2d_pred[2].astype(int))\n",
    "    p3_pred = tuple(axes_2d_pred[3].astype(int))\n",
    "\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p1_pred, (0, 0, 255), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p2_pred, (0, 255, 0), 2)\n",
    "    cv2.arrowedLine(transparent_image, p0_pred, p3_pred, (255, 0, 0), 2)\n",
    "\n",
    "    # show image after merging the two images\n",
    "    overlapImage = cv2.addWeighted(transparent_image, 0.5, image, 1, 0)\n",
    "    plt.imshow(cv2.cvtColor(overlapImage, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Object Pose Estimation (prediction is transparent)\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningAndDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
